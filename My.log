2024-06-18 13:19:28,999 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:19:29,000 Listing 'user/bdm/Formatted_zone/hotels'.
2024-06-18 13:19:29,000 Resolved path '/' to '/'.
2024-06-18 13:19:29,008 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:19:29,028 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:19:29,029 Updated root to '/user/bdm'.
2024-06-18 13:19:29,029 Resolved path 'user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:19:29,030 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:19:29,066 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,068 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:19:29,068 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:19:29,077 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,078 Listing 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:19:29,079 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:19:29,079 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:19:29,087 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,088 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to 'model_temp/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,088 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,089 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' (depth 0).
2024-06-18 13:19:29,089 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,089 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,089 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,097 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,098 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,098 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\._SUCCESS.crc'.
2024-06-18 13:19:29,098 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,099 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,112 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,114 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:19:29,215 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:19:29,216 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:19:29,216 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\._SUCCESS.crc' complete.
2024-06-18 13:19:29,217 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to 'model_temp/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,217 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,217 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:19:29,218 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,218 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,218 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,229 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,230 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,230 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,265 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,266 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,278 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,280 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,302 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 656
2024-06-18 13:19:29,302 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,303 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' complete.
2024-06-18 13:19:29,303 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to 'model_temp/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,303 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,304 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' (depth 0).
2024-06-18 13:19:29,304 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,304 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,304 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,314 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,314 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,314 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\_SUCCESS'.
2024-06-18 13:19:29,315 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,315 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,323 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,325 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,344 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:19:29,344 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:19:29,345 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\_SUCCESS' complete.
2024-06-18 13:19:29,345 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to 'model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,345 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,345 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' (depth 0).
2024-06-18 13:19:29,346 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,346 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,346 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,355 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,356 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,356 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,371 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,371 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,385 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,387 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,410 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 82434
2024-06-18 13:19:29,435 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:19:29,435 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' complete.
2024-06-18 13:19:29,435 File 20240615-1328-hotels downloaded correctly
2024-06-18 13:19:29,436 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:19:29,436 Listing 'user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:19:29,436 Resolved path '/' to '/'.
2024-06-18 13:19:29,437 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:19:29,451 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:19:29,452 Updated root to '/user/bdm'.
2024-06-18 13:19:29,452 Resolved path 'user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:19:29,452 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:19:29,460 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,460 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:19:29,460 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:19:29,469 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,470 Listing 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:19:29,470 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:19:29,470 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:19:29,512 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,513 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to 'model_temp/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,513 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,513 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' (depth 0).
2024-06-18 13:19:29,513 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,514 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,514 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,524 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,524 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,524 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\._SUCCESS.crc'.
2024-06-18 13:19:29,526 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,526 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,538 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,540 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:19:29,558 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:19:29,559 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:19:29,559 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\._SUCCESS.crc' complete.
2024-06-18 13:19:29,559 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to 'model_temp/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,559 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,560 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:19:29,560 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,560 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,561 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,569 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,570 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,570 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,570 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,570 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,582 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,584 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,606 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 136
2024-06-18 13:19:29,606 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,607 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' complete.
2024-06-18 13:19:29,607 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to 'model_temp/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,607 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,608 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' (depth 0).
2024-06-18 13:19:29,608 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,608 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,608 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,617 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,618 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,618 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\_SUCCESS'.
2024-06-18 13:19:29,618 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,618 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,628 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,630 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,649 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:19:29,649 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:19:29,649 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\_SUCCESS' complete.
2024-06-18 13:19:29,651 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to 'model_temp/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,651 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,651 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' (depth 0).
2024-06-18 13:19:29,652 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,652 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,652 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,661 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,661 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,662 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,663 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,663 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,674 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,676 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,694 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 16241
2024-06-18 13:19:29,734 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:19:29,734 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' complete.
2024-06-18 13:19:29,735 File 20240614-2304-renda_familiar downloaded correctly
2024-06-18 13:19:29,735 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:19:29,735 Listing 'user/bdm/Formatted_zone/idealista'.
2024-06-18 13:19:29,735 Resolved path '/' to '/'.
2024-06-18 13:19:29,736 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:19:29,752 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:19:29,753 Updated root to '/user/bdm'.
2024-06-18 13:19:29,753 Resolved path 'user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:19:29,753 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:19:29,761 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,761 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:19:29,761 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:19:29,770 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,774 Listing 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:19:29,775 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:19:29,775 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:19:29,783 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,784 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to 'model_temp/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,784 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,785 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' (depth 0).
2024-06-18 13:19:29,785 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,785 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,785 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,795 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,796 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,796 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\._SUCCESS.crc'.
2024-06-18 13:19:29,796 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,796 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,809 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,811 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:19:29,829 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:19:29,829 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:19:29,829 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\._SUCCESS.crc' complete.
2024-06-18 13:19:29,829 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to 'model_temp/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,829 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,830 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:19:29,830 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,830 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,830 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,841 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,842 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,842 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,842 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,842 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,854 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,856 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,875 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 10236
2024-06-18 13:19:29,875 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:19:29,875 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' complete.
2024-06-18 13:19:29,876 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to 'model_temp/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,876 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,876 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' (depth 0).
2024-06-18 13:19:29,877 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,877 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,877 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,887 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,887 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,887 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\_SUCCESS'.
2024-06-18 13:19:29,888 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,888 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,897 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,899 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,917 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:19:29,917 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:19:29,917 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\_SUCCESS' complete.
2024-06-18 13:19:29,917 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to 'model_temp/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,918 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,918 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' (depth 0).
2024-06-18 13:19:29,918 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,919 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,919 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,928 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:29,928 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:29,929 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,931 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,931 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:29,966 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:29,968 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:29,988 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 1308973
2024-06-18 13:19:30,217 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:19:30,217 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' complete.
2024-06-18 13:19:30,217 File 20240614-2306-idealista downloaded correctly
2024-06-18 13:19:30,218 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:19:30,218 Listing 'user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:19:30,218 Resolved path '/' to '/'.
2024-06-18 13:19:30,219 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:19:30,234 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:19:30,235 Updated root to '/user/bdm'.
2024-06-18 13:19:30,235 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:19:30,235 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:19:30,245 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,245 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:19:30,245 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:19:30,256 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,257 Listing 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:19:30,257 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:19:30,257 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:19:30,267 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,268 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to 'model_temp/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,268 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,269 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' (depth 0).
2024-06-18 13:19:30,269 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,269 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,269 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,277 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,277 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:30,277 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\._SUCCESS.crc'.
2024-06-18 13:19:30,278 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,278 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,290 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:30,293 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:19:30,309 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:19:30,310 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:19:30,310 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\._SUCCESS.crc' complete.
2024-06-18 13:19:30,310 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to 'model_temp/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,310 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,311 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:19:30,311 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,311 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,311 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,320 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,320 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:30,320 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,321 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,321 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,332 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:30,333 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:30,351 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 88
2024-06-18 13:19:30,351 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:19:30,352 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' complete.
2024-06-18 13:19:30,352 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to 'model_temp/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,352 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,353 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' (depth 0).
2024-06-18 13:19:30,353 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,353 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,353 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,396 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,396 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:30,397 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\_SUCCESS'.
2024-06-18 13:19:30,398 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,398 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,408 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:30,410 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:30,432 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:19:30,432 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:19:30,432 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\_SUCCESS' complete.
2024-06-18 13:19:30,432 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to 'model_temp/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,434 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,434 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' (depth 0).
2024-06-18 13:19:30,434 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,434 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,434 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,446 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:19:30,447 Downloading 1 files using 1 thread(s).
2024-06-18 13:19:30,447 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,448 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,448 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,461 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:19:30,463 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:19:30,483 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 9934
2024-06-18 13:19:30,484 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:19:30,484 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' complete.
2024-06-18 13:19:30,484 File 20240614-2308-lookup_renta_idealista downloaded correctly
2024-06-18 13:22:27,608 GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
2024-06-18 13:22:27,609 Command to send: A
d8b36e589d5b5f75f36e1098e64000379d724a996fd49df5d915ba73880dac92

2024-06-18 13:22:27,644 Answer received: !yv
2024-06-18 13:22:27,644 Command to send: j
i
rj
org.apache.spark.SparkConf
e

2024-06-18 13:22:27,646 Answer received: !yv
2024-06-18 13:22:27,646 Command to send: j
i
rj
org.apache.spark.api.java.*
e

2024-06-18 13:22:27,647 Answer received: !yv
2024-06-18 13:22:27,647 Command to send: j
i
rj
org.apache.spark.api.python.*
e

2024-06-18 13:22:27,647 Answer received: !yv
2024-06-18 13:22:27,648 Command to send: j
i
rj
org.apache.spark.ml.python.*
e

2024-06-18 13:22:27,648 Answer received: !yv
2024-06-18 13:22:27,648 Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

2024-06-18 13:22:27,649 Answer received: !yv
2024-06-18 13:22:27,649 Command to send: j
i
rj
org.apache.spark.resource.*
e

2024-06-18 13:22:27,650 Answer received: !yv
2024-06-18 13:22:27,650 Command to send: j
i
rj
org.apache.spark.sql.*
e

2024-06-18 13:22:27,650 Answer received: !yv
2024-06-18 13:22:27,650 Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

2024-06-18 13:22:27,651 Answer received: !yv
2024-06-18 13:22:27,651 Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

2024-06-18 13:22:27,652 Answer received: !yv
2024-06-18 13:22:27,652 Command to send: j
i
rj
scala.Tuple2
e

2024-06-18 13:22:27,652 Answer received: !yv
2024-06-18 13:22:27,652 Command to send: r
u
SparkConf
rj
e

2024-06-18 13:22:27,655 Answer received: !ycorg.apache.spark.SparkConf
2024-06-18 13:22:27,655 Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-06-18 13:22:27,679 Answer received: !yro0
2024-06-18 13:22:27,679 Command to send: c
o0
set
sspark.master
slocal
e

2024-06-18 13:22:27,758 Answer received: !yro1
2024-06-18 13:22:27,758 Command to send: c
o0
set
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:22:27,759 Answer received: !yro2
2024-06-18 13:22:27,759 Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

2024-06-18 13:22:27,761 Answer received: !ybfalse
2024-06-18 13:22:27,762 Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

2024-06-18 13:22:27,762 Answer received: !yro3
2024-06-18 13:22:27,762 Command to send: c
o0
contains
sspark.rdd.compress
e

2024-06-18 13:22:27,763 Answer received: !ybfalse
2024-06-18 13:22:27,763 Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

2024-06-18 13:22:27,763 Answer received: !yro4
2024-06-18 13:22:27,763 Command to send: c
o0
contains
sspark.master
e

2024-06-18 13:22:27,764 Answer received: !ybtrue
2024-06-18 13:22:27,764 Command to send: c
o0
contains
sspark.app.name
e

2024-06-18 13:22:27,765 Answer received: !ybtrue
2024-06-18 13:22:27,765 Command to send: c
o0
contains
sspark.master
e

2024-06-18 13:22:27,765 Answer received: !ybtrue
2024-06-18 13:22:27,766 Command to send: c
o0
get
sspark.master
e

2024-06-18 13:22:27,767 Answer received: !yslocal
2024-06-18 13:22:27,768 Command to send: c
o0
contains
sspark.app.name
e

2024-06-18 13:22:27,768 Answer received: !ybtrue
2024-06-18 13:22:27,768 Command to send: c
o0
get
sspark.app.name
e

2024-06-18 13:22:27,769 Answer received: !ysFormatted zone loader
2024-06-18 13:22:27,769 Command to send: c
o0
contains
sspark.home
e

2024-06-18 13:22:27,769 Answer received: !ybfalse
2024-06-18 13:22:27,769 Command to send: c
o0
getAll
e

2024-06-18 13:22:27,770 Answer received: !yto5
2024-06-18 13:22:27,771 Command to send: a
e
o5
e

2024-06-18 13:22:27,771 Answer received: !yi8
2024-06-18 13:22:27,771 Command to send: a
g
o5
i0
e

2024-06-18 13:22:27,772 Answer received: !yro6
2024-06-18 13:22:27,772 Command to send: c
o6
_1
e

2024-06-18 13:22:27,773 Answer received: !ysspark.master
2024-06-18 13:22:27,773 Command to send: c
o6
_2
e

2024-06-18 13:22:27,774 Answer received: !yslocal
2024-06-18 13:22:27,774 Command to send: a
e
o5
e

2024-06-18 13:22:27,774 Answer received: !yi8
2024-06-18 13:22:27,774 Command to send: a
g
o5
i1
e

2024-06-18 13:22:27,775 Answer received: !yro7
2024-06-18 13:22:27,775 Command to send: c
o7
_1
e

2024-06-18 13:22:27,775 Answer received: !ysspark.rdd.compress
2024-06-18 13:22:27,775 Command to send: c
o7
_2
e

2024-06-18 13:22:27,776 Answer received: !ysTrue
2024-06-18 13:22:27,776 Command to send: a
e
o5
e

2024-06-18 13:22:27,776 Answer received: !yi8
2024-06-18 13:22:27,776 Command to send: a
g
o5
i2
e

2024-06-18 13:22:27,777 Answer received: !yro8
2024-06-18 13:22:27,777 Command to send: c
o8
_1
e

2024-06-18 13:22:27,777 Answer received: !ysspark.serializer.objectStreamReset
2024-06-18 13:22:27,777 Command to send: c
o8
_2
e

2024-06-18 13:22:27,778 Answer received: !ys100
2024-06-18 13:22:27,778 Command to send: a
e
o5
e

2024-06-18 13:22:27,778 Answer received: !yi8
2024-06-18 13:22:27,778 Command to send: a
g
o5
i3
e

2024-06-18 13:22:27,779 Answer received: !yro9
2024-06-18 13:22:27,779 Command to send: c
o9
_1
e

2024-06-18 13:22:27,779 Answer received: !ysspark.submit.pyFiles
2024-06-18 13:22:27,780 Command to send: c
o9
_2
e

2024-06-18 13:22:27,780 Answer received: !ys
2024-06-18 13:22:27,780 Command to send: a
e
o5
e

2024-06-18 13:22:27,780 Answer received: !yi8
2024-06-18 13:22:27,780 Command to send: a
g
o5
i4
e

2024-06-18 13:22:27,781 Answer received: !yro10
2024-06-18 13:22:27,781 Command to send: c
o10
_1
e

2024-06-18 13:22:27,782 Answer received: !ysspark.app.submitTime
2024-06-18 13:22:27,782 Command to send: c
o10
_2
e

2024-06-18 13:22:27,782 Answer received: !ys1718709747485
2024-06-18 13:22:27,782 Command to send: a
e
o5
e

2024-06-18 13:22:27,783 Answer received: !yi8
2024-06-18 13:22:27,783 Command to send: a
g
o5
i5
e

2024-06-18 13:22:27,783 Answer received: !yro11
2024-06-18 13:22:27,783 Command to send: c
o11
_1
e

2024-06-18 13:22:27,784 Answer received: !ysspark.submit.deployMode
2024-06-18 13:22:27,784 Command to send: c
o11
_2
e

2024-06-18 13:22:27,784 Answer received: !ysclient
2024-06-18 13:22:27,784 Command to send: a
e
o5
e

2024-06-18 13:22:27,785 Answer received: !yi8
2024-06-18 13:22:27,785 Command to send: a
g
o5
i6
e

2024-06-18 13:22:27,785 Answer received: !yro12
2024-06-18 13:22:27,785 Command to send: c
o12
_1
e

2024-06-18 13:22:27,786 Answer received: !ysspark.app.name
2024-06-18 13:22:27,786 Command to send: c
o12
_2
e

2024-06-18 13:22:27,786 Answer received: !ysFormatted zone loader
2024-06-18 13:22:27,786 Command to send: a
e
o5
e

2024-06-18 13:22:27,787 Answer received: !yi8
2024-06-18 13:22:27,787 Command to send: a
g
o5
i7
e

2024-06-18 13:22:27,787 Answer received: !yro13
2024-06-18 13:22:27,787 Command to send: c
o13
_1
e

2024-06-18 13:22:27,788 Answer received: !ysspark.ui.showConsoleProgress
2024-06-18 13:22:27,788 Command to send: c
o13
_2
e

2024-06-18 13:22:27,788 Answer received: !ystrue
2024-06-18 13:22:27,788 Command to send: a
e
o5
e

2024-06-18 13:22:27,789 Answer received: !yi8
2024-06-18 13:22:27,789 Command to send: r
u
JavaSparkContext
rj
e

2024-06-18 13:22:27,804 Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
2024-06-18 13:22:27,804 Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

2024-06-18 13:22:28,609 Command to send: A
d8b36e589d5b5f75f36e1098e64000379d724a996fd49df5d915ba73880dac92

2024-06-18 13:22:28,610 Answer received: !yv
2024-06-18 13:22:28,610 Command to send: m
d
o1
e

2024-06-18 13:22:28,611 Answer received: !yv
2024-06-18 13:22:28,611 Command to send: m
d
o2
e

2024-06-18 13:22:28,611 Answer received: !yv
2024-06-18 13:22:28,611 Command to send: m
d
o3
e

2024-06-18 13:22:28,612 Answer received: !yv
2024-06-18 13:22:28,612 Command to send: m
d
o4
e

2024-06-18 13:22:28,612 Answer received: !yv
2024-06-18 13:22:28,612 Command to send: m
d
o5
e

2024-06-18 13:22:28,613 Answer received: !yv
2024-06-18 13:22:31,773 Answer received: !yro14
2024-06-18 13:22:31,773 Command to send: c
o14
sc
e

2024-06-18 13:22:31,780 Answer received: !yro15
2024-06-18 13:22:31,780 Command to send: c
o15
conf
e

2024-06-18 13:22:31,802 Answer received: !yro16
2024-06-18 13:22:31,804 Command to send: r
u
PythonAccumulatorV2
rj
e

2024-06-18 13:22:31,808 Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
2024-06-18 13:22:31,808 Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i57844
sd8b36e589d5b5f75f36e1098e64000379d724a996fd49df5d915ba73880dac92
e

2024-06-18 13:22:31,810 Answer received: !yro17
2024-06-18 13:22:31,811 Command to send: c
o14
sc
e

2024-06-18 13:22:31,811 Answer received: !yro18
2024-06-18 13:22:31,811 Command to send: c
o18
register
ro17
e

2024-06-18 13:22:31,817 Answer received: !yv
2024-06-18 13:22:31,817 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:22:31,819 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:22:31,819 Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

2024-06-18 13:22:31,819 Answer received: !ym
2024-06-18 13:22:31,819 Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro14
e

2024-06-18 13:22:31,824 Answer received: !ybfalse
2024-06-18 13:22:31,824 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:22:31,825 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:22:31,825 Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

2024-06-18 13:22:31,825 Answer received: !ym
2024-06-18 13:22:31,825 Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro14
e

2024-06-18 13:22:31,827 Answer received: !yL15
2024-06-18 13:22:31,827 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:22:31,827 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:22:31,828 Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

2024-06-18 13:22:31,828 Answer received: !ym
2024-06-18 13:22:31,828 Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro14
e

2024-06-18 13:22:31,828 Answer received: !yi65536
2024-06-18 13:22:31,829 Command to send: r
u
org
rj
e

2024-06-18 13:22:31,832 Answer received: !yp
2024-06-18 13:22:31,832 Command to send: r
u
org.apache
rj
e

2024-06-18 13:22:31,833 Answer received: !yp
2024-06-18 13:22:31,833 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:22:31,833 Answer received: !yp
2024-06-18 13:22:31,833 Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2024-06-18 13:22:31,835 Answer received: !ycorg.apache.spark.SparkFiles
2024-06-18 13:22:31,835 Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2024-06-18 13:22:31,835 Answer received: !ym
2024-06-18 13:22:31,835 Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2024-06-18 13:22:31,837 Answer received: !ysC:\\Users\\FX504\\AppData\\Local\\Temp\\spark-a590deca-5ece-43c1-9b42-3c5e7c2d2303\\userFiles-97df2249-6a9e-4997-ae6d-258066b5bc4c
2024-06-18 13:22:31,837 Command to send: c
o16
get
sspark.submit.pyFiles
s
e

2024-06-18 13:22:31,838 Answer received: !ys
2024-06-18 13:22:31,838 Command to send: r
u
org
rj
e

2024-06-18 13:22:31,840 Answer received: !yp
2024-06-18 13:22:31,840 Command to send: r
u
org.apache
rj
e

2024-06-18 13:22:31,841 Answer received: !yp
2024-06-18 13:22:31,841 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:22:31,841 Answer received: !yp
2024-06-18 13:22:31,841 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:22:31,842 Answer received: !yp
2024-06-18 13:22:31,842 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:22:31,844 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:22:31,844 Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

2024-06-18 13:22:31,859 Answer received: !ym
2024-06-18 13:22:31,859 Command to send: c
o14
sc
e

2024-06-18 13:22:31,860 Answer received: !yro19
2024-06-18 13:22:31,860 Command to send: c
o19
conf
e

2024-06-18 13:22:31,860 Answer received: !yro20
2024-06-18 13:22:31,860 Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro20
e

2024-06-18 13:22:31,861 Answer received: !ysC:\\Users\\FX504\\AppData\\Local\\Temp\\spark-a590deca-5ece-43c1-9b42-3c5e7c2d2303
2024-06-18 13:22:31,861 Command to send: r
u
org
rj
e

2024-06-18 13:22:31,863 Answer received: !yp
2024-06-18 13:22:31,864 Command to send: r
u
org.apache
rj
e

2024-06-18 13:22:31,864 Answer received: !yp
2024-06-18 13:22:31,864 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:22:31,866 Answer received: !yp
2024-06-18 13:22:31,866 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:22:31,866 Answer received: !yp
2024-06-18 13:22:31,867 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:22:31,867 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:22:31,867 Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

2024-06-18 13:22:31,867 Answer received: !ym
2024-06-18 13:22:31,868 Command to send: c
z:org.apache.spark.util.Utils
createTempDir
sC:\\Users\\FX504\\AppData\\Local\\Temp\\spark-a590deca-5ece-43c1-9b42-3c5e7c2d2303
spyspark
e

2024-06-18 13:22:31,870 Answer received: !yro21
2024-06-18 13:22:31,870 Command to send: c
o21
getAbsolutePath
e

2024-06-18 13:22:31,873 Answer received: !ysC:\\Users\\FX504\\AppData\\Local\\Temp\\spark-a590deca-5ece-43c1-9b42-3c5e7c2d2303\\pyspark-696d9278-4bde-4e4c-8f72-f1b3dc286297
2024-06-18 13:22:31,873 Command to send: c
o16
get
sspark.python.profile
sfalse
e

2024-06-18 13:22:31,874 Answer received: !ysfalse
2024-06-18 13:22:31,874 Command to send: c
o16
get
sspark.python.profile.memory
sfalse
e

2024-06-18 13:22:31,874 Answer received: !ysfalse
2024-06-18 13:22:31,875 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:22:32,095 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:22:32,095 Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

2024-06-18 13:22:32,243 Answer received: !ym
2024-06-18 13:22:32,243 Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

2024-06-18 13:22:32,246 Answer received: !yro22
2024-06-18 13:22:32,246 Command to send: c
o22
isDefined
e

2024-06-18 13:22:32,247 Answer received: !ybfalse
2024-06-18 13:22:32,247 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:22:32,249 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:22:32,249 Command to send: c
o14
sc
e

2024-06-18 13:22:32,250 Answer received: !yro23
2024-06-18 13:22:32,250 Command to send: i
java.util.HashMap
e

2024-06-18 13:22:32,250 Answer received: !yao24
2024-06-18 13:22:32,251 Command to send: c
o24
put
sspark.master
slocal
e

2024-06-18 13:22:32,251 Answer received: !yn
2024-06-18 13:22:32,252 Command to send: c
o24
put
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:22:32,252 Answer received: !yn
2024-06-18 13:22:32,252 Command to send: i
org.apache.spark.sql.SparkSession
ro23
ro24
e

2024-06-18 13:22:32,392 Answer received: !yro25
2024-06-18 13:22:32,392 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:22:32,393 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:22:32,394 Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

2024-06-18 13:22:32,394 Answer received: !ym
2024-06-18 13:22:32,394 Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro25
e

2024-06-18 13:22:32,395 Answer received: !yv
2024-06-18 13:22:32,395 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:22:32,396 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:22:32,397 Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

2024-06-18 13:22:32,397 Answer received: !ym
2024-06-18 13:22:32,397 Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro25
e

2024-06-18 13:22:32,398 Answer received: !yv
2024-06-18 13:22:32,616 Command to send: m
d
o0
e

2024-06-18 13:22:32,617 Answer received: !yv
2024-06-18 13:22:32,617 Command to send: m
d
o6
e

2024-06-18 13:22:32,618 Answer received: !yv
2024-06-18 13:22:32,618 Command to send: m
d
o7
e

2024-06-18 13:22:32,618 Answer received: !yv
2024-06-18 13:22:32,618 Command to send: m
d
o8
e

2024-06-18 13:22:32,619 Answer received: !yv
2024-06-18 13:22:32,619 Command to send: m
d
o9
e

2024-06-18 13:22:32,619 Answer received: !yv
2024-06-18 13:22:32,619 Command to send: m
d
o10
e

2024-06-18 13:22:32,619 Answer received: !yv
2024-06-18 13:22:32,620 Command to send: m
d
o11
e

2024-06-18 13:22:32,620 Answer received: !yv
2024-06-18 13:22:32,620 Command to send: m
d
o12
e

2024-06-18 13:22:32,620 Answer received: !yv
2024-06-18 13:22:32,621 Command to send: m
d
o13
e

2024-06-18 13:22:32,621 Answer received: !yv
2024-06-18 13:22:32,621 Command to send: m
d
o15
e

2024-06-18 13:22:32,621 Answer received: !yv
2024-06-18 13:22:32,621 Command to send: m
d
o18
e

2024-06-18 13:22:32,622 Answer received: !yv
2024-06-18 13:22:32,622 Command to send: m
d
o24
e

2024-06-18 13:22:32,622 Answer received: !yv
2024-06-18 13:22:32,622 Command to send: m
d
o19
e

2024-06-18 13:22:32,623 Answer received: !yv
2024-06-18 13:22:32,623 Command to send: m
d
o20
e

2024-06-18 13:22:32,623 Answer received: !yv
2024-06-18 13:22:32,624 Command to send: m
d
o21
e

2024-06-18 13:22:32,624 Answer received: !yv
2024-06-18 13:22:32,624 Command to send: m
d
o22
e

2024-06-18 13:22:32,624 Answer received: !yv
2024-06-18 13:22:32,625 Command to send: m
d
o23
e

2024-06-18 13:22:32,625 Answer received: !yv
2024-06-18 13:22:42,245 Command to send: r
u
SparkConf
rj
e

2024-06-18 13:22:42,246 Answer received: !ycorg.apache.spark.SparkConf
2024-06-18 13:22:42,246 Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-06-18 13:22:42,248 Answer received: !yro26
2024-06-18 13:22:42,248 Command to send: c
o26
set
sspark.master
slocal
e

2024-06-18 13:22:42,249 Answer received: !yro27
2024-06-18 13:22:42,249 Command to send: c
o26
set
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:22:42,250 Answer received: !yro28
2024-06-18 13:22:42,250 Command to send: c
o26
getAll
e

2024-06-18 13:22:42,251 Answer received: !yto29
2024-06-18 13:22:42,251 Command to send: a
e
o29
e

2024-06-18 13:22:42,251 Answer received: !yi6
2024-06-18 13:22:42,252 Command to send: a
g
o29
i0
e

2024-06-18 13:22:42,252 Answer received: !yro30
2024-06-18 13:22:42,253 Command to send: c
o30
_1
e

2024-06-18 13:22:42,254 Answer received: !ysspark.master
2024-06-18 13:22:42,254 Command to send: c
o30
_2
e

2024-06-18 13:22:42,255 Answer received: !yslocal
2024-06-18 13:22:42,256 Command to send: a
e
o29
e

2024-06-18 13:22:42,256 Answer received: !yi6
2024-06-18 13:22:42,257 Command to send: a
g
o29
i1
e

2024-06-18 13:22:42,258 Answer received: !yro31
2024-06-18 13:22:42,258 Command to send: c
o31
_1
e

2024-06-18 13:22:42,258 Answer received: !ysspark.submit.pyFiles
2024-06-18 13:22:42,258 Command to send: c
o31
_2
e

2024-06-18 13:22:42,259 Answer received: !ys
2024-06-18 13:22:42,259 Command to send: a
e
o29
e

2024-06-18 13:22:42,259 Answer received: !yi6
2024-06-18 13:22:42,259 Command to send: a
g
o29
i2
e

2024-06-18 13:22:42,260 Answer received: !yro32
2024-06-18 13:22:42,260 Command to send: c
o32
_1
e

2024-06-18 13:22:42,260 Answer received: !ysspark.app.submitTime
2024-06-18 13:22:42,260 Command to send: c
o32
_2
e

2024-06-18 13:22:42,261 Answer received: !ys1718709747485
2024-06-18 13:22:42,261 Command to send: a
e
o29
e

2024-06-18 13:22:42,261 Answer received: !yi6
2024-06-18 13:22:42,261 Command to send: a
g
o29
i3
e

2024-06-18 13:22:42,262 Answer received: !yro33
2024-06-18 13:22:42,262 Command to send: c
o33
_1
e

2024-06-18 13:22:42,262 Answer received: !ysspark.submit.deployMode
2024-06-18 13:22:42,262 Command to send: c
o33
_2
e

2024-06-18 13:22:42,263 Answer received: !ysclient
2024-06-18 13:22:42,263 Command to send: a
e
o29
e

2024-06-18 13:22:42,263 Answer received: !yi6
2024-06-18 13:22:42,263 Command to send: a
g
o29
i4
e

2024-06-18 13:22:42,264 Answer received: !yro34
2024-06-18 13:22:42,264 Command to send: c
o34
_1
e

2024-06-18 13:22:42,265 Answer received: !ysspark.app.name
2024-06-18 13:22:42,265 Command to send: c
o34
_2
e

2024-06-18 13:22:42,265 Answer received: !ysFormatted zone loader
2024-06-18 13:22:42,265 Command to send: a
e
o29
e

2024-06-18 13:22:42,266 Answer received: !yi6
2024-06-18 13:22:42,266 Command to send: a
g
o29
i5
e

2024-06-18 13:22:42,266 Answer received: !yro35
2024-06-18 13:22:42,266 Command to send: c
o35
_1
e

2024-06-18 13:22:42,267 Answer received: !ysspark.ui.showConsoleProgress
2024-06-18 13:22:42,267 Command to send: c
o35
_2
e

2024-06-18 13:22:42,267 Answer received: !ystrue
2024-06-18 13:22:42,267 Command to send: a
e
o29
e

2024-06-18 13:22:42,268 Answer received: !yi6
2024-06-18 13:22:42,268 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:22:42,270 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:22:42,271 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:22:42,274 Answer received: !yro36
2024-06-18 13:22:42,274 Command to send: i
java.util.HashMap
e

2024-06-18 13:22:42,274 Answer received: !yao37
2024-06-18 13:22:42,274 Command to send: c
o37
put
sspark.master
slocal
e

2024-06-18 13:22:42,275 Answer received: !yn
2024-06-18 13:22:42,275 Command to send: c
o37
put
sspark.submit.pyFiles
s
e

2024-06-18 13:22:42,276 Answer received: !yn
2024-06-18 13:22:42,276 Command to send: c
o37
put
sspark.app.submitTime
s1718709747485
e

2024-06-18 13:22:42,276 Answer received: !yn
2024-06-18 13:22:42,276 Command to send: c
o37
put
sspark.submit.deployMode
sclient
e

2024-06-18 13:22:42,277 Answer received: !yn
2024-06-18 13:22:42,277 Command to send: c
o37
put
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:22:42,277 Answer received: !yn
2024-06-18 13:22:42,278 Command to send: c
o37
put
sspark.ui.showConsoleProgress
strue
e

2024-06-18 13:22:42,278 Answer received: !yn
2024-06-18 13:22:42,278 Command to send: c
o36
applyModifiableSettings
ro25
ro37
e

2024-06-18 13:22:42,631 Command to send: m
d
o27
e

2024-06-18 13:22:42,631 Answer received: !yv
2024-06-18 13:22:42,631 Command to send: m
d
o28
e

2024-06-18 13:22:42,631 Answer received: !yv
2024-06-18 13:22:42,632 Command to send: m
d
o29
e

2024-06-18 13:22:42,632 Answer received: !yv
2024-06-18 13:22:44,364 Answer received: !yv
2024-06-18 13:22:44,365 Spark sesion correctly initialized
2024-06-18 13:22:44,365 Command to send: c
o25
read
e

2024-06-18 13:22:44,389 Answer received: !yro38
2024-06-18 13:22:44,390 Command to send: c
o38
option
smultiline
strue
e

2024-06-18 13:22:44,418 Answer received: !yro39
2024-06-18 13:22:44,418 Command to send: c
o39
format
sparquet
e

2024-06-18 13:22:44,419 Answer received: !yro40
2024-06-18 13:22:44,420 Command to send: c
o40
load
smodel_temp/20240614-2304-renda_familiar
e

2024-06-18 13:22:44,634 Command to send: m
d
o37
e

2024-06-18 13:22:44,634 Answer received: !yv
2024-06-18 13:22:49,733 Answer received: !yro41
2024-06-18 13:22:49,734 Command to send: c
o25
read
e

2024-06-18 13:22:49,734 Answer received: !yro42
2024-06-18 13:22:49,735 Command to send: c
o42
option
smultiline
strue
e

2024-06-18 13:22:49,735 Answer received: !yro43
2024-06-18 13:22:49,735 Command to send: c
o43
format
sparquet
e

2024-06-18 13:22:49,736 Answer received: !yro44
2024-06-18 13:22:49,736 Command to send: c
o44
load
smodel_temp/20240614-2306-idealista
e

2024-06-18 13:22:50,215 Answer received: !yro45
2024-06-18 13:22:50,215 Command to send: c
o25
read
e

2024-06-18 13:22:50,216 Answer received: !yro46
2024-06-18 13:22:50,216 Command to send: c
o46
option
smultiline
strue
e

2024-06-18 13:22:50,217 Answer received: !yro47
2024-06-18 13:22:50,217 Command to send: c
o47
format
sparquet
e

2024-06-18 13:22:50,217 Answer received: !yro48
2024-06-18 13:22:50,218 Command to send: c
o48
load
smodel_temp/20240614-2308-lookup_renta_idealista
e

2024-06-18 13:22:50,499 Answer received: !yro49
2024-06-18 13:22:50,499 Command to send: c
o25
read
e

2024-06-18 13:22:50,500 Answer received: !yro50
2024-06-18 13:22:50,500 Command to send: c
o50
option
smultiline
strue
e

2024-06-18 13:22:50,501 Answer received: !yro51
2024-06-18 13:22:50,501 Command to send: c
o51
format
sparquet
e

2024-06-18 13:22:50,501 Answer received: !yro52
2024-06-18 13:22:50,501 Command to send: c
o52
load
smodel_temp/20240615-1328-hotels
e

2024-06-18 13:22:50,824 Answer received: !yro53
2024-06-18 13:25:23,815 Command to send: r
u
functions
rj
e

2024-06-18 13:25:23,992 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:23,992 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:24,026 Answer received: !ym
2024-06-18 13:25:24,027 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:25:24,179 Answer received: !yro54
2024-06-18 13:25:24,179 Command to send: r
u
functions
rj
e

2024-06-18 13:25:24,181 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:24,182 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:24,182 Answer received: !ym
2024-06-18 13:25:24,183 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:25:24,184 Answer received: !yro55
2024-06-18 13:25:24,184 Command to send: r
u
functions
rj
e

2024-06-18 13:25:24,186 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:24,186 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:24,186 Answer received: !ym
2024-06-18 13:25:24,186 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:25:24,187 Answer received: !yro56
2024-06-18 13:25:24,187 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:24,188 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:24,188 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:24,188 Answer received: !ym
2024-06-18 13:25:24,188 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:24,189 Answer received: !ylo57
2024-06-18 13:25:24,189 Command to send: c
o57
add
ro54
e

2024-06-18 13:25:24,191 Answer received: !ybtrue
2024-06-18 13:25:24,191 Command to send: c
o57
add
ro55
e

2024-06-18 13:25:24,192 Answer received: !ybtrue
2024-06-18 13:25:24,192 Command to send: c
o57
add
ro56
e

2024-06-18 13:25:24,193 Answer received: !ybtrue
2024-06-18 13:25:24,193 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro57
e

2024-06-18 13:25:24,193 Answer received: !yro58
2024-06-18 13:25:24,194 Command to send: c
o41
select
ro58
e

2024-06-18 13:25:24,353 Answer received: !yro59
2024-06-18 13:25:24,353 Command to send: c
o59
schema
e

2024-06-18 13:25:24,354 Answer received: !yro60
2024-06-18 13:25:24,354 Command to send: c
o60
treeString
e

2024-06-18 13:25:24,358 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:25:24,358 Command to send: c
o59
count
e

2024-06-18 13:25:24,711 Command to send: m
d
o30
e

2024-06-18 13:25:24,711 Answer received: !yv
2024-06-18 13:25:24,711 Command to send: m
d
o31
e

2024-06-18 13:25:24,712 Answer received: !yv
2024-06-18 13:25:24,712 Command to send: m
d
o32
e

2024-06-18 13:25:24,712 Answer received: !yv
2024-06-18 13:25:24,712 Command to send: m
d
o33
e

2024-06-18 13:25:24,713 Answer received: !yv
2024-06-18 13:25:24,713 Command to send: m
d
o34
e

2024-06-18 13:25:24,713 Answer received: !yv
2024-06-18 13:25:24,713 Command to send: m
d
o35
e

2024-06-18 13:25:24,713 Answer received: !yv
2024-06-18 13:25:24,713 Command to send: m
d
o36
e

2024-06-18 13:25:24,714 Answer received: !yv
2024-06-18 13:25:24,714 Command to send: m
d
o38
e

2024-06-18 13:25:24,714 Answer received: !yv
2024-06-18 13:25:24,714 Command to send: m
d
o39
e

2024-06-18 13:25:24,715 Answer received: !yv
2024-06-18 13:25:24,715 Command to send: m
d
o40
e

2024-06-18 13:25:24,715 Answer received: !yv
2024-06-18 13:25:24,715 Command to send: m
d
o42
e

2024-06-18 13:25:24,715 Answer received: !yv
2024-06-18 13:25:24,716 Command to send: m
d
o43
e

2024-06-18 13:25:24,716 Answer received: !yv
2024-06-18 13:25:24,716 Command to send: m
d
o44
e

2024-06-18 13:25:24,716 Answer received: !yv
2024-06-18 13:25:24,717 Command to send: m
d
o46
e

2024-06-18 13:25:24,717 Answer received: !yv
2024-06-18 13:25:24,717 Command to send: m
d
o47
e

2024-06-18 13:25:24,717 Answer received: !yv
2024-06-18 13:25:24,717 Command to send: m
d
o48
e

2024-06-18 13:25:24,718 Answer received: !yv
2024-06-18 13:25:24,718 Command to send: m
d
o50
e

2024-06-18 13:25:24,718 Answer received: !yv
2024-06-18 13:25:24,718 Command to send: m
d
o51
e

2024-06-18 13:25:24,718 Answer received: !yv
2024-06-18 13:25:24,718 Command to send: m
d
o52
e

2024-06-18 13:25:24,719 Answer received: !yv
2024-06-18 13:25:24,719 Command to send: m
d
o57
e

2024-06-18 13:25:24,719 Answer received: !yv
2024-06-18 13:25:27,738 Answer received: !yL811
2024-06-18 13:25:27,739 Command to send: c
o59
schema
e

2024-06-18 13:25:27,739 Answer received: !yro61
2024-06-18 13:25:27,739 Command to send: c
o61
json
e

2024-06-18 13:25:27,743 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:25:27,744 Command to send: c
o59
na
e

2024-06-18 13:25:27,750 Answer received: !yro62
2024-06-18 13:25:27,750 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:27,752 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:27,752 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:27,752 Answer received: !ym
2024-06-18 13:25:27,752 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:27,753 Answer received: !ylo63
2024-06-18 13:25:27,753 Command to send: c
o63
add
sNom_Districte
e

2024-06-18 13:25:27,754 Answer received: !ybtrue
2024-06-18 13:25:27,754 Command to send: c
o63
add
sCodi_Districte
e

2024-06-18 13:25:27,755 Answer received: !ybtrue
2024-06-18 13:25:27,755 Command to send: c
o63
add
sndex RFD Barcelona = 100
e

2024-06-18 13:25:27,756 Answer received: !ybtrue
2024-06-18 13:25:27,756 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro63
e

2024-06-18 13:25:27,756 Answer received: !yro64
2024-06-18 13:25:27,757 Command to send: c
o62
drop
i3
ro64
e

2024-06-18 13:25:27,771 Answer received: !yro65
2024-06-18 13:25:27,771 Command to send: c
o65
count
e

2024-06-18 13:25:28,722 Command to send: m
d
o63
e

2024-06-18 13:25:28,722 Answer received: !yv
2024-06-18 13:25:28,753 Answer received: !yL811
2024-06-18 13:25:28,755 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,757 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,758 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,758 Answer received: !ym
2024-06-18 13:25:28,758 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:25:28,759 Answer received: !yro66
2024-06-18 13:25:28,759 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,761 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,761 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,761 Answer received: !ym
2024-06-18 13:25:28,761 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:25:28,762 Answer received: !yro67
2024-06-18 13:25:28,762 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,763 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,764 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,764 Answer received: !ym
2024-06-18 13:25:28,764 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:25:28,765 Answer received: !yro68
2024-06-18 13:25:28,765 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,767 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,767 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,768 Answer received: !ym
2024-06-18 13:25:28,768 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:25:28,768 Answer received: !yro69
2024-06-18 13:25:28,769 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,770 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,770 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,771 Answer received: !ym
2024-06-18 13:25:28,771 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:25:28,771 Answer received: !yro70
2024-06-18 13:25:28,771 Command to send: r
u
functions
rj
e

2024-06-18 13:25:28,773 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:28,774 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:28,775 Answer received: !ym
2024-06-18 13:25:28,775 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:25:28,775 Answer received: !yro71
2024-06-18 13:25:28,775 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:28,776 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:28,776 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:28,777 Answer received: !ym
2024-06-18 13:25:28,777 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:28,777 Answer received: !ylo72
2024-06-18 13:25:28,777 Command to send: c
o72
add
ro66
e

2024-06-18 13:25:28,778 Answer received: !ybtrue
2024-06-18 13:25:28,778 Command to send: c
o72
add
ro67
e

2024-06-18 13:25:28,778 Answer received: !ybtrue
2024-06-18 13:25:28,779 Command to send: c
o72
add
ro68
e

2024-06-18 13:25:28,779 Answer received: !ybtrue
2024-06-18 13:25:28,779 Command to send: c
o72
add
ro69
e

2024-06-18 13:25:28,779 Answer received: !ybtrue
2024-06-18 13:25:28,780 Command to send: c
o72
add
ro70
e

2024-06-18 13:25:28,780 Answer received: !ybtrue
2024-06-18 13:25:28,780 Command to send: c
o72
add
ro71
e

2024-06-18 13:25:28,780 Answer received: !ybtrue
2024-06-18 13:25:28,781 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro72
e

2024-06-18 13:25:28,781 Answer received: !yro73
2024-06-18 13:25:28,781 Command to send: c
o45
select
ro73
e

2024-06-18 13:25:28,793 Answer received: !yro74
2024-06-18 13:25:28,793 Command to send: c
o74
schema
e

2024-06-18 13:25:28,793 Answer received: !yro75
2024-06-18 13:25:28,793 Command to send: c
o75
treeString
e

2024-06-18 13:25:28,793 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:25:28,793 Command to send: c
o74
count
e

2024-06-18 13:25:28,999 Answer received: !yL20189
2024-06-18 13:25:29,000 Command to send: c
o74
schema
e

2024-06-18 13:25:29,001 Answer received: !yro76
2024-06-18 13:25:29,001 Command to send: c
o76
json
e

2024-06-18 13:25:29,009 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:25:29,010 Command to send: c
o74
na
e

2024-06-18 13:25:29,018 Answer received: !yro77
2024-06-18 13:25:29,019 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:29,020 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:29,020 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:29,021 Answer received: !ym
2024-06-18 13:25:29,021 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:29,021 Answer received: !ylo78
2024-06-18 13:25:29,022 Command to send: c
o78
add
sdistrict
e

2024-06-18 13:25:29,022 Answer received: !ybtrue
2024-06-18 13:25:29,022 Command to send: c
o78
add
snumPhotos
e

2024-06-18 13:25:29,022 Answer received: !ybtrue
2024-06-18 13:25:29,023 Command to send: c
o78
add
sprice
e

2024-06-18 13:25:29,023 Answer received: !ybtrue
2024-06-18 13:25:29,023 Command to send: c
o78
add
sfloor
e

2024-06-18 13:25:29,024 Answer received: !ybtrue
2024-06-18 13:25:29,024 Command to send: c
o78
add
ssize
e

2024-06-18 13:25:29,025 Answer received: !ybtrue
2024-06-18 13:25:29,025 Command to send: c
o78
add
sbathrooms
e

2024-06-18 13:25:29,026 Answer received: !ybtrue
2024-06-18 13:25:29,026 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro78
e

2024-06-18 13:25:29,026 Answer received: !yro79
2024-06-18 13:25:29,026 Command to send: c
o77
drop
i6
ro79
e

2024-06-18 13:25:29,033 Answer received: !yro80
2024-06-18 13:25:29,033 Command to send: c
o80
count
e

2024-06-18 13:25:29,369 Answer received: !yL15545
2024-06-18 13:25:29,370 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,371 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,372 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,372 Answer received: !ym
2024-06-18 13:25:29,372 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:25:29,373 Answer received: !yro81
2024-06-18 13:25:29,373 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,375 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,375 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,375 Answer received: !ym
2024-06-18 13:25:29,375 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:25:29,376 Answer received: !yro82
2024-06-18 13:25:29,376 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:29,377 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:29,377 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:29,377 Answer received: !ym
2024-06-18 13:25:29,377 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:29,378 Answer received: !ylo83
2024-06-18 13:25:29,378 Command to send: c
o83
add
ro81
e

2024-06-18 13:25:29,378 Answer received: !ybtrue
2024-06-18 13:25:29,378 Command to send: c
o83
add
ro82
e

2024-06-18 13:25:29,379 Answer received: !ybtrue
2024-06-18 13:25:29,379 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro83
e

2024-06-18 13:25:29,379 Answer received: !yro84
2024-06-18 13:25:29,379 Command to send: c
o49
select
ro84
e

2024-06-18 13:25:29,386 Answer received: !yro85
2024-06-18 13:25:29,386 Command to send: c
o85
schema
e

2024-06-18 13:25:29,386 Answer received: !yro86
2024-06-18 13:25:29,387 Command to send: c
o86
treeString
e

2024-06-18 13:25:29,387 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:25:29,387 Command to send: c
o85
count
e

2024-06-18 13:25:29,543 Answer received: !yL113
2024-06-18 13:25:29,544 Command to send: c
o85
schema
e

2024-06-18 13:25:29,544 Answer received: !yro87
2024-06-18 13:25:29,544 Command to send: c
o87
json
e

2024-06-18 13:25:29,545 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:25:29,546 Command to send: c
o85
na
e

2024-06-18 13:25:29,549 Answer received: !yro88
2024-06-18 13:25:29,549 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:29,550 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:29,550 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:29,551 Answer received: !ym
2024-06-18 13:25:29,551 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:29,551 Answer received: !ylo89
2024-06-18 13:25:29,551 Command to send: c
o89
add
sdistrict
e

2024-06-18 13:25:29,552 Answer received: !ybtrue
2024-06-18 13:25:29,552 Command to send: c
o89
add
sdistrict_id
e

2024-06-18 13:25:29,552 Answer received: !ybtrue
2024-06-18 13:25:29,552 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro89
e

2024-06-18 13:25:29,553 Answer received: !yro90
2024-06-18 13:25:29,553 Command to send: c
o88
drop
i2
ro90
e

2024-06-18 13:25:29,557 Answer received: !yro91
2024-06-18 13:25:29,557 Command to send: c
o91
count
e

2024-06-18 13:25:29,723 Command to send: m
d
o72
e

2024-06-18 13:25:29,723 Answer received: !yv
2024-06-18 13:25:29,723 Command to send: m
d
o78
e

2024-06-18 13:25:29,724 Answer received: !yv
2024-06-18 13:25:29,724 Command to send: m
d
o83
e

2024-06-18 13:25:29,724 Answer received: !yv
2024-06-18 13:25:29,724 Command to send: m
d
o89
e

2024-06-18 13:25:29,724 Answer received: !yv
2024-06-18 13:25:29,804 Answer received: !yL113
2024-06-18 13:25:29,804 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,808 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,808 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,808 Answer received: !ym
2024-06-18 13:25:29,808 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:25:29,809 Answer received: !yro92
2024-06-18 13:25:29,809 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,810 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,811 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,811 Answer received: !ym
2024-06-18 13:25:29,811 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:25:29,812 Answer received: !yro93
2024-06-18 13:25:29,812 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,814 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,814 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,814 Answer received: !ym
2024-06-18 13:25:29,815 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:25:29,815 Answer received: !yro94
2024-06-18 13:25:29,815 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,817 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,817 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,817 Answer received: !ym
2024-06-18 13:25:29,817 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:25:29,818 Answer received: !yro95
2024-06-18 13:25:29,818 Command to send: r
u
functions
rj
e

2024-06-18 13:25:29,819 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:29,820 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:29,820 Answer received: !ym
2024-06-18 13:25:29,820 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:25:29,821 Answer received: !yro96
2024-06-18 13:25:29,821 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:29,822 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:29,822 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:29,823 Answer received: !ym
2024-06-18 13:25:29,823 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:29,823 Answer received: !ylo97
2024-06-18 13:25:29,823 Command to send: c
o97
add
ro92
e

2024-06-18 13:25:29,824 Answer received: !ybtrue
2024-06-18 13:25:29,824 Command to send: c
o97
add
ro93
e

2024-06-18 13:25:29,824 Answer received: !ybtrue
2024-06-18 13:25:29,824 Command to send: c
o97
add
ro94
e

2024-06-18 13:25:29,825 Answer received: !ybtrue
2024-06-18 13:25:29,825 Command to send: c
o97
add
ro95
e

2024-06-18 13:25:29,825 Answer received: !ybtrue
2024-06-18 13:25:29,825 Command to send: c
o97
add
ro96
e

2024-06-18 13:25:29,825 Answer received: !ybtrue
2024-06-18 13:25:29,825 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro97
e

2024-06-18 13:25:29,826 Answer received: !yro98
2024-06-18 13:25:29,826 Command to send: c
o53
select
ro98
e

2024-06-18 13:25:29,834 Answer received: !yro99
2024-06-18 13:25:29,834 Command to send: c
o99
schema
e

2024-06-18 13:25:29,835 Answer received: !yro100
2024-06-18 13:25:29,835 Command to send: c
o100
treeString
e

2024-06-18 13:25:29,835 Answer received: !ysroot\n |-- addresses_district_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- secondary_filters_name: string (nullable = true)\n |-- geo_epgs_4326_lat: string (nullable = true)\n |-- geo_epgs_4326_lon: string (nullable = true)\n
2024-06-18 13:25:29,835 Command to send: c
o99
count
e

2024-06-18 13:25:29,991 Answer received: !yL444
2024-06-18 13:25:29,992 Command to send: c
o99
schema
e

2024-06-18 13:25:29,992 Answer received: !yro101
2024-06-18 13:25:29,992 Command to send: c
o101
json
e

2024-06-18 13:25:29,993 Answer received: !ys{"type":"struct","fields":[{"name":"addresses_district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"name","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"secondary_filters_name","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"geo_epgs_4326_lat","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"geo_epgs_4326_lon","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}}]}
2024-06-18 13:25:29,994 Command to send: c
o99
na
e

2024-06-18 13:25:29,997 Answer received: !yro102
2024-06-18 13:25:29,997 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:29,999 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:29,999 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:29,999 Answer received: !ym
2024-06-18 13:25:29,999 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,000 Answer received: !ylo103
2024-06-18 13:25:30,000 Command to send: c
o103
add
saddresses_district_id
e

2024-06-18 13:25:30,000 Answer received: !ybtrue
2024-06-18 13:25:30,001 Command to send: c
o103
add
sname
e

2024-06-18 13:25:30,001 Answer received: !ybtrue
2024-06-18 13:25:30,001 Command to send: c
o103
add
ssecondary_filters_name
e

2024-06-18 13:25:30,001 Answer received: !ybtrue
2024-06-18 13:25:30,002 Command to send: c
o103
add
sgeo_epgs_4326_lat
e

2024-06-18 13:25:30,002 Answer received: !ybtrue
2024-06-18 13:25:30,002 Command to send: c
o103
add
sgeo_epgs_4326_lon
e

2024-06-18 13:25:30,002 Answer received: !ybtrue
2024-06-18 13:25:30,003 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro103
e

2024-06-18 13:25:30,003 Answer received: !yro104
2024-06-18 13:25:30,003 Command to send: c
o102
drop
i5
ro104
e

2024-06-18 13:25:30,008 Answer received: !yro105
2024-06-18 13:25:30,008 Command to send: c
o105
count
e

2024-06-18 13:25:30,303 Answer received: !yL443
2024-06-18 13:25:30,304 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,306 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,306 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,307 Answer received: !ym
2024-06-18 13:25:30,307 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:25:30,308 Answer received: !yro106
2024-06-18 13:25:30,308 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,309 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,309 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:25:30,310 Answer received: !ym
2024-06-18 13:25:30,310 Command to send: c
z:org.apache.spark.sql.functions
split
ro106
s 
i-1
e

2024-06-18 13:25:30,312 Answer received: !yro107
2024-06-18 13:25:30,313 Command to send: c
o107
cast
sarray<int>
e

2024-06-18 13:25:30,360 Answer received: !yro108
2024-06-18 13:25:30,360 Command to send: c
o105
withColumn
sstars
ro108
e

2024-06-18 13:25:30,392 Answer received: !yro109
2024-06-18 13:25:30,392 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,395 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,395 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,395 Answer received: !ym
2024-06-18 13:25:30,396 Command to send: c
z:org.apache.spark.sql.functions
col
sstars
e

2024-06-18 13:25:30,396 Answer received: !yro110
2024-06-18 13:25:30,396 Command to send: c
o110
apply
i1
e

2024-06-18 13:25:30,398 Answer received: !yro111
2024-06-18 13:25:30,398 Command to send: c
o109
withColumn
sstars
ro111
e

2024-06-18 13:25:30,415 Answer received: !yro112
2024-06-18 13:25:30,415 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,416 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,416 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,417 Answer received: !ym
2024-06-18 13:25:30,417 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,417 Answer received: !ylo113
2024-06-18 13:25:30,417 Command to send: c
o113
add
ssecondary_filters_name
e

2024-06-18 13:25:30,418 Answer received: !ybtrue
2024-06-18 13:25:30,418 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro113
e

2024-06-18 13:25:30,418 Answer received: !yro114
2024-06-18 13:25:30,418 Command to send: c
o112
drop
ro114
e

2024-06-18 13:25:30,428 Answer received: !yro115
2024-06-18 13:25:30,428 Command to send: c
o115
apply
saddresses_district_id
e

2024-06-18 13:25:30,432 Answer received: !yro116
2024-06-18 13:25:30,432 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,434 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,434 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,435 Answer received: !ym
2024-06-18 13:25:30,435 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,435 Answer received: !yro117
2024-06-18 13:25:30,435 Command to send: c
o117
isDefined
e

2024-06-18 13:25:30,436 Answer received: !ybtrue
2024-06-18 13:25:30,436 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,437 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,437 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,438 Answer received: !ym
2024-06-18 13:25:30,438 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,438 Answer received: !yro118
2024-06-18 13:25:30,438 Command to send: c
o118
get
e

2024-06-18 13:25:30,439 Answer received: !yro119
2024-06-18 13:25:30,439 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:25:30,441 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:25:30,441 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:25:30,442 Answer received: !yro120
2024-06-18 13:25:30,442 Command to send: i
java.util.HashMap
e

2024-06-18 13:25:30,442 Answer received: !yao121
2024-06-18 13:25:30,442 Command to send: c
o120
applyModifiableSettings
ro119
ro121
e

2024-06-18 13:25:30,443 Answer received: !yv
2024-06-18 13:25:30,443 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:25:30,444 Answer received: !yro122
2024-06-18 13:25:30,444 Command to send: c
o116
cast
ro122
e

2024-06-18 13:25:30,444 Answer received: !yro123
2024-06-18 13:25:30,444 Command to send: c
o115
withColumn
saddresses_district_id
ro123
e

2024-06-18 13:25:30,454 Answer received: !yro124
2024-06-18 13:25:30,455 Command to send: c
o115
apply
sgeo_epgs_4326_lat
e

2024-06-18 13:25:30,456 Answer received: !yro125
2024-06-18 13:25:30,456 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,458 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,458 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,459 Answer received: !ym
2024-06-18 13:25:30,459 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,459 Answer received: !yro126
2024-06-18 13:25:30,460 Command to send: c
o126
isDefined
e

2024-06-18 13:25:30,460 Answer received: !ybtrue
2024-06-18 13:25:30,460 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,461 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,462 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,462 Answer received: !ym
2024-06-18 13:25:30,462 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,462 Answer received: !yro127
2024-06-18 13:25:30,463 Command to send: c
o127
get
e

2024-06-18 13:25:30,463 Answer received: !yro128
2024-06-18 13:25:30,463 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:25:30,465 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:25:30,466 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:25:30,466 Answer received: !yro129
2024-06-18 13:25:30,466 Command to send: i
java.util.HashMap
e

2024-06-18 13:25:30,466 Answer received: !yao130
2024-06-18 13:25:30,466 Command to send: c
o129
applyModifiableSettings
ro128
ro130
e

2024-06-18 13:25:30,467 Answer received: !yv
2024-06-18 13:25:30,467 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:25:30,468 Answer received: !yro131
2024-06-18 13:25:30,468 Command to send: c
o125
cast
ro131
e

2024-06-18 13:25:30,468 Answer received: !yro132
2024-06-18 13:25:30,468 Command to send: c
o124
withColumn
sgeo_epgs_4326_lat
ro132
e

2024-06-18 13:25:30,477 Answer received: !yro133
2024-06-18 13:25:30,477 Command to send: c
o115
apply
sgeo_epgs_4326_lon
e

2024-06-18 13:25:30,478 Answer received: !yro134
2024-06-18 13:25:30,478 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,479 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,479 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,480 Answer received: !ym
2024-06-18 13:25:30,480 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,480 Answer received: !yro135
2024-06-18 13:25:30,480 Command to send: c
o135
isDefined
e

2024-06-18 13:25:30,481 Answer received: !ybtrue
2024-06-18 13:25:30,481 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,483 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,483 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,483 Answer received: !ym
2024-06-18 13:25:30,483 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,484 Answer received: !yro136
2024-06-18 13:25:30,484 Command to send: c
o136
get
e

2024-06-18 13:25:30,484 Answer received: !yro137
2024-06-18 13:25:30,484 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:25:30,486 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:25:30,486 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:25:30,486 Answer received: !yro138
2024-06-18 13:25:30,486 Command to send: i
java.util.HashMap
e

2024-06-18 13:25:30,486 Answer received: !yao139
2024-06-18 13:25:30,487 Command to send: c
o138
applyModifiableSettings
ro137
ro139
e

2024-06-18 13:25:30,487 Answer received: !yv
2024-06-18 13:25:30,487 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:25:30,488 Answer received: !yro140
2024-06-18 13:25:30,488 Command to send: c
o134
cast
ro140
e

2024-06-18 13:25:30,488 Answer received: !yro141
2024-06-18 13:25:30,488 Command to send: c
o133
withColumn
sgeo_epgs_4326_lon
ro141
e

2024-06-18 13:25:30,497 Answer received: !yro142
2024-06-18 13:25:30,499 Command to send: c
o80
apply
sfloor
e

2024-06-18 13:25:30,500 Answer received: !yro143
2024-06-18 13:25:30,500 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,502 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,502 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,502 Answer received: !ym
2024-06-18 13:25:30,502 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,503 Answer received: !yro144
2024-06-18 13:25:30,503 Command to send: c
o144
isDefined
e

2024-06-18 13:25:30,503 Answer received: !ybtrue
2024-06-18 13:25:30,503 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:25:30,505 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:25:30,505 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,505 Answer received: !ym
2024-06-18 13:25:30,505 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:25:30,505 Answer received: !yro145
2024-06-18 13:25:30,506 Command to send: c
o145
get
e

2024-06-18 13:25:30,506 Answer received: !yro146
2024-06-18 13:25:30,506 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:25:30,508 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:25:30,508 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:25:30,508 Answer received: !yro147
2024-06-18 13:25:30,508 Command to send: i
java.util.HashMap
e

2024-06-18 13:25:30,509 Answer received: !yao148
2024-06-18 13:25:30,509 Command to send: c
o147
applyModifiableSettings
ro146
ro148
e

2024-06-18 13:25:30,509 Answer received: !yv
2024-06-18 13:25:30,509 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:25:30,510 Answer received: !yro149
2024-06-18 13:25:30,510 Command to send: c
o143
cast
ro149
e

2024-06-18 13:25:30,510 Answer received: !yro150
2024-06-18 13:25:30,510 Command to send: c
o80
withColumn
sfloor
ro150
e

2024-06-18 13:25:30,518 Answer received: !yro151
2024-06-18 13:25:30,519 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,521 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,521 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,521 Answer received: !ym
2024-06-18 13:25:30,521 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:25:30,522 Answer received: !yro152
2024-06-18 13:25:30,522 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,523 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,523 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:25:30,524 Answer received: !ym
2024-06-18 13:25:30,524 Command to send: c
z:org.apache.spark.sql.functions
split
ro152
s"
i-1
e

2024-06-18 13:25:30,524 Answer received: !yro153
2024-06-18 13:25:30,525 Command to send: c
o65
withColumn
sNom_Districte
ro153
e

2024-06-18 13:25:30,530 Answer received: !yro154
2024-06-18 13:25:30,531 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,532 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,533 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,533 Answer received: !ym
2024-06-18 13:25:30,533 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:25:30,533 Answer received: !yro155
2024-06-18 13:25:30,534 Command to send: c
o155
apply
i1
e

2024-06-18 13:25:30,534 Answer received: !yro156
2024-06-18 13:25:30,534 Command to send: c
o154
withColumn
sNom_Districte
ro156
e

2024-06-18 13:25:30,542 Answer received: !yro157
2024-06-18 13:25:30,542 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,545 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,545 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,546 Answer received: !ym
2024-06-18 13:25:30,546 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:25:30,547 Answer received: !yro158
2024-06-18 13:25:30,547 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,550 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,551 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:25:30,551 Answer received: !ym
2024-06-18 13:25:30,552 Command to send: c
z:org.apache.spark.sql.functions
split
ro158
s"
i-1
e

2024-06-18 13:25:30,552 Answer received: !yro159
2024-06-18 13:25:30,553 Command to send: c
o157
withColumn
sndex RFD Barcelona = 100
ro159
e

2024-06-18 13:25:30,561 Answer received: !yro160
2024-06-18 13:25:30,561 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,563 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,563 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,564 Answer received: !ym
2024-06-18 13:25:30,565 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:25:30,565 Answer received: !yro161
2024-06-18 13:25:30,566 Command to send: c
o161
apply
i1
e

2024-06-18 13:25:30,566 Answer received: !yro162
2024-06-18 13:25:30,566 Command to send: c
o160
withColumn
sndex RFD Barcelona = 100
ro162
e

2024-06-18 13:25:30,574 Answer received: !yro163
2024-06-18 13:25:30,574 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,576 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,576 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,577 Answer received: !ym
2024-06-18 13:25:30,577 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:25:30,577 Answer received: !yro164
2024-06-18 13:25:30,577 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,578 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,578 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,579 Answer received: !ym
2024-06-18 13:25:30,579 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,579 Answer received: !ylo165
2024-06-18 13:25:30,579 Command to send: c
o165
add
ro164
e

2024-06-18 13:25:30,580 Answer received: !ybtrue
2024-06-18 13:25:30,580 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro165
e

2024-06-18 13:25:30,581 Answer received: !yro166
2024-06-18 13:25:30,581 Command to send: c
o142
groupBy
ro166
e

2024-06-18 13:25:30,583 Answer received: !yro167
2024-06-18 13:25:30,584 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,585 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,585 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,586 Answer received: !ym
2024-06-18 13:25:30,586 Command to send: c
z:org.apache.spark.sql.functions
col
sstars
e

2024-06-18 13:25:30,586 Answer received: !yro168
2024-06-18 13:25:30,587 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,589 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,589 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:25:30,589 Answer received: !ym
2024-06-18 13:25:30,590 Command to send: c
z:org.apache.spark.sql.functions
avg
ro168
e

2024-06-18 13:25:30,593 Answer received: !yro169
2024-06-18 13:25:30,593 Command to send: c
o169
as
sAvg_stars
e

2024-06-18 13:25:30,594 Answer received: !yro170
2024-06-18 13:25:30,594 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,595 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,596 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,596 Answer received: !ym
2024-06-18 13:25:30,596 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:25:30,597 Answer received: !yro171
2024-06-18 13:25:30,598 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,599 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,599 Command to send: r
m
org.apache.spark.sql.functions
count
e

2024-06-18 13:25:30,599 Answer received: !ym
2024-06-18 13:25:30,600 Command to send: c
z:org.apache.spark.sql.functions
count
ro171
e

2024-06-18 13:25:30,600 Answer received: !yro172
2024-06-18 13:25:30,601 Command to send: c
o172
as
sN_hotels
e

2024-06-18 13:25:30,601 Answer received: !yro173
2024-06-18 13:25:30,601 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,602 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,602 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,603 Answer received: !ym
2024-06-18 13:25:30,603 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:25:30,603 Answer received: !yro174
2024-06-18 13:25:30,603 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,604 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,604 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:25:30,605 Answer received: !ym
2024-06-18 13:25:30,605 Command to send: c
z:org.apache.spark.sql.functions
avg
ro174
e

2024-06-18 13:25:30,606 Answer received: !yro175
2024-06-18 13:25:30,606 Command to send: c
o175
as
sAvg_lat
e

2024-06-18 13:25:30,606 Answer received: !yro176
2024-06-18 13:25:30,606 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,608 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,608 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,608 Answer received: !ym
2024-06-18 13:25:30,608 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:25:30,609 Answer received: !yro177
2024-06-18 13:25:30,609 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,610 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,610 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:25:30,610 Answer received: !ym
2024-06-18 13:25:30,610 Command to send: c
z:org.apache.spark.sql.functions
avg
ro177
e

2024-06-18 13:25:30,611 Answer received: !yro178
2024-06-18 13:25:30,611 Command to send: c
o178
as
sAvg_long
e

2024-06-18 13:25:30,611 Answer received: !yro179
2024-06-18 13:25:30,611 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,612 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,612 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,612 Answer received: !ym
2024-06-18 13:25:30,613 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,613 Answer received: !ylo180
2024-06-18 13:25:30,613 Command to send: c
o180
add
ro173
e

2024-06-18 13:25:30,614 Answer received: !ybtrue
2024-06-18 13:25:30,614 Command to send: c
o180
add
ro176
e

2024-06-18 13:25:30,615 Answer received: !ybtrue
2024-06-18 13:25:30,615 Command to send: c
o180
add
ro179
e

2024-06-18 13:25:30,615 Answer received: !ybtrue
2024-06-18 13:25:30,615 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro180
e

2024-06-18 13:25:30,616 Answer received: !yro181
2024-06-18 13:25:30,616 Command to send: c
o167
agg
ro170
ro181
e

2024-06-18 13:25:30,689 Answer received: !yro182
2024-06-18 13:25:30,690 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,691 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,691 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,692 Answer received: !ym
2024-06-18 13:25:30,692 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:25:30,692 Answer received: !yro183
2024-06-18 13:25:30,693 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,694 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,694 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,694 Answer received: !ym
2024-06-18 13:25:30,694 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:25:30,695 Answer received: !yro184
2024-06-18 13:25:30,695 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,695 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,695 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,696 Answer received: !ym
2024-06-18 13:25:30,696 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,696 Answer received: !ylo185
2024-06-18 13:25:30,696 Command to send: c
o185
add
ro183
e

2024-06-18 13:25:30,696 Answer received: !ybtrue
2024-06-18 13:25:30,697 Command to send: c
o185
add
ro184
e

2024-06-18 13:25:30,697 Answer received: !ybtrue
2024-06-18 13:25:30,697 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro185
e

2024-06-18 13:25:30,698 Answer received: !yro186
2024-06-18 13:25:30,698 Command to send: c
o163
groupBy
ro186
e

2024-06-18 13:25:30,700 Answer received: !yro187
2024-06-18 13:25:30,700 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,701 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,702 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:25:30,702 Answer received: !ym
2024-06-18 13:25:30,702 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:25:30,703 Answer received: !yro188
2024-06-18 13:25:30,703 Command to send: r
u
functions
rj
e

2024-06-18 13:25:30,704 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:25:30,704 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:25:30,705 Answer received: !ym
2024-06-18 13:25:30,705 Command to send: c
z:org.apache.spark.sql.functions
avg
ro188
e

2024-06-18 13:25:30,705 Answer received: !yro189
2024-06-18 13:25:30,706 Command to send: c
o189
as
sAvg_Index_RFD
e

2024-06-18 13:25:30,706 Answer received: !yro190
2024-06-18 13:25:30,706 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,707 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,707 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,708 Answer received: !ym
2024-06-18 13:25:30,708 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,708 Answer received: !ylo191
2024-06-18 13:25:30,708 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro191
e

2024-06-18 13:25:30,709 Answer received: !yro192
2024-06-18 13:25:30,709 Command to send: c
o187
agg
ro190
ro192
e

2024-06-18 13:25:30,725 Command to send: m
d
o97
e

2024-06-18 13:25:30,725 Answer received: !yv
2024-06-18 13:25:30,725 Command to send: m
d
o103
e

2024-06-18 13:25:30,726 Answer received: !yv
2024-06-18 13:25:30,726 Command to send: m
d
o113
e

2024-06-18 13:25:30,726 Answer received: !yv
2024-06-18 13:25:30,726 Command to send: m
d
o121
e

2024-06-18 13:25:30,727 Answer received: !yv
2024-06-18 13:25:30,727 Command to send: m
d
o130
e

2024-06-18 13:25:30,727 Answer received: !yv
2024-06-18 13:25:30,727 Command to send: m
d
o139
e

2024-06-18 13:25:30,727 Answer received: !yv
2024-06-18 13:25:30,728 Command to send: m
d
o45
e

2024-06-18 13:25:30,728 Answer received: !yv
2024-06-18 13:25:30,728 Command to send: m
d
o49
e

2024-06-18 13:25:30,728 Answer received: !yv
2024-06-18 13:25:30,728 Command to send: m
d
o53
e

2024-06-18 13:25:30,729 Answer received: !yv
2024-06-18 13:25:30,729 Command to send: m
d
o41
e

2024-06-18 13:25:30,729 Answer received: !yv
2024-06-18 13:25:30,729 Command to send: m
d
o54
e

2024-06-18 13:25:30,730 Answer received: !yv
2024-06-18 13:25:30,730 Command to send: m
d
o55
e

2024-06-18 13:25:30,730 Answer received: !yv
2024-06-18 13:25:30,730 Command to send: m
d
o56
e

2024-06-18 13:25:30,731 Answer received: !yv
2024-06-18 13:25:30,731 Command to send: m
d
o58
e

2024-06-18 13:25:30,731 Answer received: !yro193
2024-06-18 13:25:30,732 Command to send: c
o182
schema
e

2024-06-18 13:25:30,732 Answer received: !yv
2024-06-18 13:25:30,732 Answer received: !yro194
2024-06-18 13:25:30,732 Command to send: m
d
o59
e

2024-06-18 13:25:30,732 Command to send: c
o194
json
e

2024-06-18 13:25:30,732 Answer received: !yv
2024-06-18 13:25:30,733 Command to send: m
d
o60
e

2024-06-18 13:25:30,733 Answer received: !yv
2024-06-18 13:25:30,733 Command to send: m
d
o61
e

2024-06-18 13:25:30,733 Answer received: !ys{"type":"struct","fields":[{"name":"addresses_district_id","type":"integer","nullable":true,"metadata":{}},{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:25:30,733 Command to send: c
o182
apply
saddresses_district_id
e

2024-06-18 13:25:30,733 Answer received: !yv
2024-06-18 13:25:30,734 Command to send: m
d
o62
e

2024-06-18 13:25:30,734 Answer received: !yv
2024-06-18 13:25:30,734 Command to send: m
d
o64
e

2024-06-18 13:25:30,734 Answer received: !yv
2024-06-18 13:25:30,734 Command to send: m
d
o66
e

2024-06-18 13:25:30,734 Answer received: !yro195
2024-06-18 13:25:30,735 Command to send: c
o193
schema
e

2024-06-18 13:25:30,735 Answer received: !yv
2024-06-18 13:25:30,735 Command to send: m
d
o67
e

2024-06-18 13:25:30,735 Answer received: !yro196
2024-06-18 13:25:30,735 Answer received: !yv
2024-06-18 13:25:30,735 Command to send: c
o196
json
e

2024-06-18 13:25:30,735 Command to send: m
d
o68
e

2024-06-18 13:25:30,736 Answer received: !yv
2024-06-18 13:25:30,736 Command to send: m
d
o69
e

2024-06-18 13:25:30,736 Answer received: !yv
2024-06-18 13:25:30,736 Command to send: m
d
o70
e

2024-06-18 13:25:30,736 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:25:30,736 Answer received: !yv
2024-06-18 13:25:30,737 Command to send: c
o193
apply
sCodi_Districte
e

2024-06-18 13:25:30,737 Command to send: m
d
o71
e

2024-06-18 13:25:30,737 Answer received: !yv
2024-06-18 13:25:30,737 Command to send: m
d
o73
e

2024-06-18 13:25:30,738 Answer received: !yv
2024-06-18 13:25:30,738 Answer received: !yro197
2024-06-18 13:25:30,738 Command to send: m
d
o74
e

2024-06-18 13:25:30,738 Command to send: c
o195
equalTo
ro197
e

2024-06-18 13:25:30,738 Answer received: !yv
2024-06-18 13:25:30,738 Command to send: m
d
o75
e

2024-06-18 13:25:30,739 Answer received: !yv
2024-06-18 13:25:30,739 Command to send: m
d
o76
e

2024-06-18 13:25:30,739 Answer received: !yro198
2024-06-18 13:25:30,740 Command to send: c
o182
join
ro193
ro198
sinner
e

2024-06-18 13:25:30,740 Answer received: !yv
2024-06-18 13:25:30,740 Command to send: m
d
o77
e

2024-06-18 13:25:30,740 Answer received: !yv
2024-06-18 13:25:30,740 Command to send: m
d
o79
e

2024-06-18 13:25:30,741 Answer received: !yv
2024-06-18 13:25:30,741 Command to send: m
d
o81
e

2024-06-18 13:25:30,741 Answer received: !yv
2024-06-18 13:25:30,741 Command to send: m
d
o82
e

2024-06-18 13:25:30,741 Answer received: !yv
2024-06-18 13:25:30,741 Command to send: m
d
o84
e

2024-06-18 13:25:30,742 Answer received: !yv
2024-06-18 13:25:30,742 Command to send: m
d
o85
e

2024-06-18 13:25:30,742 Answer received: !yv
2024-06-18 13:25:30,742 Command to send: m
d
o86
e

2024-06-18 13:25:30,742 Answer received: !yv
2024-06-18 13:25:30,742 Command to send: m
d
o87
e

2024-06-18 13:25:30,743 Answer received: !yv
2024-06-18 13:25:30,743 Command to send: m
d
o88
e

2024-06-18 13:25:30,743 Answer received: !yv
2024-06-18 13:25:30,743 Command to send: m
d
o90
e

2024-06-18 13:25:30,743 Answer received: !yv
2024-06-18 13:25:30,743 Command to send: m
d
o92
e

2024-06-18 13:25:30,743 Answer received: !yv
2024-06-18 13:25:30,744 Command to send: m
d
o93
e

2024-06-18 13:25:30,744 Answer received: !yv
2024-06-18 13:25:30,744 Command to send: m
d
o94
e

2024-06-18 13:25:30,744 Answer received: !yv
2024-06-18 13:25:30,744 Command to send: m
d
o95
e

2024-06-18 13:25:30,744 Answer received: !yv
2024-06-18 13:25:30,745 Command to send: m
d
o96
e

2024-06-18 13:25:30,745 Answer received: !yv
2024-06-18 13:25:30,745 Command to send: m
d
o98
e

2024-06-18 13:25:30,745 Answer received: !yv
2024-06-18 13:25:30,745 Command to send: m
d
o99
e

2024-06-18 13:25:30,746 Answer received: !yv
2024-06-18 13:25:30,746 Command to send: m
d
o100
e

2024-06-18 13:25:30,746 Answer received: !yv
2024-06-18 13:25:30,746 Command to send: m
d
o101
e

2024-06-18 13:25:30,746 Answer received: !yv
2024-06-18 13:25:30,746 Command to send: m
d
o102
e

2024-06-18 13:25:30,747 Answer received: !yv
2024-06-18 13:25:30,747 Command to send: m
d
o104
e

2024-06-18 13:25:30,747 Answer received: !yv
2024-06-18 13:25:30,748 Command to send: m
d
o106
e

2024-06-18 13:25:30,748 Answer received: !yv
2024-06-18 13:25:30,748 Command to send: m
d
o107
e

2024-06-18 13:25:30,748 Answer received: !yv
2024-06-18 13:25:30,748 Command to send: m
d
o108
e

2024-06-18 13:25:30,749 Answer received: !yv
2024-06-18 13:25:30,749 Command to send: m
d
o109
e

2024-06-18 13:25:30,749 Answer received: !yv
2024-06-18 13:25:30,749 Command to send: m
d
o110
e

2024-06-18 13:25:30,749 Answer received: !yv
2024-06-18 13:25:30,749 Command to send: m
d
o111
e

2024-06-18 13:25:30,750 Answer received: !yv
2024-06-18 13:25:30,750 Command to send: m
d
o112
e

2024-06-18 13:25:30,750 Answer received: !yv
2024-06-18 13:25:30,750 Command to send: m
d
o114
e

2024-06-18 13:25:30,750 Answer received: !yv
2024-06-18 13:25:30,750 Command to send: m
d
o115
e

2024-06-18 13:25:30,750 Answer received: !yv
2024-06-18 13:25:30,751 Command to send: m
d
o116
e

2024-06-18 13:25:30,751 Answer received: !yv
2024-06-18 13:25:30,751 Command to send: m
d
o117
e

2024-06-18 13:25:30,751 Answer received: !yv
2024-06-18 13:25:30,751 Command to send: m
d
o118
e

2024-06-18 13:25:30,751 Answer received: !yv
2024-06-18 13:25:30,751 Command to send: m
d
o119
e

2024-06-18 13:25:30,752 Answer received: !yv
2024-06-18 13:25:30,752 Command to send: m
d
o120
e

2024-06-18 13:25:30,752 Answer received: !yv
2024-06-18 13:25:30,752 Command to send: m
d
o122
e

2024-06-18 13:25:30,752 Answer received: !yv
2024-06-18 13:25:30,752 Command to send: m
d
o123
e

2024-06-18 13:25:30,753 Answer received: !yv
2024-06-18 13:25:30,753 Command to send: m
d
o124
e

2024-06-18 13:25:30,753 Answer received: !yv
2024-06-18 13:25:30,753 Command to send: m
d
o125
e

2024-06-18 13:25:30,753 Answer received: !yv
2024-06-18 13:25:30,753 Command to send: m
d
o126
e

2024-06-18 13:25:30,753 Answer received: !yv
2024-06-18 13:25:30,754 Command to send: m
d
o127
e

2024-06-18 13:25:30,754 Answer received: !yv
2024-06-18 13:25:30,754 Command to send: m
d
o128
e

2024-06-18 13:25:30,754 Answer received: !yv
2024-06-18 13:25:30,754 Command to send: m
d
o129
e

2024-06-18 13:25:30,754 Answer received: !yv
2024-06-18 13:25:30,754 Command to send: m
d
o131
e

2024-06-18 13:25:30,755 Answer received: !yv
2024-06-18 13:25:30,755 Command to send: m
d
o132
e

2024-06-18 13:25:30,755 Answer received: !yv
2024-06-18 13:25:30,755 Command to send: m
d
o133
e

2024-06-18 13:25:30,756 Answer received: !yv
2024-06-18 13:25:30,756 Command to send: m
d
o134
e

2024-06-18 13:25:30,756 Answer received: !yv
2024-06-18 13:25:30,757 Command to send: m
d
o135
e

2024-06-18 13:25:30,757 Answer received: !yv
2024-06-18 13:25:30,757 Command to send: m
d
o136
e

2024-06-18 13:25:30,757 Answer received: !yv
2024-06-18 13:25:30,758 Command to send: m
d
o138
e

2024-06-18 13:25:30,758 Answer received: !yv
2024-06-18 13:25:30,758 Command to send: m
d
o140
e

2024-06-18 13:25:30,758 Answer received: !yv
2024-06-18 13:25:30,758 Command to send: m
d
o141
e

2024-06-18 13:25:30,759 Answer received: !yv
2024-06-18 13:25:30,759 Command to send: m
d
o148
e

2024-06-18 13:25:30,759 Answer received: !yv
2024-06-18 13:25:30,759 Command to send: m
d
o165
e

2024-06-18 13:25:30,759 Answer received: !yv
2024-06-18 13:25:30,760 Command to send: m
d
o180
e

2024-06-18 13:25:30,760 Answer received: !yv
2024-06-18 13:25:30,760 Command to send: m
d
o185
e

2024-06-18 13:25:30,760 Answer received: !yv
2024-06-18 13:25:30,760 Command to send: m
d
o191
e

2024-06-18 13:25:30,760 Answer received: !yv
2024-06-18 13:25:30,802 Answer received: !yro199
2024-06-18 13:25:30,803 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,803 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,804 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,804 Answer received: !ym
2024-06-18 13:25:30,804 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,804 Answer received: !ylo200
2024-06-18 13:25:30,805 Command to send: c
o200
add
sCodi_Districte
e

2024-06-18 13:25:30,805 Answer received: !ybtrue
2024-06-18 13:25:30,805 Command to send: c
o200
add
saddresses_district_id
e

2024-06-18 13:25:30,806 Answer received: !ybtrue
2024-06-18 13:25:30,806 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro200
e

2024-06-18 13:25:30,807 Answer received: !yro201
2024-06-18 13:25:30,807 Command to send: c
o199
drop
ro201
e

2024-06-18 13:25:30,818 Answer received: !yro202
2024-06-18 13:25:30,819 Command to send: c
o202
schema
e

2024-06-18 13:25:30,819 Answer received: !yro203
2024-06-18 13:25:30,819 Command to send: c
o203
json
e

2024-06-18 13:25:30,820 Answer received: !ys{"type":"struct","fields":[{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}},{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:25:30,820 Command to send: c
o202
apply
sNom_Districte
e

2024-06-18 13:25:30,821 Answer received: !yro204
2024-06-18 13:25:30,821 Command to send: c
o91
schema
e

2024-06-18 13:25:30,821 Answer received: !yro205
2024-06-18 13:25:30,821 Command to send: c
o205
json
e

2024-06-18 13:25:30,822 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:25:30,822 Command to send: c
o91
apply
sdistrict
e

2024-06-18 13:25:30,823 Answer received: !yro206
2024-06-18 13:25:30,824 Command to send: c
o204
equalTo
ro206
e

2024-06-18 13:25:30,824 Answer received: !yro207
2024-06-18 13:25:30,824 Command to send: c
o202
join
ro91
ro207
sinner
e

2024-06-18 13:25:30,836 Answer received: !yro208
2024-06-18 13:25:30,836 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,837 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,837 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,838 Answer received: !ym
2024-06-18 13:25:30,838 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,838 Answer received: !ylo209
2024-06-18 13:25:30,838 Command to send: c
o209
add
sdistrict_id
e

2024-06-18 13:25:30,839 Answer received: !ybtrue
2024-06-18 13:25:30,839 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro209
e

2024-06-18 13:25:30,839 Answer received: !yro210
2024-06-18 13:25:30,839 Command to send: c
o208
drop
ro210
e

2024-06-18 13:25:30,846 Answer received: !yro211
2024-06-18 13:25:30,846 Command to send: c
o211
schema
e

2024-06-18 13:25:30,846 Answer received: !yro212
2024-06-18 13:25:30,847 Command to send: c
o212
json
e

2024-06-18 13:25:30,848 Answer received: !ys{"type":"struct","fields":[{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}},{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}},{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:25:30,848 Command to send: c
o211
apply
sdistrict
e

2024-06-18 13:25:30,849 Answer received: !yro213
2024-06-18 13:25:30,849 Command to send: c
o151
schema
e

2024-06-18 13:25:30,850 Answer received: !yro214
2024-06-18 13:25:30,850 Command to send: c
o214
json
e

2024-06-18 13:25:30,850 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"integer","nullable":true,"metadata":{}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:25:30,851 Command to send: c
o151
apply
sdistrict
e

2024-06-18 13:25:30,852 Answer received: !yro215
2024-06-18 13:25:30,852 Command to send: c
o213
equalTo
ro215
e

2024-06-18 13:25:30,852 Answer received: !yro216
2024-06-18 13:25:30,852 Command to send: c
o211
join
ro151
ro216
sinner
e

2024-06-18 13:25:30,864 Answer received: !yro217
2024-06-18 13:25:30,864 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,866 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,866 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,866 Answer received: !ym
2024-06-18 13:25:30,867 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,867 Answer received: !ylo218
2024-06-18 13:25:30,867 Command to send: c
o218
add
sdistrict
e

2024-06-18 13:25:30,867 Answer received: !ybtrue
2024-06-18 13:25:30,868 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro218
e

2024-06-18 13:25:30,868 Answer received: !yro219
2024-06-18 13:25:30,868 Command to send: c
o217
drop
ro219
e

2024-06-18 13:25:30,876 Answer received: !yro220
2024-06-18 13:25:30,877 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:25:30,878 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:25:30,878 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:25:30,878 Answer received: !ym
2024-06-18 13:25:30,879 Command to send: i
java.util.ArrayList
e

2024-06-18 13:25:30,879 Answer received: !ylo221
2024-06-18 13:25:30,879 Command to send: c
o221
add
sNom_districte
e

2024-06-18 13:25:30,880 Answer received: !ybtrue
2024-06-18 13:25:30,880 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro221
e

2024-06-18 13:25:30,880 Answer received: !yro222
2024-06-18 13:25:30,881 Command to send: c
o220
drop
ro222
e

2024-06-18 13:25:30,889 Answer received: !yro223
2024-06-18 13:25:31,761 Command to send: m
d
o200
e

2024-06-18 13:25:31,761 Answer received: !yv
2024-06-18 13:25:31,761 Command to send: m
d
o209
e

2024-06-18 13:25:31,762 Answer received: !yv
2024-06-18 13:25:31,762 Command to send: m
d
o218
e

2024-06-18 13:25:31,762 Answer received: !yv
2024-06-18 13:25:31,762 Command to send: m
d
o221
e

2024-06-18 13:25:31,762 Answer received: !yv
2024-06-18 13:25:31,762 Command to send: m
d
o143
e

2024-06-18 13:25:31,763 Answer received: !yv
2024-06-18 13:25:31,763 Command to send: m
d
o144
e

2024-06-18 13:25:31,763 Answer received: !yv
2024-06-18 13:25:31,763 Command to send: m
d
o145
e

2024-06-18 13:25:31,763 Answer received: !yv
2024-06-18 13:25:31,764 Command to send: m
d
o147
e

2024-06-18 13:25:31,764 Answer received: !yv
2024-06-18 13:25:31,764 Command to send: m
d
o149
e

2024-06-18 13:25:31,764 Answer received: !yv
2024-06-18 13:25:31,764 Command to send: m
d
o150
e

2024-06-18 13:25:31,765 Answer received: !yv
2024-06-18 13:25:31,765 Command to send: m
d
o152
e

2024-06-18 13:25:31,765 Answer received: !yv
2024-06-18 13:25:31,765 Command to send: m
d
o153
e

2024-06-18 13:25:31,765 Answer received: !yv
2024-06-18 13:25:31,766 Command to send: m
d
o154
e

2024-06-18 13:25:31,766 Answer received: !yv
2024-06-18 13:25:31,766 Command to send: m
d
o155
e

2024-06-18 13:25:31,766 Answer received: !yv
2024-06-18 13:25:31,766 Command to send: m
d
o156
e

2024-06-18 13:25:31,767 Answer received: !yv
2024-06-18 13:25:31,767 Command to send: m
d
o157
e

2024-06-18 13:25:31,767 Answer received: !yv
2024-06-18 13:25:31,767 Command to send: m
d
o158
e

2024-06-18 13:25:31,767 Answer received: !yv
2024-06-18 13:25:31,768 Command to send: m
d
o159
e

2024-06-18 13:25:31,768 Answer received: !yv
2024-06-18 13:25:31,768 Command to send: m
d
o160
e

2024-06-18 13:25:31,768 Answer received: !yv
2024-06-18 13:25:31,768 Command to send: m
d
o161
e

2024-06-18 13:25:31,769 Answer received: !yv
2024-06-18 13:25:31,769 Command to send: m
d
o162
e

2024-06-18 13:25:31,769 Answer received: !yv
2024-06-18 13:25:31,769 Command to send: m
d
o163
e

2024-06-18 13:25:31,769 Answer received: !yv
2024-06-18 13:25:31,769 Command to send: m
d
o164
e

2024-06-18 13:25:31,770 Answer received: !yv
2024-06-18 13:25:31,770 Command to send: m
d
o166
e

2024-06-18 13:25:31,770 Answer received: !yv
2024-06-18 13:25:31,770 Command to send: m
d
o167
e

2024-06-18 13:25:31,770 Answer received: !yv
2024-06-18 13:25:31,771 Command to send: m
d
o168
e

2024-06-18 13:25:31,771 Answer received: !yv
2024-06-18 13:25:31,771 Command to send: m
d
o169
e

2024-06-18 13:25:31,772 Answer received: !yv
2024-06-18 13:25:31,772 Command to send: m
d
o170
e

2024-06-18 13:25:31,772 Answer received: !yv
2024-06-18 13:25:31,772 Command to send: m
d
o171
e

2024-06-18 13:25:31,772 Answer received: !yv
2024-06-18 13:25:31,772 Command to send: m
d
o172
e

2024-06-18 13:25:31,773 Answer received: !yv
2024-06-18 13:25:31,773 Command to send: m
d
o173
e

2024-06-18 13:25:31,773 Answer received: !yv
2024-06-18 13:25:31,773 Command to send: m
d
o174
e

2024-06-18 13:25:31,773 Answer received: !yv
2024-06-18 13:25:31,774 Command to send: m
d
o175
e

2024-06-18 13:25:31,774 Answer received: !yv
2024-06-18 13:25:31,774 Command to send: m
d
o176
e

2024-06-18 13:25:31,774 Answer received: !yv
2024-06-18 13:25:31,774 Command to send: m
d
o177
e

2024-06-18 13:25:31,774 Answer received: !yv
2024-06-18 13:25:31,774 Command to send: m
d
o178
e

2024-06-18 13:25:31,774 Answer received: !yv
2024-06-18 13:25:31,774 Command to send: m
d
o179
e

2024-06-18 13:25:31,775 Answer received: !yv
2024-06-18 13:25:31,775 Command to send: m
d
o181
e

2024-06-18 13:25:31,775 Answer received: !yv
2024-06-18 13:25:31,775 Command to send: m
d
o183
e

2024-06-18 13:25:31,775 Answer received: !yv
2024-06-18 13:25:31,776 Command to send: m
d
o184
e

2024-06-18 13:25:31,777 Answer received: !yv
2024-06-18 13:25:31,777 Command to send: m
d
o186
e

2024-06-18 13:25:31,777 Answer received: !yv
2024-06-18 13:25:31,777 Command to send: m
d
o187
e

2024-06-18 13:25:31,777 Answer received: !yv
2024-06-18 13:25:31,777 Command to send: m
d
o188
e

2024-06-18 13:25:31,777 Answer received: !yv
2024-06-18 13:25:31,777 Command to send: m
d
o189
e

2024-06-18 13:25:31,778 Answer received: !yv
2024-06-18 13:25:31,778 Command to send: m
d
o190
e

2024-06-18 13:25:31,778 Answer received: !yv
2024-06-18 13:25:31,778 Command to send: m
d
o192
e

2024-06-18 13:25:31,778 Answer received: !yv
2024-06-18 13:25:31,779 Command to send: m
d
o194
e

2024-06-18 13:25:31,779 Answer received: !yv
2024-06-18 13:25:31,779 Command to send: m
d
o195
e

2024-06-18 13:25:31,780 Answer received: !yv
2024-06-18 13:25:31,780 Command to send: m
d
o196
e

2024-06-18 13:25:31,780 Answer received: !yv
2024-06-18 13:25:31,780 Command to send: m
d
o197
e

2024-06-18 13:25:31,780 Answer received: !yv
2024-06-18 13:25:31,780 Command to send: m
d
o198
e

2024-06-18 13:25:31,780 Answer received: !yv
2024-06-18 13:25:31,781 Command to send: m
d
o199
e

2024-06-18 13:25:31,781 Answer received: !yv
2024-06-18 13:25:31,781 Command to send: m
d
o201
e

2024-06-18 13:25:31,781 Answer received: !yv
2024-06-18 13:25:31,781 Command to send: m
d
o202
e

2024-06-18 13:25:31,782 Answer received: !yv
2024-06-18 13:25:31,782 Command to send: m
d
o203
e

2024-06-18 13:25:31,782 Answer received: !yv
2024-06-18 13:25:31,782 Command to send: m
d
o204
e

2024-06-18 13:25:31,783 Answer received: !yv
2024-06-18 13:25:31,783 Command to send: m
d
o205
e

2024-06-18 13:25:31,783 Answer received: !yv
2024-06-18 13:25:31,783 Command to send: m
d
o206
e

2024-06-18 13:25:31,783 Answer received: !yv
2024-06-18 13:25:31,783 Command to send: m
d
o207
e

2024-06-18 13:25:31,784 Answer received: !yv
2024-06-18 13:25:31,784 Command to send: m
d
o208
e

2024-06-18 13:25:31,784 Answer received: !yv
2024-06-18 13:25:31,784 Command to send: m
d
o210
e

2024-06-18 13:25:31,784 Answer received: !yv
2024-06-18 13:25:31,785 Command to send: m
d
o211
e

2024-06-18 13:25:31,785 Answer received: !yv
2024-06-18 13:25:31,785 Command to send: m
d
o212
e

2024-06-18 13:25:31,785 Answer received: !yv
2024-06-18 13:25:31,785 Command to send: m
d
o213
e

2024-06-18 13:25:31,786 Answer received: !yv
2024-06-18 13:25:31,786 Command to send: m
d
o214
e

2024-06-18 13:25:31,786 Answer received: !yv
2024-06-18 13:25:31,786 Command to send: m
d
o215
e

2024-06-18 13:25:31,786 Answer received: !yv
2024-06-18 13:25:31,787 Command to send: m
d
o216
e

2024-06-18 13:25:31,787 Answer received: !yv
2024-06-18 13:25:31,787 Command to send: m
d
o217
e

2024-06-18 13:25:31,787 Answer received: !yv
2024-06-18 13:25:31,787 Command to send: m
d
o219
e

2024-06-18 13:25:31,788 Answer received: !yv
2024-06-18 13:25:31,788 Command to send: m
d
o220
e

2024-06-18 13:25:31,788 Answer received: !yv
2024-06-18 13:25:31,788 Command to send: m
d
o222
e

2024-06-18 13:25:31,789 Answer received: !yv
2024-06-18 13:38:19,671 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:38:19,672 Command to send: c
o223
write
e

2024-06-18 13:38:19,709 Answer received: !yro224
2024-06-18 13:38:19,710 Command to send: c
o224
mode
soverwrite
e

2024-06-18 13:38:19,740 Answer received: !yro225
2024-06-18 13:38:19,741 Command to send: c
o225
parquet
smodel_temp/20240615-1328-hotels
e

2024-06-18 13:38:23,653 Answer received: !yv
2024-06-18 13:38:23,653 Uploading 'model_temp/20240615-1328-hotels' to 'user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels'.
2024-06-18 13:38:23,654 Resolved path '/' to '/'.
2024-06-18 13:38:23,656 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:38:23,671 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:38:23,671 Updated root to '/user/bdm'.
2024-06-18 13:38:23,671 Resolved path 'user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels'.
2024-06-18 13:38:23,671 Listing '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels'.
2024-06-18 13:38:23,672 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels'.
2024-06-18 13:38:23,672 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels'.
2024-06-18 13:38:23,681 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels?user.name=bdm&op=LISTSTATUS HTTP/1.1" 404 None
2024-06-18 13:38:23,682 Uploading 4 files using 1 thread(s).
2024-06-18 13:38:23,682 Uploading 'model_temp/20240615-1328-hotels\\.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc'.
2024-06-18 13:38:23,699 Writing to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc'.
2024-06-18 13:38:23,699 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc'.
2024-06-18 13:38:23,708 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:38:23,711 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:38:23,742 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/.part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet.crc?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:38:23,743 Uploading 'model_temp/20240615-1328-hotels\\._SUCCESS.crc' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:38:23,750 Writing to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:38:23,751 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:38:23,760 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:38:23,763 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:38:23,789 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/._SUCCESS.crc?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:38:23,790 Uploading 'model_temp/20240615-1328-hotels\\part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet'.
2024-06-18 13:38:23,811 Writing to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet'.
2024-06-18 13:38:23,811 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet'.
2024-06-18 13:38:23,820 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:38:23,823 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:38:23,887 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/part-00000-b20de059-e7f4-4e67-82e0-e0ffe195ddf6-c000.snappy.parquet?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:38:23,887 Uploading 'model_temp/20240615-1328-hotels\\_SUCCESS' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS'.
2024-06-18 13:38:23,888 Writing to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS'.
2024-06-18 13:38:23,888 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS'.
2024-06-18 13:38:23,897 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:38:23,900 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:38:23,919 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels/_SUCCESS?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:38:23,920 Upload of 'model_temp/20240615-1328-hotels' to '/user/bdm/user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels' complete.
2024-06-18 13:38:23,920 Model 20240615-1328-hotels uploaded correctly at 'user/bdm/Model_explotation_zone/20240615-1328-hotels/V120240615-1328-hotels' path
2024-06-18 13:42:00,275 Command to send: m
d
o224
e

2024-06-18 13:42:00,275 Answer received: !yv
2024-06-18 13:42:00,275 Command to send: m
d
o225
e

2024-06-18 13:42:00,276 Answer received: !yv
2024-06-18 13:42:29,603 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:42:38,867 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:42:38,868 Command to send: c
o223
write
e

2024-06-18 13:42:38,883 Answer received: !yro226
2024-06-18 13:42:39,295 Command to send: m
d
o226
e

2024-06-18 13:42:39,295 Answer received: !yv
2024-06-18 13:43:20,926 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:43:20,927 Command to send: c
o223
write
e

2024-06-18 13:43:20,930 Answer received: !yro227
2024-06-18 13:43:20,931 Command to send: c
o227
mode
soverwrite
e

2024-06-18 13:43:20,931 Answer received: !yro228
2024-06-18 13:43:20,931 Command to send: c
o228
parquet
smodel_temp/GLM
e

2024-06-18 13:43:21,811 Answer received: !xro229
2024-06-18 13:43:21,812 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,816 Answer received: !yp
2024-06-18 13:43:21,816 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,818 Answer received: !yp
2024-06-18 13:43:21,819 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,819 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,820 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,820 Answer received: !ym
2024-06-18 13:43:21,820 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro229
e

2024-06-18 13:43:21,822 Answer received: !ybfalse
2024-06-18 13:43:21,823 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,828 Answer received: !yp
2024-06-18 13:43:21,828 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,829 Answer received: !yp
2024-06-18 13:43:21,829 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,829 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,829 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,829 Answer received: !ym
2024-06-18 13:43:21,830 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro229
e

2024-06-18 13:43:21,830 Answer received: !ybfalse
2024-06-18 13:43:21,830 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,835 Answer received: !yp
2024-06-18 13:43:21,835 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,837 Answer received: !yp
2024-06-18 13:43:21,837 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,837 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,837 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,837 Answer received: !ym
2024-06-18 13:43:21,838 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro229
e

2024-06-18 13:43:21,840 Answer received: !ybfalse
2024-06-18 13:43:21,840 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,845 Answer received: !yp
2024-06-18 13:43:21,845 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,846 Answer received: !yp
2024-06-18 13:43:21,846 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,846 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,846 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,846 Answer received: !ym
2024-06-18 13:43:21,847 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro229
e

2024-06-18 13:43:21,867 Answer received: !ybfalse
2024-06-18 13:43:21,867 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,871 Answer received: !yp
2024-06-18 13:43:21,871 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,872 Answer received: !yp
2024-06-18 13:43:21,872 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,872 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,872 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,872 Answer received: !ym
2024-06-18 13:43:21,873 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro229
e

2024-06-18 13:43:21,873 Answer received: !ybfalse
2024-06-18 13:43:21,873 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,879 Answer received: !yp
2024-06-18 13:43:21,879 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,880 Answer received: !yp
2024-06-18 13:43:21,880 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,880 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,880 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,881 Answer received: !ym
2024-06-18 13:43:21,881 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro229
e

2024-06-18 13:43:21,882 Answer received: !ybfalse
2024-06-18 13:43:21,882 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,887 Answer received: !yp
2024-06-18 13:43:21,887 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,887 Answer received: !yp
2024-06-18 13:43:21,887 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,888 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,888 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,888 Answer received: !ym
2024-06-18 13:43:21,888 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro229
e

2024-06-18 13:43:21,888 Answer received: !ybfalse
2024-06-18 13:43:21,889 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,892 Answer received: !yp
2024-06-18 13:43:21,892 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,894 Answer received: !yp
2024-06-18 13:43:21,894 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,895 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,895 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,895 Answer received: !ym
2024-06-18 13:43:21,896 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro229
e

2024-06-18 13:43:21,896 Answer received: !ybfalse
2024-06-18 13:43:21,896 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,899 Answer received: !yp
2024-06-18 13:43:21,900 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,901 Answer received: !yp
2024-06-18 13:43:21,902 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,902 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,902 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,903 Answer received: !ym
2024-06-18 13:43:21,903 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro229
e

2024-06-18 13:43:21,904 Answer received: !ybfalse
2024-06-18 13:43:21,904 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,906 Answer received: !yp
2024-06-18 13:43:21,906 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,908 Answer received: !yp
2024-06-18 13:43:21,908 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,909 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,909 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,909 Answer received: !ym
2024-06-18 13:43:21,910 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro229
e

2024-06-18 13:43:21,911 Answer received: !ybfalse
2024-06-18 13:43:21,911 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,913 Answer received: !yp
2024-06-18 13:43:21,913 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,914 Answer received: !yp
2024-06-18 13:43:21,914 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,914 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,914 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,915 Answer received: !ym
2024-06-18 13:43:21,915 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro229
e

2024-06-18 13:43:21,916 Answer received: !ybfalse
2024-06-18 13:43:21,917 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,921 Answer received: !yp
2024-06-18 13:43:21,921 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,922 Answer received: !yp
2024-06-18 13:43:21,922 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,922 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,922 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,922 Answer received: !ym
2024-06-18 13:43:21,923 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro229
e

2024-06-18 13:43:21,924 Answer received: !ybfalse
2024-06-18 13:43:21,924 Command to send: c
o229
getCause
e

2024-06-18 13:43:21,925 Answer received: !yro230
2024-06-18 13:43:21,926 Command to send: r
u
org
rj
e

2024-06-18 13:43:21,930 Answer received: !yp
2024-06-18 13:43:21,930 Command to send: r
u
org.apache
rj
e

2024-06-18 13:43:21,930 Answer received: !yp
2024-06-18 13:43:21,931 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:43:21,931 Answer received: !yp
2024-06-18 13:43:21,931 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:43:21,933 Answer received: !yp
2024-06-18 13:43:21,933 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:43:21,933 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:43:21,933 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:43:21,935 Answer received: !ym
2024-06-18 13:43:21,936 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro229
e

2024-06-18 13:43:21,938 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 26) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:21,938 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,941 Answer received: !yp
2024-06-18 13:43:21,941 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,942 Answer received: !yp
2024-06-18 13:43:21,942 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,943 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,943 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,943 Answer received: !ym
2024-06-18 13:43:21,944 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro230
e

2024-06-18 13:43:21,945 Answer received: !ybfalse
2024-06-18 13:43:21,946 Command to send: c
o229
toString
e

2024-06-18 13:43:21,946 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 26) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:43:21,946 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,949 Answer received: !yp
2024-06-18 13:43:21,950 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,950 Answer received: !yp
2024-06-18 13:43:21,950 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,950 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,951 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,951 Answer received: !ym
2024-06-18 13:43:21,951 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro230
e

2024-06-18 13:43:21,952 Answer received: !ybfalse
2024-06-18 13:43:21,952 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,954 Answer received: !yp
2024-06-18 13:43:21,955 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,955 Answer received: !yp
2024-06-18 13:43:21,955 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,955 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,955 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,956 Answer received: !ym
2024-06-18 13:43:21,956 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro230
e

2024-06-18 13:43:21,956 Answer received: !ybfalse
2024-06-18 13:43:21,957 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,960 Answer received: !yp
2024-06-18 13:43:21,961 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,962 Answer received: !yp
2024-06-18 13:43:21,962 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,962 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,962 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,962 Answer received: !ym
2024-06-18 13:43:21,963 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro230
e

2024-06-18 13:43:21,963 Answer received: !ybfalse
2024-06-18 13:43:21,963 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,965 Answer received: !yp
2024-06-18 13:43:21,966 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,967 Answer received: !yp
2024-06-18 13:43:21,967 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,967 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,967 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,968 Answer received: !ym
2024-06-18 13:43:21,968 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro230
e

2024-06-18 13:43:21,969 Answer received: !ybfalse
2024-06-18 13:43:21,969 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,971 Answer received: !yp
2024-06-18 13:43:21,971 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,972 Answer received: !yp
2024-06-18 13:43:21,972 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,972 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,972 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,972 Answer received: !ym
2024-06-18 13:43:21,973 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro230
e

2024-06-18 13:43:21,973 Answer received: !ybfalse
2024-06-18 13:43:21,974 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,979 Answer received: !yp
2024-06-18 13:43:21,979 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,980 Answer received: !yp
2024-06-18 13:43:21,980 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,980 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,980 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,980 Answer received: !ym
2024-06-18 13:43:21,980 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro230
e

2024-06-18 13:43:21,981 Answer received: !ybfalse
2024-06-18 13:43:21,981 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,986 Answer received: !yp
2024-06-18 13:43:21,987 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,987 Answer received: !yp
2024-06-18 13:43:21,987 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,987 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,988 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,988 Answer received: !ym
2024-06-18 13:43:21,988 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro230
e

2024-06-18 13:43:21,988 Answer received: !ybfalse
2024-06-18 13:43:21,989 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,992 Answer received: !yp
2024-06-18 13:43:21,992 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,994 Answer received: !yp
2024-06-18 13:43:21,995 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:21,995 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:21,995 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:21,995 Answer received: !ym
2024-06-18 13:43:21,996 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro230
e

2024-06-18 13:43:21,996 Answer received: !ybfalse
2024-06-18 13:43:21,996 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:21,998 Answer received: !yp
2024-06-18 13:43:21,998 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:21,999 Answer received: !yp
2024-06-18 13:43:21,999 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:22,000 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:22,000 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:22,000 Answer received: !ym
2024-06-18 13:43:22,001 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro230
e

2024-06-18 13:43:22,001 Answer received: !ybfalse
2024-06-18 13:43:22,002 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:22,004 Answer received: !yp
2024-06-18 13:43:22,005 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:22,005 Answer received: !yp
2024-06-18 13:43:22,005 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:22,005 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:22,005 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:22,006 Answer received: !ym
2024-06-18 13:43:22,006 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro230
e

2024-06-18 13:43:22,006 Answer received: !ybfalse
2024-06-18 13:43:22,007 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:22,012 Answer received: !yp
2024-06-18 13:43:22,012 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:22,012 Answer received: !yp
2024-06-18 13:43:22,012 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:22,013 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:22,013 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:22,013 Answer received: !ym
2024-06-18 13:43:22,013 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro230
e

2024-06-18 13:43:22,014 Answer received: !ybfalse
2024-06-18 13:43:22,014 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:22,017 Answer received: !yp
2024-06-18 13:43:22,017 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:22,019 Answer received: !yp
2024-06-18 13:43:22,020 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:22,020 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:22,021 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:22,021 Answer received: !ym
2024-06-18 13:43:22,021 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro230
e

2024-06-18 13:43:22,021 Answer received: !ybfalse
2024-06-18 13:43:22,021 Command to send: c
o230
getCause
e

2024-06-18 13:43:22,022 Answer received: !yn
2024-06-18 13:43:22,022 Command to send: r
u
org
rj
e

2024-06-18 13:43:22,025 Answer received: !yp
2024-06-18 13:43:22,025 Command to send: r
u
org.apache
rj
e

2024-06-18 13:43:22,026 Answer received: !yp
2024-06-18 13:43:22,027 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:43:22,028 Answer received: !yp
2024-06-18 13:43:22,028 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:43:22,028 Answer received: !yp
2024-06-18 13:43:22,029 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:43:22,029 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:43:22,029 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:43:22,029 Answer received: !ym
2024-06-18 13:43:22,029 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro230
e

2024-06-18 13:43:22,030 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:22,030 Command to send: c
o230
toString
e

2024-06-18 13:43:22,030 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:43:22,334 Command to send: m
d
o227
e

2024-06-18 13:43:22,354 Answer received: !yv
2024-06-18 13:43:22,354 Command to send: m
d
o230
e

2024-06-18 13:43:22,354 Answer received: !yv
2024-06-18 13:43:22,355 Command to send: m
d
o65
e

2024-06-18 13:43:22,355 Answer received: !yv
2024-06-18 13:43:22,355 Command to send: m
d
o80
e

2024-06-18 13:43:22,366 Answer received: !yv
2024-06-18 13:43:22,367 Command to send: m
d
o142
e

2024-06-18 13:43:22,368 Answer received: !yv
2024-06-18 13:43:22,368 Command to send: m
d
o137
e

2024-06-18 13:43:22,381 Answer received: !yv
2024-06-18 13:43:22,535 Command to send: p
ro229
e

2024-06-18 13:43:22,536 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 26) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:22,537 Command to send: p
ro229
e

2024-06-18 13:43:22,537 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 26) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:22,538 Command to send: p
ro229
e

2024-06-18 13:43:22,539 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 36.0 failed 1 times, most recent failure: Lost task 0.0 in stage 36.0 (TID 26) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:41,450 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:43:41,451 Command to send: c
o223
write
e

2024-06-18 13:43:41,455 Answer received: !yro231
2024-06-18 13:43:41,455 Command to send: c
o231
mode
soverwrite
e

2024-06-18 13:43:41,456 Answer received: !yro232
2024-06-18 13:43:41,456 Command to send: c
o232
parquet
smodel_temp/GLM
e

2024-06-18 13:43:41,828 Answer received: !xro233
2024-06-18 13:43:41,829 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,832 Answer received: !yp
2024-06-18 13:43:41,832 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,833 Answer received: !yp
2024-06-18 13:43:41,833 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,833 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,833 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,833 Answer received: !ym
2024-06-18 13:43:41,834 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro233
e

2024-06-18 13:43:41,834 Answer received: !ybfalse
2024-06-18 13:43:41,834 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,836 Answer received: !yp
2024-06-18 13:43:41,837 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,838 Answer received: !yp
2024-06-18 13:43:41,838 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,838 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,839 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,839 Answer received: !ym
2024-06-18 13:43:41,839 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro233
e

2024-06-18 13:43:41,840 Answer received: !ybfalse
2024-06-18 13:43:41,840 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,843 Answer received: !yp
2024-06-18 13:43:41,843 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,844 Answer received: !yp
2024-06-18 13:43:41,844 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,844 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,844 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,845 Answer received: !ym
2024-06-18 13:43:41,845 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro233
e

2024-06-18 13:43:41,845 Answer received: !ybfalse
2024-06-18 13:43:41,846 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,848 Answer received: !yp
2024-06-18 13:43:41,848 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,849 Answer received: !yp
2024-06-18 13:43:41,849 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,849 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,849 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,850 Answer received: !ym
2024-06-18 13:43:41,850 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro233
e

2024-06-18 13:43:41,850 Answer received: !ybfalse
2024-06-18 13:43:41,850 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,852 Answer received: !yp
2024-06-18 13:43:41,853 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,853 Answer received: !yp
2024-06-18 13:43:41,853 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,854 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,854 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,854 Answer received: !ym
2024-06-18 13:43:41,854 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro233
e

2024-06-18 13:43:41,855 Answer received: !ybfalse
2024-06-18 13:43:41,855 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,856 Answer received: !yp
2024-06-18 13:43:41,857 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,857 Answer received: !yp
2024-06-18 13:43:41,857 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,858 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,858 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,858 Answer received: !ym
2024-06-18 13:43:41,858 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro233
e

2024-06-18 13:43:41,859 Answer received: !ybfalse
2024-06-18 13:43:41,859 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,860 Answer received: !yp
2024-06-18 13:43:41,860 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,861 Answer received: !yp
2024-06-18 13:43:41,861 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,861 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,861 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,862 Answer received: !ym
2024-06-18 13:43:41,862 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro233
e

2024-06-18 13:43:41,862 Answer received: !ybfalse
2024-06-18 13:43:41,862 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,864 Answer received: !yp
2024-06-18 13:43:41,864 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,864 Answer received: !yp
2024-06-18 13:43:41,864 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,865 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,865 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,865 Answer received: !ym
2024-06-18 13:43:41,865 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro233
e

2024-06-18 13:43:41,866 Answer received: !ybfalse
2024-06-18 13:43:41,866 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,867 Answer received: !yp
2024-06-18 13:43:41,867 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,868 Answer received: !yp
2024-06-18 13:43:41,868 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,868 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,869 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,869 Answer received: !ym
2024-06-18 13:43:41,869 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro233
e

2024-06-18 13:43:41,869 Answer received: !ybfalse
2024-06-18 13:43:41,870 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,871 Answer received: !yp
2024-06-18 13:43:41,871 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,872 Answer received: !yp
2024-06-18 13:43:41,872 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,872 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,872 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,873 Answer received: !ym
2024-06-18 13:43:41,873 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro233
e

2024-06-18 13:43:41,873 Answer received: !ybfalse
2024-06-18 13:43:41,873 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,875 Answer received: !yp
2024-06-18 13:43:41,875 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,876 Answer received: !yp
2024-06-18 13:43:41,876 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,876 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,876 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,877 Answer received: !ym
2024-06-18 13:43:41,877 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro233
e

2024-06-18 13:43:41,877 Answer received: !ybfalse
2024-06-18 13:43:41,877 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,879 Answer received: !yp
2024-06-18 13:43:41,879 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,880 Answer received: !yp
2024-06-18 13:43:41,880 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,880 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,880 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,881 Answer received: !ym
2024-06-18 13:43:41,881 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro233
e

2024-06-18 13:43:41,881 Answer received: !ybfalse
2024-06-18 13:43:41,881 Command to send: c
o233
getCause
e

2024-06-18 13:43:41,882 Answer received: !yro234
2024-06-18 13:43:41,882 Command to send: r
u
org
rj
e

2024-06-18 13:43:41,884 Answer received: !yp
2024-06-18 13:43:41,884 Command to send: r
u
org.apache
rj
e

2024-06-18 13:43:41,885 Answer received: !yp
2024-06-18 13:43:41,885 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:43:41,885 Answer received: !yp
2024-06-18 13:43:41,886 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:43:41,886 Answer received: !yp
2024-06-18 13:43:41,886 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:43:41,886 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:43:41,887 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:43:41,887 Answer received: !ym
2024-06-18 13:43:41,887 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro233
e

2024-06-18 13:43:41,888 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 30) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:41,888 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,891 Answer received: !yp
2024-06-18 13:43:41,891 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,892 Answer received: !yp
2024-06-18 13:43:41,892 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,892 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,892 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,893 Answer received: !ym
2024-06-18 13:43:41,893 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro234
e

2024-06-18 13:43:41,893 Answer received: !ybfalse
2024-06-18 13:43:41,893 Command to send: c
o233
toString
e

2024-06-18 13:43:41,894 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 30) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:43:41,894 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,896 Answer received: !yp
2024-06-18 13:43:41,897 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,897 Answer received: !yp
2024-06-18 13:43:41,897 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,897 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,898 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,898 Answer received: !ym
2024-06-18 13:43:41,898 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro234
e

2024-06-18 13:43:41,898 Answer received: !ybfalse
2024-06-18 13:43:41,899 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,901 Answer received: !yp
2024-06-18 13:43:41,901 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,902 Answer received: !yp
2024-06-18 13:43:41,902 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,902 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,902 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,902 Answer received: !ym
2024-06-18 13:43:41,903 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro234
e

2024-06-18 13:43:41,903 Answer received: !ybfalse
2024-06-18 13:43:41,903 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,905 Answer received: !yp
2024-06-18 13:43:41,905 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,906 Answer received: !yp
2024-06-18 13:43:41,906 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,906 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,906 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,906 Answer received: !ym
2024-06-18 13:43:41,907 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro234
e

2024-06-18 13:43:41,907 Answer received: !ybfalse
2024-06-18 13:43:41,907 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,909 Answer received: !yp
2024-06-18 13:43:41,909 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,909 Answer received: !yp
2024-06-18 13:43:41,910 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,910 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,910 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,910 Answer received: !ym
2024-06-18 13:43:41,910 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro234
e

2024-06-18 13:43:41,911 Answer received: !ybfalse
2024-06-18 13:43:41,911 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,913 Answer received: !yp
2024-06-18 13:43:41,913 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,913 Answer received: !yp
2024-06-18 13:43:41,914 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,914 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,914 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,914 Answer received: !ym
2024-06-18 13:43:41,915 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro234
e

2024-06-18 13:43:41,915 Answer received: !ybfalse
2024-06-18 13:43:41,915 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,917 Answer received: !yp
2024-06-18 13:43:41,917 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,917 Answer received: !yp
2024-06-18 13:43:41,918 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,918 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,918 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,918 Answer received: !ym
2024-06-18 13:43:41,918 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro234
e

2024-06-18 13:43:41,919 Answer received: !ybfalse
2024-06-18 13:43:41,919 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,921 Answer received: !yp
2024-06-18 13:43:41,921 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,921 Answer received: !yp
2024-06-18 13:43:41,922 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,922 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,922 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,922 Answer received: !ym
2024-06-18 13:43:41,923 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro234
e

2024-06-18 13:43:41,923 Answer received: !ybfalse
2024-06-18 13:43:41,923 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,925 Answer received: !yp
2024-06-18 13:43:41,925 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,925 Answer received: !yp
2024-06-18 13:43:41,926 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,926 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,926 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,926 Answer received: !ym
2024-06-18 13:43:41,926 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro234
e

2024-06-18 13:43:41,927 Answer received: !ybfalse
2024-06-18 13:43:41,927 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,929 Answer received: !yp
2024-06-18 13:43:41,929 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,929 Answer received: !yp
2024-06-18 13:43:41,929 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,930 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,930 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,930 Answer received: !ym
2024-06-18 13:43:41,930 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro234
e

2024-06-18 13:43:41,931 Answer received: !ybfalse
2024-06-18 13:43:41,931 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,933 Answer received: !yp
2024-06-18 13:43:41,933 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,933 Answer received: !yp
2024-06-18 13:43:41,934 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,934 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,934 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,934 Answer received: !ym
2024-06-18 13:43:41,934 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro234
e

2024-06-18 13:43:41,935 Answer received: !ybfalse
2024-06-18 13:43:41,935 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,937 Answer received: !yp
2024-06-18 13:43:41,937 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,937 Answer received: !yp
2024-06-18 13:43:41,938 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,938 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,938 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,938 Answer received: !ym
2024-06-18 13:43:41,939 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro234
e

2024-06-18 13:43:41,939 Answer received: !ybfalse
2024-06-18 13:43:41,939 Command to send: r
u
py4j
rj
e

2024-06-18 13:43:41,941 Answer received: !yp
2024-06-18 13:43:41,941 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:43:41,941 Answer received: !yp
2024-06-18 13:43:41,941 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:43:41,942 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:43:41,942 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:43:41,942 Answer received: !ym
2024-06-18 13:43:41,942 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro234
e

2024-06-18 13:43:41,943 Answer received: !ybfalse
2024-06-18 13:43:41,943 Command to send: c
o234
getCause
e

2024-06-18 13:43:41,943 Answer received: !yn
2024-06-18 13:43:41,943 Command to send: r
u
org
rj
e

2024-06-18 13:43:41,945 Answer received: !yp
2024-06-18 13:43:41,945 Command to send: r
u
org.apache
rj
e

2024-06-18 13:43:41,945 Answer received: !yp
2024-06-18 13:43:41,945 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:43:41,946 Answer received: !yp
2024-06-18 13:43:41,946 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:43:41,946 Answer received: !yp
2024-06-18 13:43:41,946 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:43:41,947 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:43:41,947 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:43:41,947 Answer received: !ym
2024-06-18 13:43:41,947 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro234
e

2024-06-18 13:43:41,948 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:41,948 Command to send: c
o234
toString
e

2024-06-18 13:43:41,948 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:43:42,000 Command to send: p
ro233
e

2024-06-18 13:43:42,000 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 30) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:42,002 Command to send: p
ro233
e

2024-06-18 13:43:42,002 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 30) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:42,003 Command to send: p
ro233
e

2024-06-18 13:43:42,004 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 42.0 failed 1 times, most recent failure: Lost task 0.0 in stage 42.0 (TID 30) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:43:42,388 Command to send: m
d
o231
e

2024-06-18 13:43:42,388 Answer received: !yv
2024-06-18 13:43:42,388 Command to send: m
d
o234
e

2024-06-18 13:43:42,388 Answer received: !yv
2024-06-18 13:44:22,832 Command to send: r
u
functions
rj
e

2024-06-18 13:44:22,834 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:44:22,834 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:44:22,836 Answer received: !ym
2024-06-18 13:44:22,836 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:44:22,837 Answer received: !yro235
2024-06-18 13:44:22,837 Command to send: r
u
functions
rj
e

2024-06-18 13:44:22,840 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:44:22,840 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:44:22,840 Answer received: !ym
2024-06-18 13:44:22,840 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:44:22,841 Answer received: !yro236
2024-06-18 13:44:22,841 Command to send: r
u
functions
rj
e

2024-06-18 13:44:22,843 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:44:22,843 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:44:22,843 Answer received: !ym
2024-06-18 13:44:22,844 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:44:22,844 Answer received: !yro237
2024-06-18 13:44:22,844 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:44:22,845 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:44:22,845 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:44:22,846 Answer received: !ym
2024-06-18 13:44:22,846 Command to send: i
java.util.ArrayList
e

2024-06-18 13:44:22,846 Answer received: !ylo238
2024-06-18 13:44:22,847 Command to send: c
o238
add
ro235
e

2024-06-18 13:44:22,847 Answer received: !ybtrue
2024-06-18 13:44:22,847 Command to send: c
o238
add
ro236
e

2024-06-18 13:44:22,847 Answer received: !ybtrue
2024-06-18 13:44:22,848 Command to send: c
o238
add
ro237
e

2024-06-18 13:44:22,848 Answer received: !ybtrue
2024-06-18 13:44:22,848 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro238
e

2024-06-18 13:44:22,848 Answer received: !yro239
2024-06-18 13:44:22,849 Command to send: c
o193
select
ro239
e

2024-06-18 13:44:23,410 Command to send: m
d
o238
e

2024-06-18 13:44:23,410 Answer received: !yv
2024-06-18 13:44:24,471 Answer received: !xro240
2024-06-18 13:44:24,471 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:24,473 Answer received: !yp
2024-06-18 13:44:24,474 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:24,475 Answer received: !yp
2024-06-18 13:44:24,475 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:24,475 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:24,475 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:24,475 Answer received: !ym
2024-06-18 13:44:24,476 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro240
e

2024-06-18 13:44:24,476 Answer received: !ybfalse
2024-06-18 13:44:24,476 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:24,478 Answer received: !yp
2024-06-18 13:44:24,478 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:24,479 Answer received: !yp
2024-06-18 13:44:24,479 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:24,479 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:24,479 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:24,480 Answer received: !ym
2024-06-18 13:44:24,480 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro240
e

2024-06-18 13:44:24,480 Answer received: !ybtrue
2024-06-18 13:44:24,480 Command to send: c
o240
getMessage
e

2024-06-18 13:44:24,483 Answer received: !ys[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ndex RFD Barcelona = 100` cannot be resolved. Did you mean one of the following? [`Avg_Index_RFD`, `Codi_Districte`, `Nom_Districte`].;\n'Project [Nom_Districte#340, Codi_Districte#1, 'ndex RFD Barcelona = 100]\n+- Aggregate [Nom_Districte#340, Codi_Districte#1], [Nom_Districte#340, Codi_Districte#1, avg(cast(ndex RFD Barcelona = 100#348 as double)) AS Avg_Index_RFD#374]\n   +- Project [Nom_Districte#340, Codi_Districte#1, ndex RFD Barcelona = 100#344[1] AS ndex RFD Barcelona = 100#348]\n      +- Project [Nom_Districte#340, Codi_Districte#1, split(ndex RFD Barcelona = 100#6, ", -1) AS ndex RFD Barcelona = 100#344]\n         +- Project [Nom_Districte#336[1] AS Nom_Districte#340, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n            +- Project [split(Nom_Districte#2, ", -1) AS Nom_Districte#336, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n               +- Filter atleastnnonnulls(3, Nom_Districte#2, Codi_Districte#1, ndex RFD Barcelona = 100#6)\n                  +- Project [Nom_Districte#2, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n                     +- Relation [Any#0,Codi_Districte#1,Nom_Districte#2,Codi_Barri#3,Nom_Barri#4,Poblaci#5,ndex RFD Barcelona = 100#6] parquet\n
2024-06-18 13:44:24,483 Command to send: r
u
org
rj
e

2024-06-18 13:44:24,485 Answer received: !yp
2024-06-18 13:44:24,485 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:24,486 Answer received: !yp
2024-06-18 13:44:24,486 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:24,486 Answer received: !yp
2024-06-18 13:44:24,486 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:44:24,487 Answer received: !yp
2024-06-18 13:44:24,487 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:44:24,487 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:44:24,487 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:44:24,488 Answer received: !ym
2024-06-18 13:44:24,488 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro240
e

2024-06-18 13:44:24,489 Answer received: !ysorg.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ndex RFD Barcelona = 100` cannot be resolved. Did you mean one of the following? [`Avg_Index_RFD`, `Codi_Districte`, `Nom_Districte`].;\n'Project [Nom_Districte#340, Codi_Districte#1, 'ndex RFD Barcelona = 100]\n+- Aggregate [Nom_Districte#340, Codi_Districte#1], [Nom_Districte#340, Codi_Districte#1, avg(cast(ndex RFD Barcelona = 100#348 as double)) AS Avg_Index_RFD#374]\n   +- Project [Nom_Districte#340, Codi_Districte#1, ndex RFD Barcelona = 100#344[1] AS ndex RFD Barcelona = 100#348]\n      +- Project [Nom_Districte#340, Codi_Districte#1, split(ndex RFD Barcelona = 100#6, ", -1) AS ndex RFD Barcelona = 100#344]\n         +- Project [Nom_Districte#336[1] AS Nom_Districte#340, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n            +- Project [split(Nom_Districte#2, ", -1) AS Nom_Districte#336, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n               +- Filter atleastnnonnulls(3, Nom_Districte#2, Codi_Districte#1, ndex RFD Barcelona = 100#6)\n                  +- Project [Nom_Districte#2, Codi_Districte#1, ndex RFD Barcelona = 100#6]\n                     +- Relation [Any#0,Codi_Districte#1,Nom_Districte#2,Codi_Barri#3,Nom_Barri#4,Poblaci#5,ndex RFD Barcelona = 100#6] parquet\n\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)\r\n	at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4351)\r\n	at org.apache.spark.sql.Dataset.select(Dataset.scala:1540)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:24,490 Command to send: c
o240
getCause
e

2024-06-18 13:44:24,491 Answer received: !yn
2024-06-18 13:44:24,728 Command to send: r
u
org
rj
e

2024-06-18 13:44:24,730 Answer received: !yp
2024-06-18 13:44:24,730 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:24,731 Answer received: !yp
2024-06-18 13:44:24,731 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:24,732 Answer received: !yp
2024-06-18 13:44:24,732 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:44:24,733 Answer received: !yp
2024-06-18 13:44:24,733 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:44:24,734 Answer received: !yp
2024-06-18 13:44:24,734 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:44:24,734 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:44:24,734 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,736 Answer received: !ym
2024-06-18 13:44:24,737 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,738 Answer received: !yro241
2024-06-18 13:44:24,738 Command to send: c
o241
pysparkJVMStacktraceEnabled
e

2024-06-18 13:44:24,740 Answer received: !ybfalse
2024-06-18 13:44:24,742 Command to send: r
u
org
rj
e

2024-06-18 13:44:24,744 Answer received: !yp
2024-06-18 13:44:24,744 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:24,744 Answer received: !yp
2024-06-18 13:44:24,745 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:24,745 Answer received: !yp
2024-06-18 13:44:24,745 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:44:24,746 Answer received: !yp
2024-06-18 13:44:24,746 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:44:24,746 Answer received: !yp
2024-06-18 13:44:24,747 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:44:24,747 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:44:24,747 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,747 Answer received: !ym
2024-06-18 13:44:24,748 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,748 Answer received: !yro242
2024-06-18 13:44:24,748 Command to send: c
o242
pysparkJVMStacktraceEnabled
e

2024-06-18 13:44:24,749 Answer received: !ybfalse
2024-06-18 13:44:24,750 Command to send: r
u
org
rj
e

2024-06-18 13:44:24,752 Answer received: !yp
2024-06-18 13:44:24,752 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:24,753 Answer received: !yp
2024-06-18 13:44:24,753 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:24,753 Answer received: !yp
2024-06-18 13:44:24,754 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:44:24,754 Answer received: !yp
2024-06-18 13:44:24,754 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:44:24,755 Answer received: !yp
2024-06-18 13:44:24,755 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:44:24,755 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:44:24,755 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,755 Answer received: !ym
2024-06-18 13:44:24,756 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:44:24,756 Answer received: !yro243
2024-06-18 13:44:24,756 Command to send: c
o243
pysparkJVMStacktraceEnabled
e

2024-06-18 13:44:24,757 Answer received: !ybfalse
2024-06-18 13:44:25,411 Command to send: m
d
o235
e

2024-06-18 13:44:25,411 Answer received: !yv
2024-06-18 13:44:25,411 Command to send: m
d
o236
e

2024-06-18 13:44:25,411 Answer received: !yv
2024-06-18 13:44:25,412 Command to send: m
d
o237
e

2024-06-18 13:44:25,412 Answer received: !yv
2024-06-18 13:44:31,834 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:44:31,835 Command to send: c
o223
write
e

2024-06-18 13:44:31,837 Answer received: !yro244
2024-06-18 13:44:31,838 Command to send: c
o244
mode
soverwrite
e

2024-06-18 13:44:31,838 Answer received: !yro245
2024-06-18 13:44:31,839 Command to send: c
o245
parquet
smodel_temp/GLM
e

2024-06-18 13:44:32,155 Answer received: !xro246
2024-06-18 13:44:32,155 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,158 Answer received: !yp
2024-06-18 13:44:32,158 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,158 Answer received: !yp
2024-06-18 13:44:32,158 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,159 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,159 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,159 Answer received: !ym
2024-06-18 13:44:32,159 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro246
e

2024-06-18 13:44:32,160 Answer received: !ybfalse
2024-06-18 13:44:32,160 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,162 Answer received: !yp
2024-06-18 13:44:32,162 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,162 Answer received: !yp
2024-06-18 13:44:32,163 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,163 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,163 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,163 Answer received: !ym
2024-06-18 13:44:32,164 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro246
e

2024-06-18 13:44:32,164 Answer received: !ybfalse
2024-06-18 13:44:32,164 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,166 Answer received: !yp
2024-06-18 13:44:32,166 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,167 Answer received: !yp
2024-06-18 13:44:32,167 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,167 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,167 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,168 Answer received: !ym
2024-06-18 13:44:32,168 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro246
e

2024-06-18 13:44:32,168 Answer received: !ybfalse
2024-06-18 13:44:32,168 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,171 Answer received: !yp
2024-06-18 13:44:32,171 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,172 Answer received: !yp
2024-06-18 13:44:32,172 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,172 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,172 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,173 Answer received: !ym
2024-06-18 13:44:32,173 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro246
e

2024-06-18 13:44:32,173 Answer received: !ybfalse
2024-06-18 13:44:32,173 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,175 Answer received: !yp
2024-06-18 13:44:32,175 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,175 Answer received: !yp
2024-06-18 13:44:32,175 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,176 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,176 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,176 Answer received: !ym
2024-06-18 13:44:32,176 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro246
e

2024-06-18 13:44:32,176 Answer received: !ybfalse
2024-06-18 13:44:32,177 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,178 Answer received: !yp
2024-06-18 13:44:32,178 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,179 Answer received: !yp
2024-06-18 13:44:32,179 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,179 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,179 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,179 Answer received: !ym
2024-06-18 13:44:32,180 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro246
e

2024-06-18 13:44:32,180 Answer received: !ybfalse
2024-06-18 13:44:32,180 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,182 Answer received: !yp
2024-06-18 13:44:32,182 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,183 Answer received: !yp
2024-06-18 13:44:32,183 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,183 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,183 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,183 Answer received: !ym
2024-06-18 13:44:32,183 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro246
e

2024-06-18 13:44:32,184 Answer received: !ybfalse
2024-06-18 13:44:32,184 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,185 Answer received: !yp
2024-06-18 13:44:32,185 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,186 Answer received: !yp
2024-06-18 13:44:32,186 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,186 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,186 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,186 Answer received: !ym
2024-06-18 13:44:32,186 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro246
e

2024-06-18 13:44:32,187 Answer received: !ybfalse
2024-06-18 13:44:32,187 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,189 Answer received: !yp
2024-06-18 13:44:32,189 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,190 Answer received: !yp
2024-06-18 13:44:32,190 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,190 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,190 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,190 Answer received: !ym
2024-06-18 13:44:32,191 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro246
e

2024-06-18 13:44:32,191 Answer received: !ybfalse
2024-06-18 13:44:32,191 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,194 Answer received: !yp
2024-06-18 13:44:32,194 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,194 Answer received: !yp
2024-06-18 13:44:32,194 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,194 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,195 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,195 Answer received: !ym
2024-06-18 13:44:32,195 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro246
e

2024-06-18 13:44:32,195 Answer received: !ybfalse
2024-06-18 13:44:32,195 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,197 Answer received: !yp
2024-06-18 13:44:32,198 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,198 Answer received: !yp
2024-06-18 13:44:32,198 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,198 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,198 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,199 Answer received: !ym
2024-06-18 13:44:32,199 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro246
e

2024-06-18 13:44:32,199 Answer received: !ybfalse
2024-06-18 13:44:32,199 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,201 Answer received: !yp
2024-06-18 13:44:32,201 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,201 Answer received: !yp
2024-06-18 13:44:32,201 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,202 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,202 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,202 Answer received: !ym
2024-06-18 13:44:32,202 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro246
e

2024-06-18 13:44:32,202 Answer received: !ybfalse
2024-06-18 13:44:32,202 Command to send: c
o246
getCause
e

2024-06-18 13:44:32,203 Answer received: !yro247
2024-06-18 13:44:32,203 Command to send: r
u
org
rj
e

2024-06-18 13:44:32,204 Answer received: !yp
2024-06-18 13:44:32,204 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:32,205 Answer received: !yp
2024-06-18 13:44:32,205 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:32,205 Answer received: !yp
2024-06-18 13:44:32,205 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:44:32,206 Answer received: !yp
2024-06-18 13:44:32,206 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:44:32,206 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:44:32,206 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:44:32,206 Answer received: !ym
2024-06-18 13:44:32,206 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro246
e

2024-06-18 13:44:32,207 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 34) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:32,207 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,209 Answer received: !yp
2024-06-18 13:44:32,209 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,209 Answer received: !yp
2024-06-18 13:44:32,209 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,210 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,210 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,210 Answer received: !ym
2024-06-18 13:44:32,210 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro247
e

2024-06-18 13:44:32,210 Answer received: !ybfalse
2024-06-18 13:44:32,210 Command to send: c
o246
toString
e

2024-06-18 13:44:32,211 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 34) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:44:32,211 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,213 Answer received: !yp
2024-06-18 13:44:32,213 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,213 Answer received: !yp
2024-06-18 13:44:32,213 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,213 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,214 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,214 Answer received: !ym
2024-06-18 13:44:32,214 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro247
e

2024-06-18 13:44:32,214 Answer received: !ybfalse
2024-06-18 13:44:32,214 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,216 Answer received: !yp
2024-06-18 13:44:32,216 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,217 Answer received: !yp
2024-06-18 13:44:32,217 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,217 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,217 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,217 Answer received: !ym
2024-06-18 13:44:32,218 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro247
e

2024-06-18 13:44:32,218 Answer received: !ybfalse
2024-06-18 13:44:32,218 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,219 Answer received: !yp
2024-06-18 13:44:32,220 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,220 Answer received: !yp
2024-06-18 13:44:32,220 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,220 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,220 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,221 Answer received: !ym
2024-06-18 13:44:32,221 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro247
e

2024-06-18 13:44:32,221 Answer received: !ybfalse
2024-06-18 13:44:32,221 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,223 Answer received: !yp
2024-06-18 13:44:32,223 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,224 Answer received: !yp
2024-06-18 13:44:32,224 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,224 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,224 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,224 Answer received: !ym
2024-06-18 13:44:32,225 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro247
e

2024-06-18 13:44:32,225 Answer received: !ybfalse
2024-06-18 13:44:32,225 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,227 Answer received: !yp
2024-06-18 13:44:32,227 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,227 Answer received: !yp
2024-06-18 13:44:32,227 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,227 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,227 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,228 Answer received: !ym
2024-06-18 13:44:32,228 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro247
e

2024-06-18 13:44:32,228 Answer received: !ybfalse
2024-06-18 13:44:32,228 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,230 Answer received: !yp
2024-06-18 13:44:32,230 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,231 Answer received: !yp
2024-06-18 13:44:32,231 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,231 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,231 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,232 Answer received: !ym
2024-06-18 13:44:32,232 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro247
e

2024-06-18 13:44:32,232 Answer received: !ybfalse
2024-06-18 13:44:32,232 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,234 Answer received: !yp
2024-06-18 13:44:32,235 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,235 Answer received: !yp
2024-06-18 13:44:32,235 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,235 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,235 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,236 Answer received: !ym
2024-06-18 13:44:32,236 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro247
e

2024-06-18 13:44:32,236 Answer received: !ybfalse
2024-06-18 13:44:32,236 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,238 Answer received: !yp
2024-06-18 13:44:32,238 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,239 Answer received: !yp
2024-06-18 13:44:32,239 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,239 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,239 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,240 Answer received: !ym
2024-06-18 13:44:32,240 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro247
e

2024-06-18 13:44:32,240 Answer received: !ybfalse
2024-06-18 13:44:32,240 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,242 Answer received: !yp
2024-06-18 13:44:32,242 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,242 Answer received: !yp
2024-06-18 13:44:32,242 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,242 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,243 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,243 Answer received: !ym
2024-06-18 13:44:32,243 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro247
e

2024-06-18 13:44:32,243 Answer received: !ybfalse
2024-06-18 13:44:32,243 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,246 Answer received: !yp
2024-06-18 13:44:32,246 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,247 Answer received: !yp
2024-06-18 13:44:32,247 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,248 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,248 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,248 Answer received: !ym
2024-06-18 13:44:32,248 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro247
e

2024-06-18 13:44:32,248 Answer received: !ybfalse
2024-06-18 13:44:32,249 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,251 Answer received: !yp
2024-06-18 13:44:32,251 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,251 Answer received: !yp
2024-06-18 13:44:32,251 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,252 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,252 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,252 Answer received: !ym
2024-06-18 13:44:32,252 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro247
e

2024-06-18 13:44:32,252 Answer received: !ybfalse
2024-06-18 13:44:32,253 Command to send: r
u
py4j
rj
e

2024-06-18 13:44:32,254 Answer received: !yp
2024-06-18 13:44:32,254 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:44:32,255 Answer received: !yp
2024-06-18 13:44:32,255 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:44:32,256 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:44:32,256 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:44:32,256 Answer received: !ym
2024-06-18 13:44:32,256 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro247
e

2024-06-18 13:44:32,257 Answer received: !ybfalse
2024-06-18 13:44:32,257 Command to send: c
o247
getCause
e

2024-06-18 13:44:32,257 Answer received: !yn
2024-06-18 13:44:32,257 Command to send: r
u
org
rj
e

2024-06-18 13:44:32,259 Answer received: !yp
2024-06-18 13:44:32,259 Command to send: r
u
org.apache
rj
e

2024-06-18 13:44:32,260 Answer received: !yp
2024-06-18 13:44:32,260 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:44:32,260 Answer received: !yp
2024-06-18 13:44:32,260 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:44:32,261 Answer received: !yp
2024-06-18 13:44:32,261 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:44:32,261 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:44:32,261 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:44:32,261 Answer received: !ym
2024-06-18 13:44:32,261 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro247
e

2024-06-18 13:44:32,262 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:32,262 Command to send: c
o247
toString
e

2024-06-18 13:44:32,262 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:44:32,319 Command to send: p
ro246
e

2024-06-18 13:44:32,320 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 34) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:32,321 Command to send: p
ro246
e

2024-06-18 13:44:32,321 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 34) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:32,322 Command to send: p
ro246
e

2024-06-18 13:44:32,323 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 46.0 failed 1 times, most recent failure: Lost task 0.0 in stage 46.0 (TID 34) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:44:32,417 Command to send: m
d
o241
e

2024-06-18 13:44:32,417 Answer received: !yv
2024-06-18 13:44:32,417 Command to send: m
d
o242
e

2024-06-18 13:44:32,418 Answer received: !yv
2024-06-18 13:44:32,418 Command to send: m
d
o243
e

2024-06-18 13:44:32,418 Answer received: !yv
2024-06-18 13:44:32,418 Command to send: m
d
o244
e

2024-06-18 13:44:32,418 Answer received: !yv
2024-06-18 13:44:32,419 Command to send: m
d
o247
e

2024-06-18 13:44:32,419 Answer received: !yv
2024-06-18 13:46:43,648 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:46:43,648 Command to send: c
o223
write
e

2024-06-18 13:46:43,651 Answer received: !yro248
2024-06-18 13:46:43,651 Command to send: c
o248
mode
soverwrite
e

2024-06-18 13:46:43,652 Answer received: !yro249
2024-06-18 13:46:43,652 Command to send: c
o249
parquet
smodel_temp/GLM_V1
e

2024-06-18 13:46:43,960 Answer received: !xro250
2024-06-18 13:46:43,961 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,965 Answer received: !yp
2024-06-18 13:46:43,966 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,967 Answer received: !yp
2024-06-18 13:46:43,967 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,968 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,968 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,968 Answer received: !ym
2024-06-18 13:46:43,968 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro250
e

2024-06-18 13:46:43,969 Answer received: !ybfalse
2024-06-18 13:46:43,969 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,972 Answer received: !yp
2024-06-18 13:46:43,972 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,973 Answer received: !yp
2024-06-18 13:46:43,973 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,973 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,973 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,974 Answer received: !ym
2024-06-18 13:46:43,974 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro250
e

2024-06-18 13:46:43,974 Answer received: !ybfalse
2024-06-18 13:46:43,974 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,977 Answer received: !yp
2024-06-18 13:46:43,977 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,979 Answer received: !yp
2024-06-18 13:46:43,979 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,979 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,979 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,980 Answer received: !ym
2024-06-18 13:46:43,980 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro250
e

2024-06-18 13:46:43,980 Answer received: !ybfalse
2024-06-18 13:46:43,980 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,983 Answer received: !yp
2024-06-18 13:46:43,983 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,984 Answer received: !yp
2024-06-18 13:46:43,984 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,984 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,985 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,985 Answer received: !ym
2024-06-18 13:46:43,985 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro250
e

2024-06-18 13:46:43,985 Answer received: !ybfalse
2024-06-18 13:46:43,985 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,988 Answer received: !yp
2024-06-18 13:46:43,988 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,989 Answer received: !yp
2024-06-18 13:46:43,989 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,990 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,990 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,990 Answer received: !ym
2024-06-18 13:46:43,991 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro250
e

2024-06-18 13:46:43,991 Answer received: !ybfalse
2024-06-18 13:46:43,991 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,995 Answer received: !yp
2024-06-18 13:46:43,995 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,996 Answer received: !yp
2024-06-18 13:46:43,996 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:43,996 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:43,996 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:43,997 Answer received: !ym
2024-06-18 13:46:43,997 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro250
e

2024-06-18 13:46:43,997 Answer received: !ybfalse
2024-06-18 13:46:43,997 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:43,999 Answer received: !yp
2024-06-18 13:46:43,999 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:43,999 Answer received: !yp
2024-06-18 13:46:44,000 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,000 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,000 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,000 Answer received: !ym
2024-06-18 13:46:44,000 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro250
e

2024-06-18 13:46:44,001 Answer received: !ybfalse
2024-06-18 13:46:44,001 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,003 Answer received: !yp
2024-06-18 13:46:44,003 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,004 Answer received: !yp
2024-06-18 13:46:44,004 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,005 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,005 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,005 Answer received: !ym
2024-06-18 13:46:44,005 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro250
e

2024-06-18 13:46:44,005 Answer received: !ybfalse
2024-06-18 13:46:44,006 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,007 Answer received: !yp
2024-06-18 13:46:44,007 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,008 Answer received: !yp
2024-06-18 13:46:44,008 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,008 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,008 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,008 Answer received: !ym
2024-06-18 13:46:44,009 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro250
e

2024-06-18 13:46:44,009 Answer received: !ybfalse
2024-06-18 13:46:44,009 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,011 Answer received: !yp
2024-06-18 13:46:44,011 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,011 Answer received: !yp
2024-06-18 13:46:44,011 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,011 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,012 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,012 Answer received: !ym
2024-06-18 13:46:44,012 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro250
e

2024-06-18 13:46:44,012 Answer received: !ybfalse
2024-06-18 13:46:44,012 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,014 Answer received: !yp
2024-06-18 13:46:44,014 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,014 Answer received: !yp
2024-06-18 13:46:44,015 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,015 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,015 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,015 Answer received: !ym
2024-06-18 13:46:44,015 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro250
e

2024-06-18 13:46:44,016 Answer received: !ybfalse
2024-06-18 13:46:44,016 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,017 Answer received: !yp
2024-06-18 13:46:44,017 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,017 Answer received: !yp
2024-06-18 13:46:44,018 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,018 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,018 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,018 Answer received: !ym
2024-06-18 13:46:44,018 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro250
e

2024-06-18 13:46:44,018 Answer received: !ybfalse
2024-06-18 13:46:44,019 Command to send: c
o250
getCause
e

2024-06-18 13:46:44,019 Answer received: !yro251
2024-06-18 13:46:44,019 Command to send: r
u
org
rj
e

2024-06-18 13:46:44,021 Answer received: !yp
2024-06-18 13:46:44,021 Command to send: r
u
org.apache
rj
e

2024-06-18 13:46:44,021 Answer received: !yp
2024-06-18 13:46:44,021 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:46:44,022 Answer received: !yp
2024-06-18 13:46:44,022 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:46:44,022 Answer received: !yp
2024-06-18 13:46:44,023 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:46:44,023 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:46:44,023 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:46:44,023 Answer received: !ym
2024-06-18 13:46:44,023 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro250
e

2024-06-18 13:46:44,024 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 38) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:46:44,024 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,025 Answer received: !yp
2024-06-18 13:46:44,026 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,026 Answer received: !yp
2024-06-18 13:46:44,026 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,026 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,026 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,026 Answer received: !ym
2024-06-18 13:46:44,027 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro251
e

2024-06-18 13:46:44,027 Answer received: !ybfalse
2024-06-18 13:46:44,027 Command to send: c
o250
toString
e

2024-06-18 13:46:44,027 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 38) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:46:44,027 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,029 Answer received: !yp
2024-06-18 13:46:44,029 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,030 Answer received: !yp
2024-06-18 13:46:44,030 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,030 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,030 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,031 Answer received: !ym
2024-06-18 13:46:44,031 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro251
e

2024-06-18 13:46:44,031 Answer received: !ybfalse
2024-06-18 13:46:44,031 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,033 Answer received: !yp
2024-06-18 13:46:44,033 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,033 Answer received: !yp
2024-06-18 13:46:44,034 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,034 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,034 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,034 Answer received: !ym
2024-06-18 13:46:44,034 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro251
e

2024-06-18 13:46:44,034 Answer received: !ybfalse
2024-06-18 13:46:44,035 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,036 Answer received: !yp
2024-06-18 13:46:44,036 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,037 Answer received: !yp
2024-06-18 13:46:44,037 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,037 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,037 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,037 Answer received: !ym
2024-06-18 13:46:44,038 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro251
e

2024-06-18 13:46:44,038 Answer received: !ybfalse
2024-06-18 13:46:44,038 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,040 Answer received: !yp
2024-06-18 13:46:44,040 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,041 Answer received: !yp
2024-06-18 13:46:44,041 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,041 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,041 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,041 Answer received: !ym
2024-06-18 13:46:44,041 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro251
e

2024-06-18 13:46:44,042 Answer received: !ybfalse
2024-06-18 13:46:44,042 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,043 Answer received: !yp
2024-06-18 13:46:44,043 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,044 Answer received: !yp
2024-06-18 13:46:44,044 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,044 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,044 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,044 Answer received: !ym
2024-06-18 13:46:44,044 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro251
e

2024-06-18 13:46:44,045 Answer received: !ybfalse
2024-06-18 13:46:44,045 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,046 Answer received: !yp
2024-06-18 13:46:44,046 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,047 Answer received: !yp
2024-06-18 13:46:44,047 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,047 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,047 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,047 Answer received: !ym
2024-06-18 13:46:44,047 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro251
e

2024-06-18 13:46:44,048 Answer received: !ybfalse
2024-06-18 13:46:44,048 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,049 Answer received: !yp
2024-06-18 13:46:44,049 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,050 Answer received: !yp
2024-06-18 13:46:44,050 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,050 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,050 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,050 Answer received: !ym
2024-06-18 13:46:44,050 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro251
e

2024-06-18 13:46:44,051 Answer received: !ybfalse
2024-06-18 13:46:44,051 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,051 Answer received: !yp
2024-06-18 13:46:44,051 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,052 Answer received: !yp
2024-06-18 13:46:44,052 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,052 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,052 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,052 Answer received: !ym
2024-06-18 13:46:44,053 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro251
e

2024-06-18 13:46:44,053 Answer received: !ybfalse
2024-06-18 13:46:44,053 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,055 Answer received: !yp
2024-06-18 13:46:44,055 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,055 Answer received: !yp
2024-06-18 13:46:44,055 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,055 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,055 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,056 Answer received: !ym
2024-06-18 13:46:44,056 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro251
e

2024-06-18 13:46:44,056 Answer received: !ybfalse
2024-06-18 13:46:44,056 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,058 Answer received: !yp
2024-06-18 13:46:44,058 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,058 Answer received: !yp
2024-06-18 13:46:44,058 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,059 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,059 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,059 Answer received: !ym
2024-06-18 13:46:44,059 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro251
e

2024-06-18 13:46:44,059 Answer received: !ybfalse
2024-06-18 13:46:44,059 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,061 Answer received: !yp
2024-06-18 13:46:44,061 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,061 Answer received: !yp
2024-06-18 13:46:44,062 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,062 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,062 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,062 Answer received: !ym
2024-06-18 13:46:44,062 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro251
e

2024-06-18 13:46:44,063 Answer received: !ybfalse
2024-06-18 13:46:44,063 Command to send: r
u
py4j
rj
e

2024-06-18 13:46:44,064 Answer received: !yp
2024-06-18 13:46:44,064 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:46:44,065 Answer received: !yp
2024-06-18 13:46:44,065 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:46:44,065 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:46:44,065 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:46:44,065 Answer received: !ym
2024-06-18 13:46:44,065 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro251
e

2024-06-18 13:46:44,066 Answer received: !ybfalse
2024-06-18 13:46:44,066 Command to send: c
o251
getCause
e

2024-06-18 13:46:44,066 Answer received: !yn
2024-06-18 13:46:44,066 Command to send: r
u
org
rj
e

2024-06-18 13:46:44,068 Answer received: !yp
2024-06-18 13:46:44,068 Command to send: r
u
org.apache
rj
e

2024-06-18 13:46:44,068 Answer received: !yp
2024-06-18 13:46:44,068 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:46:44,069 Answer received: !yp
2024-06-18 13:46:44,069 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:46:44,069 Answer received: !yp
2024-06-18 13:46:44,069 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:46:44,069 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:46:44,069 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:46:44,070 Answer received: !ym
2024-06-18 13:46:44,070 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro251
e

2024-06-18 13:46:44,070 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:46:44,070 Command to send: c
o251
toString
e

2024-06-18 13:46:44,070 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:46:44,125 Command to send: p
ro250
e

2024-06-18 13:46:44,125 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 38) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:46:44,126 Command to send: p
ro250
e

2024-06-18 13:46:44,127 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 38) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:46:44,128 Command to send: p
ro250
e

2024-06-18 13:46:44,128 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 50.0 failed 1 times, most recent failure: Lost task 0.0 in stage 50.0 (TID 38) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:46:44,481 Command to send: m
d
o248
e

2024-06-18 13:46:44,481 Answer received: !yv
2024-06-18 13:46:44,481 Command to send: m
d
o251
e

2024-06-18 13:46:44,481 Answer received: !yv
2024-06-18 13:47:02,249 Command to send: c
o223
schema
e

2024-06-18 13:47:02,249 Answer received: !yro252
2024-06-18 13:47:02,249 Command to send: c
o252
json
e

2024-06-18 13:47:02,250 Answer received: !ys{"type":"struct","fields":[{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"integer","nullable":true,"metadata":{}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:47:02,251 Command to send: c
o25
sessionState
e

2024-06-18 13:47:02,253 Answer received: !yro253
2024-06-18 13:47:02,253 Command to send: c
o253
conf
e

2024-06-18 13:47:02,253 Answer received: !yro254
2024-06-18 13:47:02,254 Command to send: c
o254
isReplEagerEvalEnabled
e

2024-06-18 13:47:02,254 Answer received: !ybfalse
2024-06-18 13:47:02,254 Command to send: c
o25
sessionState
e

2024-06-18 13:47:02,255 Answer received: !yro255
2024-06-18 13:47:02,255 Command to send: c
o255
conf
e

2024-06-18 13:47:02,255 Answer received: !yro256
2024-06-18 13:47:02,255 Command to send: c
o256
isReplEagerEvalEnabled
e

2024-06-18 13:47:02,256 Answer received: !ybfalse
2024-06-18 13:48:00,875 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:48:00,875 Command to send: c
o223
write
e

2024-06-18 13:48:00,878 Answer received: !yro257
2024-06-18 13:48:00,878 Command to send: c
o257
mode
soverwrite
e

2024-06-18 13:48:00,878 Answer received: !yro258
2024-06-18 13:48:00,879 Command to send: c
o258
save
smodel_temp/GLM_V1.parquet
e

2024-06-18 13:48:01,213 Answer received: !xro259
2024-06-18 13:48:01,213 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,218 Answer received: !yp
2024-06-18 13:48:01,218 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,219 Answer received: !yp
2024-06-18 13:48:01,219 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,220 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,220 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,220 Answer received: !ym
2024-06-18 13:48:01,221 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro259
e

2024-06-18 13:48:01,221 Answer received: !ybfalse
2024-06-18 13:48:01,222 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,225 Answer received: !yp
2024-06-18 13:48:01,225 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,227 Answer received: !yp
2024-06-18 13:48:01,228 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,229 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,229 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,229 Answer received: !ym
2024-06-18 13:48:01,230 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro259
e

2024-06-18 13:48:01,230 Answer received: !ybfalse
2024-06-18 13:48:01,230 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,234 Answer received: !yp
2024-06-18 13:48:01,234 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,235 Answer received: !yp
2024-06-18 13:48:01,235 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,236 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,236 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,236 Answer received: !ym
2024-06-18 13:48:01,237 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro259
e

2024-06-18 13:48:01,237 Answer received: !ybfalse
2024-06-18 13:48:01,238 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,240 Answer received: !yp
2024-06-18 13:48:01,240 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,240 Answer received: !yp
2024-06-18 13:48:01,241 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,241 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,241 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,241 Answer received: !ym
2024-06-18 13:48:01,242 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro259
e

2024-06-18 13:48:01,242 Answer received: !ybfalse
2024-06-18 13:48:01,243 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,247 Answer received: !yp
2024-06-18 13:48:01,247 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,248 Answer received: !yp
2024-06-18 13:48:01,248 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,248 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,248 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,248 Answer received: !ym
2024-06-18 13:48:01,249 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro259
e

2024-06-18 13:48:01,249 Answer received: !ybfalse
2024-06-18 13:48:01,249 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,251 Answer received: !yp
2024-06-18 13:48:01,251 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,252 Answer received: !yp
2024-06-18 13:48:01,252 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,252 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,252 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,252 Answer received: !ym
2024-06-18 13:48:01,253 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro259
e

2024-06-18 13:48:01,253 Answer received: !ybfalse
2024-06-18 13:48:01,253 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,256 Answer received: !yp
2024-06-18 13:48:01,256 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,256 Answer received: !yp
2024-06-18 13:48:01,256 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,257 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,257 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,257 Answer received: !ym
2024-06-18 13:48:01,257 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro259
e

2024-06-18 13:48:01,257 Answer received: !ybfalse
2024-06-18 13:48:01,258 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,260 Answer received: !yp
2024-06-18 13:48:01,260 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,260 Answer received: !yp
2024-06-18 13:48:01,261 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,261 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,261 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,262 Answer received: !ym
2024-06-18 13:48:01,262 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro259
e

2024-06-18 13:48:01,262 Answer received: !ybfalse
2024-06-18 13:48:01,262 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,265 Answer received: !yp
2024-06-18 13:48:01,265 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,265 Answer received: !yp
2024-06-18 13:48:01,265 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,266 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,266 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,266 Answer received: !ym
2024-06-18 13:48:01,266 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro259
e

2024-06-18 13:48:01,267 Answer received: !ybfalse
2024-06-18 13:48:01,267 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,269 Answer received: !yp
2024-06-18 13:48:01,270 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,270 Answer received: !yp
2024-06-18 13:48:01,270 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,270 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,270 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,271 Answer received: !ym
2024-06-18 13:48:01,271 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro259
e

2024-06-18 13:48:01,271 Answer received: !ybfalse
2024-06-18 13:48:01,271 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,273 Answer received: !yp
2024-06-18 13:48:01,273 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,273 Answer received: !yp
2024-06-18 13:48:01,274 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,274 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,274 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,274 Answer received: !ym
2024-06-18 13:48:01,274 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro259
e

2024-06-18 13:48:01,275 Answer received: !ybfalse
2024-06-18 13:48:01,275 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,277 Answer received: !yp
2024-06-18 13:48:01,277 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,278 Answer received: !yp
2024-06-18 13:48:01,278 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,278 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,278 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,278 Answer received: !ym
2024-06-18 13:48:01,279 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro259
e

2024-06-18 13:48:01,279 Answer received: !ybfalse
2024-06-18 13:48:01,279 Command to send: c
o259
getCause
e

2024-06-18 13:48:01,279 Answer received: !yro260
2024-06-18 13:48:01,279 Command to send: r
u
org
rj
e

2024-06-18 13:48:01,281 Answer received: !yp
2024-06-18 13:48:01,281 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:01,282 Answer received: !yp
2024-06-18 13:48:01,282 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:01,282 Answer received: !yp
2024-06-18 13:48:01,282 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:01,283 Answer received: !yp
2024-06-18 13:48:01,283 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:01,283 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:01,283 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:01,283 Answer received: !ym
2024-06-18 13:48:01,283 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro259
e

2024-06-18 13:48:01,284 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 42) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:01,284 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,286 Answer received: !yp
2024-06-18 13:48:01,286 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,286 Answer received: !yp
2024-06-18 13:48:01,286 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,287 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,287 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,287 Answer received: !ym
2024-06-18 13:48:01,287 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro260
e

2024-06-18 13:48:01,287 Answer received: !ybfalse
2024-06-18 13:48:01,287 Command to send: c
o259
toString
e

2024-06-18 13:48:01,288 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 42) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:48:01,288 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,289 Answer received: !yp
2024-06-18 13:48:01,290 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,290 Answer received: !yp
2024-06-18 13:48:01,290 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,291 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,291 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,291 Answer received: !ym
2024-06-18 13:48:01,291 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro260
e

2024-06-18 13:48:01,291 Answer received: !ybfalse
2024-06-18 13:48:01,292 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,293 Answer received: !yp
2024-06-18 13:48:01,293 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,294 Answer received: !yp
2024-06-18 13:48:01,294 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,294 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,294 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,294 Answer received: !ym
2024-06-18 13:48:01,295 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro260
e

2024-06-18 13:48:01,295 Answer received: !ybfalse
2024-06-18 13:48:01,295 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,297 Answer received: !yp
2024-06-18 13:48:01,297 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,297 Answer received: !yp
2024-06-18 13:48:01,297 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,297 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,297 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,298 Answer received: !ym
2024-06-18 13:48:01,298 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro260
e

2024-06-18 13:48:01,298 Answer received: !ybfalse
2024-06-18 13:48:01,298 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,300 Answer received: !yp
2024-06-18 13:48:01,300 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,300 Answer received: !yp
2024-06-18 13:48:01,300 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,301 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,301 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,301 Answer received: !ym
2024-06-18 13:48:01,301 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro260
e

2024-06-18 13:48:01,301 Answer received: !ybfalse
2024-06-18 13:48:01,301 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,303 Answer received: !yp
2024-06-18 13:48:01,303 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,304 Answer received: !yp
2024-06-18 13:48:01,304 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,304 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,304 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,304 Answer received: !ym
2024-06-18 13:48:01,304 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro260
e

2024-06-18 13:48:01,305 Answer received: !ybfalse
2024-06-18 13:48:01,305 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,306 Answer received: !yp
2024-06-18 13:48:01,307 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,307 Answer received: !yp
2024-06-18 13:48:01,307 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,307 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,307 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,308 Answer received: !ym
2024-06-18 13:48:01,308 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro260
e

2024-06-18 13:48:01,308 Answer received: !ybfalse
2024-06-18 13:48:01,308 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,310 Answer received: !yp
2024-06-18 13:48:01,310 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,310 Answer received: !yp
2024-06-18 13:48:01,311 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,311 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,311 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,311 Answer received: !ym
2024-06-18 13:48:01,311 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro260
e

2024-06-18 13:48:01,311 Answer received: !ybfalse
2024-06-18 13:48:01,312 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,313 Answer received: !yp
2024-06-18 13:48:01,313 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,314 Answer received: !yp
2024-06-18 13:48:01,314 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,314 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,314 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,314 Answer received: !ym
2024-06-18 13:48:01,314 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro260
e

2024-06-18 13:48:01,315 Answer received: !ybfalse
2024-06-18 13:48:01,315 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,317 Answer received: !yp
2024-06-18 13:48:01,317 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,317 Answer received: !yp
2024-06-18 13:48:01,317 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,317 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,318 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,318 Answer received: !ym
2024-06-18 13:48:01,318 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro260
e

2024-06-18 13:48:01,318 Answer received: !ybfalse
2024-06-18 13:48:01,318 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,320 Answer received: !yp
2024-06-18 13:48:01,320 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,321 Answer received: !yp
2024-06-18 13:48:01,321 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,321 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,321 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,321 Answer received: !ym
2024-06-18 13:48:01,321 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro260
e

2024-06-18 13:48:01,322 Answer received: !ybfalse
2024-06-18 13:48:01,322 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,323 Answer received: !yp
2024-06-18 13:48:01,324 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,324 Answer received: !yp
2024-06-18 13:48:01,324 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,324 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,324 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,325 Answer received: !ym
2024-06-18 13:48:01,325 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro260
e

2024-06-18 13:48:01,325 Answer received: !ybfalse
2024-06-18 13:48:01,325 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:01,328 Answer received: !yp
2024-06-18 13:48:01,328 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:01,328 Answer received: !yp
2024-06-18 13:48:01,329 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:01,329 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:01,329 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:01,329 Answer received: !ym
2024-06-18 13:48:01,329 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro260
e

2024-06-18 13:48:01,330 Answer received: !ybfalse
2024-06-18 13:48:01,330 Command to send: c
o260
getCause
e

2024-06-18 13:48:01,330 Answer received: !yn
2024-06-18 13:48:01,330 Command to send: r
u
org
rj
e

2024-06-18 13:48:01,332 Answer received: !yp
2024-06-18 13:48:01,332 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:01,333 Answer received: !yp
2024-06-18 13:48:01,333 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:01,333 Answer received: !yp
2024-06-18 13:48:01,333 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:01,334 Answer received: !yp
2024-06-18 13:48:01,334 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:01,334 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:01,334 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:01,334 Answer received: !ym
2024-06-18 13:48:01,335 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro260
e

2024-06-18 13:48:01,335 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:01,335 Command to send: c
o260
toString
e

2024-06-18 13:48:01,335 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:48:01,391 Command to send: p
ro259
e

2024-06-18 13:48:01,392 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 42) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:01,393 Command to send: p
ro259
e

2024-06-18 13:48:01,393 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 42) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:01,394 Command to send: p
ro259
e

2024-06-18 13:48:01,395 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 54.0 failed 1 times, most recent failure: Lost task 0.0 in stage 54.0 (TID 42) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:01,519 Command to send: m
d
o252
e

2024-06-18 13:48:01,519 Answer received: !yv
2024-06-18 13:48:01,520 Command to send: m
d
o253
e

2024-06-18 13:48:01,520 Answer received: !yv
2024-06-18 13:48:01,520 Command to send: m
d
o254
e

2024-06-18 13:48:01,520 Answer received: !yv
2024-06-18 13:48:01,520 Command to send: m
d
o255
e

2024-06-18 13:48:01,521 Answer received: !yv
2024-06-18 13:48:01,521 Command to send: m
d
o256
e

2024-06-18 13:48:01,521 Answer received: !yv
2024-06-18 13:48:01,521 Command to send: m
d
o257
e

2024-06-18 13:48:01,521 Answer received: !yv
2024-06-18 13:48:01,521 Command to send: m
d
o260
e

2024-06-18 13:48:01,522 Answer received: !yv
2024-06-18 13:48:01,522 Command to send: m
d
o250
e

2024-06-18 13:48:01,522 Answer received: !yv
2024-06-18 13:48:01,522 Command to send: m
d
o249
e

2024-06-18 13:48:01,522 Answer received: !yv
2024-06-18 13:48:31,588 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:48:31,589 Command to send: c
o223
write
e

2024-06-18 13:48:31,591 Answer received: !yro261
2024-06-18 13:48:31,592 Command to send: c
o261
mode
soverwrite
e

2024-06-18 13:48:31,592 Answer received: !yro262
2024-06-18 13:48:31,592 Command to send: c
o262
save
smodel_temp/GLM_V1.parquet
e

2024-06-18 13:48:32,073 Answer received: !xro263
2024-06-18 13:48:32,074 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,077 Answer received: !yp
2024-06-18 13:48:32,077 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,078 Answer received: !yp
2024-06-18 13:48:32,078 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,079 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,079 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,079 Answer received: !ym
2024-06-18 13:48:32,080 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro263
e

2024-06-18 13:48:32,081 Answer received: !ybfalse
2024-06-18 13:48:32,081 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,084 Answer received: !yp
2024-06-18 13:48:32,084 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,084 Answer received: !yp
2024-06-18 13:48:32,085 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,085 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,085 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,085 Answer received: !ym
2024-06-18 13:48:32,086 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro263
e

2024-06-18 13:48:32,086 Answer received: !ybfalse
2024-06-18 13:48:32,086 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,088 Answer received: !yp
2024-06-18 13:48:32,089 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,089 Answer received: !yp
2024-06-18 13:48:32,089 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,089 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,089 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,090 Answer received: !ym
2024-06-18 13:48:32,090 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro263
e

2024-06-18 13:48:32,090 Answer received: !ybfalse
2024-06-18 13:48:32,090 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,092 Answer received: !yp
2024-06-18 13:48:32,092 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,093 Answer received: !yp
2024-06-18 13:48:32,093 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,093 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,093 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,093 Answer received: !ym
2024-06-18 13:48:32,094 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro263
e

2024-06-18 13:48:32,094 Answer received: !ybfalse
2024-06-18 13:48:32,094 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,096 Answer received: !yp
2024-06-18 13:48:32,096 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,096 Answer received: !yp
2024-06-18 13:48:32,097 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,097 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,097 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,097 Answer received: !ym
2024-06-18 13:48:32,097 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro263
e

2024-06-18 13:48:32,098 Answer received: !ybfalse
2024-06-18 13:48:32,098 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,100 Answer received: !yp
2024-06-18 13:48:32,100 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,100 Answer received: !yp
2024-06-18 13:48:32,101 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,101 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,101 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,101 Answer received: !ym
2024-06-18 13:48:32,102 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro263
e

2024-06-18 13:48:32,102 Answer received: !ybfalse
2024-06-18 13:48:32,102 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,104 Answer received: !yp
2024-06-18 13:48:32,104 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,104 Answer received: !yp
2024-06-18 13:48:32,104 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,105 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,105 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,105 Answer received: !ym
2024-06-18 13:48:32,105 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro263
e

2024-06-18 13:48:32,106 Answer received: !ybfalse
2024-06-18 13:48:32,106 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,107 Answer received: !yp
2024-06-18 13:48:32,108 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,108 Answer received: !yp
2024-06-18 13:48:32,108 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,108 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,109 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,109 Answer received: !ym
2024-06-18 13:48:32,109 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro263
e

2024-06-18 13:48:32,109 Answer received: !ybfalse
2024-06-18 13:48:32,109 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,111 Answer received: !yp
2024-06-18 13:48:32,111 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,112 Answer received: !yp
2024-06-18 13:48:32,112 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,112 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,112 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,113 Answer received: !ym
2024-06-18 13:48:32,113 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro263
e

2024-06-18 13:48:32,113 Answer received: !ybfalse
2024-06-18 13:48:32,113 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,115 Answer received: !yp
2024-06-18 13:48:32,115 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,116 Answer received: !yp
2024-06-18 13:48:32,116 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,116 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,116 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,116 Answer received: !ym
2024-06-18 13:48:32,117 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro263
e

2024-06-18 13:48:32,117 Answer received: !ybfalse
2024-06-18 13:48:32,117 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,119 Answer received: !yp
2024-06-18 13:48:32,119 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,119 Answer received: !yp
2024-06-18 13:48:32,120 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,120 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,120 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,120 Answer received: !ym
2024-06-18 13:48:32,120 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro263
e

2024-06-18 13:48:32,121 Answer received: !ybfalse
2024-06-18 13:48:32,121 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,123 Answer received: !yp
2024-06-18 13:48:32,123 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,124 Answer received: !yp
2024-06-18 13:48:32,124 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,124 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,124 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,125 Answer received: !ym
2024-06-18 13:48:32,125 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro263
e

2024-06-18 13:48:32,125 Answer received: !ybfalse
2024-06-18 13:48:32,125 Command to send: c
o263
getCause
e

2024-06-18 13:48:32,126 Answer received: !yro264
2024-06-18 13:48:32,126 Command to send: r
u
org
rj
e

2024-06-18 13:48:32,128 Answer received: !yp
2024-06-18 13:48:32,128 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:32,128 Answer received: !yp
2024-06-18 13:48:32,128 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:32,129 Answer received: !yp
2024-06-18 13:48:32,129 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:32,129 Answer received: !yp
2024-06-18 13:48:32,130 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:32,130 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:32,130 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:32,130 Answer received: !ym
2024-06-18 13:48:32,131 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro263
e

2024-06-18 13:48:32,131 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 44) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:32,131 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,133 Answer received: !yp
2024-06-18 13:48:32,133 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,134 Answer received: !yp
2024-06-18 13:48:32,134 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,134 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,134 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,134 Answer received: !ym
2024-06-18 13:48:32,135 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro264
e

2024-06-18 13:48:32,135 Answer received: !ybfalse
2024-06-18 13:48:32,135 Command to send: c
o263
toString
e

2024-06-18 13:48:32,135 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 44) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:48:32,135 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,137 Answer received: !yp
2024-06-18 13:48:32,138 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,138 Answer received: !yp
2024-06-18 13:48:32,138 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,139 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,139 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,139 Answer received: !ym
2024-06-18 13:48:32,139 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro264
e

2024-06-18 13:48:32,140 Answer received: !ybfalse
2024-06-18 13:48:32,140 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,142 Answer received: !yp
2024-06-18 13:48:32,142 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,142 Answer received: !yp
2024-06-18 13:48:32,142 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,142 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,142 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,143 Answer received: !ym
2024-06-18 13:48:32,143 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro264
e

2024-06-18 13:48:32,143 Answer received: !ybfalse
2024-06-18 13:48:32,143 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,145 Answer received: !yp
2024-06-18 13:48:32,145 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,145 Answer received: !yp
2024-06-18 13:48:32,145 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,145 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,146 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,146 Answer received: !ym
2024-06-18 13:48:32,146 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro264
e

2024-06-18 13:48:32,146 Answer received: !ybfalse
2024-06-18 13:48:32,146 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,148 Answer received: !yp
2024-06-18 13:48:32,148 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,149 Answer received: !yp
2024-06-18 13:48:32,149 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,149 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,149 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,149 Answer received: !ym
2024-06-18 13:48:32,149 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro264
e

2024-06-18 13:48:32,150 Answer received: !ybfalse
2024-06-18 13:48:32,150 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,151 Answer received: !yp
2024-06-18 13:48:32,151 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,152 Answer received: !yp
2024-06-18 13:48:32,152 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,152 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,152 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,153 Answer received: !ym
2024-06-18 13:48:32,153 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro264
e

2024-06-18 13:48:32,153 Answer received: !ybfalse
2024-06-18 13:48:32,153 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,156 Answer received: !yp
2024-06-18 13:48:32,156 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,156 Answer received: !yp
2024-06-18 13:48:32,157 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,157 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,157 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,157 Answer received: !ym
2024-06-18 13:48:32,157 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro264
e

2024-06-18 13:48:32,158 Answer received: !ybfalse
2024-06-18 13:48:32,158 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,160 Answer received: !yp
2024-06-18 13:48:32,160 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,160 Answer received: !yp
2024-06-18 13:48:32,160 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,160 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,161 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,161 Answer received: !ym
2024-06-18 13:48:32,161 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro264
e

2024-06-18 13:48:32,161 Answer received: !ybfalse
2024-06-18 13:48:32,161 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,163 Answer received: !yp
2024-06-18 13:48:32,163 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,164 Answer received: !yp
2024-06-18 13:48:32,164 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,164 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,164 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,165 Answer received: !ym
2024-06-18 13:48:32,165 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro264
e

2024-06-18 13:48:32,165 Answer received: !ybfalse
2024-06-18 13:48:32,165 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,167 Answer received: !yp
2024-06-18 13:48:32,167 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,167 Answer received: !yp
2024-06-18 13:48:32,167 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,168 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,168 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,168 Answer received: !ym
2024-06-18 13:48:32,168 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro264
e

2024-06-18 13:48:32,168 Answer received: !ybfalse
2024-06-18 13:48:32,168 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,170 Answer received: !yp
2024-06-18 13:48:32,170 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,170 Answer received: !yp
2024-06-18 13:48:32,170 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,171 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,171 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,171 Answer received: !ym
2024-06-18 13:48:32,171 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro264
e

2024-06-18 13:48:32,171 Answer received: !ybfalse
2024-06-18 13:48:32,172 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,174 Answer received: !yp
2024-06-18 13:48:32,174 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,175 Answer received: !yp
2024-06-18 13:48:32,175 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,175 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,175 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,175 Answer received: !ym
2024-06-18 13:48:32,176 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro264
e

2024-06-18 13:48:32,176 Answer received: !ybfalse
2024-06-18 13:48:32,176 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:32,178 Answer received: !yp
2024-06-18 13:48:32,178 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:32,178 Answer received: !yp
2024-06-18 13:48:32,178 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:32,178 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:32,179 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:32,179 Answer received: !ym
2024-06-18 13:48:32,179 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro264
e

2024-06-18 13:48:32,179 Answer received: !ybfalse
2024-06-18 13:48:32,179 Command to send: c
o264
getCause
e

2024-06-18 13:48:32,179 Answer received: !yn
2024-06-18 13:48:32,180 Command to send: r
u
org
rj
e

2024-06-18 13:48:32,182 Answer received: !yp
2024-06-18 13:48:32,182 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:32,182 Answer received: !yp
2024-06-18 13:48:32,183 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:32,183 Answer received: !yp
2024-06-18 13:48:32,183 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:32,183 Answer received: !yp
2024-06-18 13:48:32,183 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:32,184 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:32,184 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:32,184 Answer received: !ym
2024-06-18 13:48:32,184 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro264
e

2024-06-18 13:48:32,184 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:32,185 Command to send: c
o264
toString
e

2024-06-18 13:48:32,185 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:48:32,238 Command to send: p
ro263
e

2024-06-18 13:48:32,239 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 44) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:32,240 Command to send: p
ro263
e

2024-06-18 13:48:32,241 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 44) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:32,242 Command to send: p
ro263
e

2024-06-18 13:48:32,242 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 56.0 failed 1 times, most recent failure: Lost task 0.0 in stage 56.0 (TID 44) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:32,539 Command to send: m
d
o261
e

2024-06-18 13:48:32,539 Answer received: !yv
2024-06-18 13:48:32,539 Command to send: m
d
o264
e

2024-06-18 13:48:32,539 Answer received: !yv
2024-06-18 13:48:59,336 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:48:59,337 Command to send: c
o223
write
e

2024-06-18 13:48:59,340 Answer received: !yro265
2024-06-18 13:48:59,340 Command to send: c
o265
mode
soverwrite
e

2024-06-18 13:48:59,341 Answer received: !yro266
2024-06-18 13:48:59,341 Command to send: c
o266
save
sname.parquet
e

2024-06-18 13:48:59,644 Answer received: !xro267
2024-06-18 13:48:59,645 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,649 Answer received: !yp
2024-06-18 13:48:59,649 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,650 Answer received: !yp
2024-06-18 13:48:59,651 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,652 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,652 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,653 Answer received: !ym
2024-06-18 13:48:59,653 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro267
e

2024-06-18 13:48:59,654 Answer received: !ybfalse
2024-06-18 13:48:59,654 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,658 Answer received: !yp
2024-06-18 13:48:59,658 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,659 Answer received: !yp
2024-06-18 13:48:59,659 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,660 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,660 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,660 Answer received: !ym
2024-06-18 13:48:59,661 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro267
e

2024-06-18 13:48:59,661 Answer received: !ybfalse
2024-06-18 13:48:59,661 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,666 Answer received: !yp
2024-06-18 13:48:59,666 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,668 Answer received: !yp
2024-06-18 13:48:59,668 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,668 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,669 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,669 Answer received: !ym
2024-06-18 13:48:59,669 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro267
e

2024-06-18 13:48:59,670 Answer received: !ybfalse
2024-06-18 13:48:59,670 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,673 Answer received: !yp
2024-06-18 13:48:59,673 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,673 Answer received: !yp
2024-06-18 13:48:59,673 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,674 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,674 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,674 Answer received: !ym
2024-06-18 13:48:59,674 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro267
e

2024-06-18 13:48:59,675 Answer received: !ybfalse
2024-06-18 13:48:59,675 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,679 Answer received: !yp
2024-06-18 13:48:59,679 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,680 Answer received: !yp
2024-06-18 13:48:59,680 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,680 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,680 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,680 Answer received: !ym
2024-06-18 13:48:59,681 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro267
e

2024-06-18 13:48:59,681 Answer received: !ybfalse
2024-06-18 13:48:59,681 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,684 Answer received: !yp
2024-06-18 13:48:59,684 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,684 Answer received: !yp
2024-06-18 13:48:59,685 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,685 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,685 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,685 Answer received: !ym
2024-06-18 13:48:59,686 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro267
e

2024-06-18 13:48:59,686 Answer received: !ybfalse
2024-06-18 13:48:59,687 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,689 Answer received: !yp
2024-06-18 13:48:59,689 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,689 Answer received: !yp
2024-06-18 13:48:59,689 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,690 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,690 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,690 Answer received: !ym
2024-06-18 13:48:59,690 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro267
e

2024-06-18 13:48:59,691 Answer received: !ybfalse
2024-06-18 13:48:59,691 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,693 Answer received: !yp
2024-06-18 13:48:59,693 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,694 Answer received: !yp
2024-06-18 13:48:59,694 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,694 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,694 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,695 Answer received: !ym
2024-06-18 13:48:59,695 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro267
e

2024-06-18 13:48:59,695 Answer received: !ybfalse
2024-06-18 13:48:59,695 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,697 Answer received: !yp
2024-06-18 13:48:59,697 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,698 Answer received: !yp
2024-06-18 13:48:59,698 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,698 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,698 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,698 Answer received: !ym
2024-06-18 13:48:59,699 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro267
e

2024-06-18 13:48:59,699 Answer received: !ybfalse
2024-06-18 13:48:59,699 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,701 Answer received: !yp
2024-06-18 13:48:59,701 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,701 Answer received: !yp
2024-06-18 13:48:59,701 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,702 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,702 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,702 Answer received: !ym
2024-06-18 13:48:59,702 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro267
e

2024-06-18 13:48:59,702 Answer received: !ybfalse
2024-06-18 13:48:59,703 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,705 Answer received: !yp
2024-06-18 13:48:59,705 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,706 Answer received: !yp
2024-06-18 13:48:59,706 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,706 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,706 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,706 Answer received: !ym
2024-06-18 13:48:59,707 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro267
e

2024-06-18 13:48:59,707 Answer received: !ybfalse
2024-06-18 13:48:59,707 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,710 Answer received: !yp
2024-06-18 13:48:59,710 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,710 Answer received: !yp
2024-06-18 13:48:59,710 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,711 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,711 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,711 Answer received: !ym
2024-06-18 13:48:59,711 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro267
e

2024-06-18 13:48:59,711 Answer received: !ybfalse
2024-06-18 13:48:59,711 Command to send: c
o267
getCause
e

2024-06-18 13:48:59,712 Answer received: !yro268
2024-06-18 13:48:59,712 Command to send: r
u
org
rj
e

2024-06-18 13:48:59,714 Answer received: !yp
2024-06-18 13:48:59,714 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:59,715 Answer received: !yp
2024-06-18 13:48:59,715 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:59,716 Answer received: !yp
2024-06-18 13:48:59,716 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:59,717 Answer received: !yp
2024-06-18 13:48:59,717 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:59,717 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:59,717 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:59,718 Answer received: !ym
2024-06-18 13:48:59,718 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro267
e

2024-06-18 13:48:59,719 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 49) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:59,719 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,722 Answer received: !yp
2024-06-18 13:48:59,723 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,723 Answer received: !yp
2024-06-18 13:48:59,723 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,723 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,724 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,724 Answer received: !ym
2024-06-18 13:48:59,724 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro268
e

2024-06-18 13:48:59,724 Answer received: !ybfalse
2024-06-18 13:48:59,724 Command to send: c
o267
toString
e

2024-06-18 13:48:59,725 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 49) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:48:59,725 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,727 Answer received: !yp
2024-06-18 13:48:59,727 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,728 Answer received: !yp
2024-06-18 13:48:59,728 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,728 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,728 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,729 Answer received: !ym
2024-06-18 13:48:59,729 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro268
e

2024-06-18 13:48:59,729 Answer received: !ybfalse
2024-06-18 13:48:59,730 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,732 Answer received: !yp
2024-06-18 13:48:59,733 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,733 Answer received: !yp
2024-06-18 13:48:59,733 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,733 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,734 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,734 Answer received: !ym
2024-06-18 13:48:59,734 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro268
e

2024-06-18 13:48:59,735 Answer received: !ybfalse
2024-06-18 13:48:59,735 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,738 Answer received: !yp
2024-06-18 13:48:59,738 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,738 Answer received: !yp
2024-06-18 13:48:59,739 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,739 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,739 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,739 Answer received: !ym
2024-06-18 13:48:59,739 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro268
e

2024-06-18 13:48:59,739 Answer received: !ybfalse
2024-06-18 13:48:59,740 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,741 Answer received: !yp
2024-06-18 13:48:59,741 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,742 Answer received: !yp
2024-06-18 13:48:59,742 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,742 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,742 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,742 Answer received: !ym
2024-06-18 13:48:59,742 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro268
e

2024-06-18 13:48:59,743 Answer received: !ybfalse
2024-06-18 13:48:59,743 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,745 Answer received: !yp
2024-06-18 13:48:59,745 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,745 Answer received: !yp
2024-06-18 13:48:59,745 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,746 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,746 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,746 Answer received: !ym
2024-06-18 13:48:59,746 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro268
e

2024-06-18 13:48:59,746 Answer received: !ybfalse
2024-06-18 13:48:59,747 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,748 Answer received: !yp
2024-06-18 13:48:59,748 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,749 Answer received: !yp
2024-06-18 13:48:59,749 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,749 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,749 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,749 Answer received: !ym
2024-06-18 13:48:59,750 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro268
e

2024-06-18 13:48:59,750 Answer received: !ybfalse
2024-06-18 13:48:59,750 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,752 Answer received: !yp
2024-06-18 13:48:59,752 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,752 Answer received: !yp
2024-06-18 13:48:59,752 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,753 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,753 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,753 Answer received: !ym
2024-06-18 13:48:59,753 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro268
e

2024-06-18 13:48:59,753 Answer received: !ybfalse
2024-06-18 13:48:59,754 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,755 Answer received: !yp
2024-06-18 13:48:59,755 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,756 Answer received: !yp
2024-06-18 13:48:59,756 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,756 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,756 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,756 Answer received: !ym
2024-06-18 13:48:59,756 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro268
e

2024-06-18 13:48:59,756 Answer received: !ybfalse
2024-06-18 13:48:59,756 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,757 Answer received: !yp
2024-06-18 13:48:59,757 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,758 Answer received: !yp
2024-06-18 13:48:59,758 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,758 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,758 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,758 Answer received: !ym
2024-06-18 13:48:59,759 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro268
e

2024-06-18 13:48:59,759 Answer received: !ybfalse
2024-06-18 13:48:59,759 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,760 Answer received: !yp
2024-06-18 13:48:59,761 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,761 Answer received: !yp
2024-06-18 13:48:59,761 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,761 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,761 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,761 Answer received: !ym
2024-06-18 13:48:59,762 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro268
e

2024-06-18 13:48:59,762 Answer received: !ybfalse
2024-06-18 13:48:59,762 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,763 Answer received: !yp
2024-06-18 13:48:59,764 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,764 Answer received: !yp
2024-06-18 13:48:59,764 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,764 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,764 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,764 Answer received: !ym
2024-06-18 13:48:59,765 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro268
e

2024-06-18 13:48:59,765 Answer received: !ybfalse
2024-06-18 13:48:59,765 Command to send: r
u
py4j
rj
e

2024-06-18 13:48:59,767 Answer received: !yp
2024-06-18 13:48:59,767 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:48:59,767 Answer received: !yp
2024-06-18 13:48:59,767 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:48:59,768 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:48:59,768 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:48:59,768 Answer received: !ym
2024-06-18 13:48:59,768 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro268
e

2024-06-18 13:48:59,768 Answer received: !ybfalse
2024-06-18 13:48:59,768 Command to send: c
o268
getCause
e

2024-06-18 13:48:59,769 Answer received: !yn
2024-06-18 13:48:59,769 Command to send: r
u
org
rj
e

2024-06-18 13:48:59,770 Answer received: !yp
2024-06-18 13:48:59,771 Command to send: r
u
org.apache
rj
e

2024-06-18 13:48:59,771 Answer received: !yp
2024-06-18 13:48:59,771 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:48:59,771 Answer received: !yp
2024-06-18 13:48:59,772 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:48:59,772 Answer received: !yp
2024-06-18 13:48:59,772 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:48:59,772 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:48:59,772 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:48:59,772 Answer received: !ym
2024-06-18 13:48:59,773 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro268
e

2024-06-18 13:48:59,773 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:59,773 Command to send: c
o268
toString
e

2024-06-18 13:48:59,773 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:48:59,901 Command to send: p
ro267
e

2024-06-18 13:48:59,902 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 49) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:59,903 Command to send: p
ro267
e

2024-06-18 13:48:59,903 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 49) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:48:59,904 Command to send: p
ro267
e

2024-06-18 13:48:59,905 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 62.0 failed 1 times, most recent failure: Lost task 0.0 in stage 62.0 (TID 49) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:00,553 Command to send: m
d
o265
e

2024-06-18 13:49:00,553 Answer received: !yv
2024-06-18 13:49:00,553 Command to send: m
d
o268
e

2024-06-18 13:49:00,553 Answer received: !yv
2024-06-18 13:49:00,554 Command to send: m
d
o263
e

2024-06-18 13:49:00,554 Answer received: !yv
2024-06-18 13:49:00,554 Command to send: m
d
o262
e

2024-06-18 13:49:00,554 Answer received: !yv
2024-06-18 13:49:00,554 Command to send: m
d
o105
e

2024-06-18 13:49:00,555 Answer received: !yv
2024-06-18 13:49:00,555 Command to send: m
d
o229
e

2024-06-18 13:49:00,555 Answer received: !yv
2024-06-18 13:49:00,555 Command to send: m
d
o228
e

2024-06-18 13:49:00,555 Answer received: !yv
2024-06-18 13:49:00,555 Command to send: m
d
o233
e

2024-06-18 13:49:00,556 Answer received: !yv
2024-06-18 13:49:00,556 Command to send: m
d
o232
e

2024-06-18 13:49:00,556 Answer received: !yv
2024-06-18 13:49:00,556 Command to send: m
d
o240
e

2024-06-18 13:49:00,557 Answer received: !yv
2024-06-18 13:49:00,557 Command to send: m
d
o239
e

2024-06-18 13:49:00,557 Answer received: !yv
2024-06-18 13:49:00,557 Command to send: m
d
o246
e

2024-06-18 13:49:00,557 Answer received: !yv
2024-06-18 13:49:00,557 Command to send: m
d
o245
e

2024-06-18 13:49:00,558 Answer received: !yv
2024-06-18 13:49:00,558 Command to send: m
d
o259
e

2024-06-18 13:49:00,558 Answer received: !yv
2024-06-18 13:49:00,558 Command to send: m
d
o258
e

2024-06-18 13:49:00,558 Answer received: !yv
2024-06-18 13:49:24,789 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:49:24,790 Command to send: c
o223
write
e

2024-06-18 13:49:24,792 Answer received: !yro269
2024-06-18 13:49:24,793 Command to send: c
o269
mode
soverwrite
e

2024-06-18 13:49:24,793 Answer received: !yro270
2024-06-18 13:49:24,793 Command to send: c
o270
format
sparquet
e

2024-06-18 13:49:24,794 Answer received: !yro271
2024-06-18 13:49:24,795 Command to send: c
o271
save
sname.parquet
e

2024-06-18 13:49:25,164 Answer received: !xro272
2024-06-18 13:49:25,165 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,170 Answer received: !yp
2024-06-18 13:49:25,170 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,171 Answer received: !yp
2024-06-18 13:49:25,171 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,172 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,172 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,172 Answer received: !ym
2024-06-18 13:49:25,173 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro272
e

2024-06-18 13:49:25,174 Answer received: !ybfalse
2024-06-18 13:49:25,174 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,177 Answer received: !yp
2024-06-18 13:49:25,177 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,178 Answer received: !yp
2024-06-18 13:49:25,179 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,179 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,179 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,179 Answer received: !ym
2024-06-18 13:49:25,180 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro272
e

2024-06-18 13:49:25,180 Answer received: !ybfalse
2024-06-18 13:49:25,180 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,184 Answer received: !yp
2024-06-18 13:49:25,184 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,185 Answer received: !yp
2024-06-18 13:49:25,185 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,185 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,185 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,185 Answer received: !ym
2024-06-18 13:49:25,186 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro272
e

2024-06-18 13:49:25,186 Answer received: !ybfalse
2024-06-18 13:49:25,186 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,190 Answer received: !yp
2024-06-18 13:49:25,190 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,191 Answer received: !yp
2024-06-18 13:49:25,191 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,192 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,192 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,192 Answer received: !ym
2024-06-18 13:49:25,192 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro272
e

2024-06-18 13:49:25,193 Answer received: !ybfalse
2024-06-18 13:49:25,193 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,195 Answer received: !yp
2024-06-18 13:49:25,195 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,195 Answer received: !yp
2024-06-18 13:49:25,195 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,196 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,196 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,196 Answer received: !ym
2024-06-18 13:49:25,196 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro272
e

2024-06-18 13:49:25,197 Answer received: !ybfalse
2024-06-18 13:49:25,197 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,200 Answer received: !yp
2024-06-18 13:49:25,200 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,201 Answer received: !yp
2024-06-18 13:49:25,201 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,201 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,201 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,201 Answer received: !ym
2024-06-18 13:49:25,202 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro272
e

2024-06-18 13:49:25,202 Answer received: !ybfalse
2024-06-18 13:49:25,202 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,205 Answer received: !yp
2024-06-18 13:49:25,206 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,206 Answer received: !yp
2024-06-18 13:49:25,207 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,207 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,207 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,207 Answer received: !ym
2024-06-18 13:49:25,208 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro272
e

2024-06-18 13:49:25,208 Answer received: !ybfalse
2024-06-18 13:49:25,208 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,210 Answer received: !yp
2024-06-18 13:49:25,210 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,210 Answer received: !yp
2024-06-18 13:49:25,210 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,211 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,211 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,211 Answer received: !ym
2024-06-18 13:49:25,211 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro272
e

2024-06-18 13:49:25,211 Answer received: !ybfalse
2024-06-18 13:49:25,212 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,214 Answer received: !yp
2024-06-18 13:49:25,214 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,214 Answer received: !yp
2024-06-18 13:49:25,214 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,214 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,215 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,215 Answer received: !ym
2024-06-18 13:49:25,215 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro272
e

2024-06-18 13:49:25,215 Answer received: !ybfalse
2024-06-18 13:49:25,215 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,217 Answer received: !yp
2024-06-18 13:49:25,217 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,218 Answer received: !yp
2024-06-18 13:49:25,218 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,218 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,218 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,218 Answer received: !ym
2024-06-18 13:49:25,219 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro272
e

2024-06-18 13:49:25,219 Answer received: !ybfalse
2024-06-18 13:49:25,219 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,221 Answer received: !yp
2024-06-18 13:49:25,221 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,221 Answer received: !yp
2024-06-18 13:49:25,222 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,222 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,222 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,222 Answer received: !ym
2024-06-18 13:49:25,222 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro272
e

2024-06-18 13:49:25,223 Answer received: !ybfalse
2024-06-18 13:49:25,223 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,224 Answer received: !yp
2024-06-18 13:49:25,225 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,225 Answer received: !yp
2024-06-18 13:49:25,225 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,225 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,225 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,225 Answer received: !ym
2024-06-18 13:49:25,226 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro272
e

2024-06-18 13:49:25,226 Answer received: !ybfalse
2024-06-18 13:49:25,226 Command to send: c
o272
getCause
e

2024-06-18 13:49:25,226 Answer received: !yro273
2024-06-18 13:49:25,226 Command to send: r
u
org
rj
e

2024-06-18 13:49:25,228 Answer received: !yp
2024-06-18 13:49:25,228 Command to send: r
u
org.apache
rj
e

2024-06-18 13:49:25,229 Answer received: !yp
2024-06-18 13:49:25,229 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:49:25,229 Answer received: !yp
2024-06-18 13:49:25,229 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:49:25,230 Answer received: !yp
2024-06-18 13:49:25,230 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:49:25,230 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:49:25,230 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:49:25,230 Answer received: !ym
2024-06-18 13:49:25,231 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro272
e

2024-06-18 13:49:25,231 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 53) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:25,232 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,234 Answer received: !yp
2024-06-18 13:49:25,234 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,234 Answer received: !yp
2024-06-18 13:49:25,235 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,235 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,235 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,235 Answer received: !ym
2024-06-18 13:49:25,235 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro273
e

2024-06-18 13:49:25,236 Answer received: !ybfalse
2024-06-18 13:49:25,236 Command to send: c
o272
toString
e

2024-06-18 13:49:25,236 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 53) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:49:25,236 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,238 Answer received: !yp
2024-06-18 13:49:25,238 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,239 Answer received: !yp
2024-06-18 13:49:25,239 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,239 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,239 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,239 Answer received: !ym
2024-06-18 13:49:25,240 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro273
e

2024-06-18 13:49:25,240 Answer received: !ybfalse
2024-06-18 13:49:25,240 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,242 Answer received: !yp
2024-06-18 13:49:25,242 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,243 Answer received: !yp
2024-06-18 13:49:25,243 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,243 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,244 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,244 Answer received: !ym
2024-06-18 13:49:25,244 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro273
e

2024-06-18 13:49:25,245 Answer received: !ybfalse
2024-06-18 13:49:25,245 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,247 Answer received: !yp
2024-06-18 13:49:25,247 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,247 Answer received: !yp
2024-06-18 13:49:25,248 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,248 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,248 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,248 Answer received: !ym
2024-06-18 13:49:25,248 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro273
e

2024-06-18 13:49:25,249 Answer received: !ybfalse
2024-06-18 13:49:25,249 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,250 Answer received: !yp
2024-06-18 13:49:25,250 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,251 Answer received: !yp
2024-06-18 13:49:25,251 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,251 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,251 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,251 Answer received: !ym
2024-06-18 13:49:25,252 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro273
e

2024-06-18 13:49:25,252 Answer received: !ybfalse
2024-06-18 13:49:25,252 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,254 Answer received: !yp
2024-06-18 13:49:25,254 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,254 Answer received: !yp
2024-06-18 13:49:25,254 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,255 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,255 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,255 Answer received: !ym
2024-06-18 13:49:25,255 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro273
e

2024-06-18 13:49:25,255 Answer received: !ybfalse
2024-06-18 13:49:25,256 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,257 Answer received: !yp
2024-06-18 13:49:25,257 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,258 Answer received: !yp
2024-06-18 13:49:25,258 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,258 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,258 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,258 Answer received: !ym
2024-06-18 13:49:25,259 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro273
e

2024-06-18 13:49:25,259 Answer received: !ybfalse
2024-06-18 13:49:25,259 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,260 Answer received: !yp
2024-06-18 13:49:25,261 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,261 Answer received: !yp
2024-06-18 13:49:25,261 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,261 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,261 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,261 Answer received: !ym
2024-06-18 13:49:25,262 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro273
e

2024-06-18 13:49:25,262 Answer received: !ybfalse
2024-06-18 13:49:25,262 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,264 Answer received: !yp
2024-06-18 13:49:25,264 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,264 Answer received: !yp
2024-06-18 13:49:25,264 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,264 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,264 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,265 Answer received: !ym
2024-06-18 13:49:25,265 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro273
e

2024-06-18 13:49:25,265 Answer received: !ybfalse
2024-06-18 13:49:25,265 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,267 Answer received: !yp
2024-06-18 13:49:25,267 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,267 Answer received: !yp
2024-06-18 13:49:25,268 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,268 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,268 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,268 Answer received: !ym
2024-06-18 13:49:25,268 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro273
e

2024-06-18 13:49:25,269 Answer received: !ybfalse
2024-06-18 13:49:25,269 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,271 Answer received: !yp
2024-06-18 13:49:25,271 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,271 Answer received: !yp
2024-06-18 13:49:25,271 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,272 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,272 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,272 Answer received: !ym
2024-06-18 13:49:25,272 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro273
e

2024-06-18 13:49:25,272 Answer received: !ybfalse
2024-06-18 13:49:25,273 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,274 Answer received: !yp
2024-06-18 13:49:25,274 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,275 Answer received: !yp
2024-06-18 13:49:25,275 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,275 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,275 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,275 Answer received: !ym
2024-06-18 13:49:25,275 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro273
e

2024-06-18 13:49:25,276 Answer received: !ybfalse
2024-06-18 13:49:25,276 Command to send: r
u
py4j
rj
e

2024-06-18 13:49:25,277 Answer received: !yp
2024-06-18 13:49:25,277 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:49:25,278 Answer received: !yp
2024-06-18 13:49:25,278 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:49:25,278 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:49:25,278 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:49:25,278 Answer received: !ym
2024-06-18 13:49:25,278 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro273
e

2024-06-18 13:49:25,279 Answer received: !ybfalse
2024-06-18 13:49:25,279 Command to send: c
o273
getCause
e

2024-06-18 13:49:25,279 Answer received: !yn
2024-06-18 13:49:25,279 Command to send: r
u
org
rj
e

2024-06-18 13:49:25,281 Answer received: !yp
2024-06-18 13:49:25,281 Command to send: r
u
org.apache
rj
e

2024-06-18 13:49:25,281 Answer received: !yp
2024-06-18 13:49:25,281 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:49:25,282 Answer received: !yp
2024-06-18 13:49:25,282 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:49:25,282 Answer received: !yp
2024-06-18 13:49:25,282 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:49:25,283 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:49:25,283 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:49:25,283 Answer received: !ym
2024-06-18 13:49:25,283 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro273
e

2024-06-18 13:49:25,284 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:25,284 Command to send: c
o273
toString
e

2024-06-18 13:49:25,284 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:49:25,339 Command to send: p
ro272
e

2024-06-18 13:49:25,340 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 53) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:25,341 Command to send: p
ro272
e

2024-06-18 13:49:25,341 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 53) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:25,342 Command to send: p
ro272
e

2024-06-18 13:49:25,343 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 66.0 failed 1 times, most recent failure: Lost task 0.0 in stage 66.0 (TID 53) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:49:25,570 Command to send: m
d
o269
e

2024-06-18 13:49:25,570 Answer received: !yv
2024-06-18 13:49:25,570 Command to send: m
d
o270
e

2024-06-18 13:49:25,571 Answer received: !yv
2024-06-18 13:49:25,571 Command to send: m
d
o273
e

2024-06-18 13:49:25,571 Answer received: !yv
2024-06-18 13:51:02,095 Command to send: c
o223
write
e

2024-06-18 13:51:02,098 Answer received: !yro274
2024-06-18 13:51:02,098 Command to send: c
o274
csv
stest.cvs
e

2024-06-18 13:51:02,359 Answer received: !xro275
2024-06-18 13:51:02,360 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,363 Answer received: !yp
2024-06-18 13:51:02,363 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,363 Answer received: !yp
2024-06-18 13:51:02,363 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,364 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,364 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,364 Answer received: !ym
2024-06-18 13:51:02,364 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro275
e

2024-06-18 13:51:02,365 Answer received: !ybfalse
2024-06-18 13:51:02,365 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,367 Answer received: !yp
2024-06-18 13:51:02,367 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,368 Answer received: !yp
2024-06-18 13:51:02,368 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,368 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,368 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,369 Answer received: !ym
2024-06-18 13:51:02,369 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro275
e

2024-06-18 13:51:02,369 Answer received: !ybfalse
2024-06-18 13:51:02,369 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,371 Answer received: !yp
2024-06-18 13:51:02,371 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,372 Answer received: !yp
2024-06-18 13:51:02,372 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,372 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,372 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,373 Answer received: !ym
2024-06-18 13:51:02,373 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro275
e

2024-06-18 13:51:02,373 Answer received: !ybfalse
2024-06-18 13:51:02,373 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,375 Answer received: !yp
2024-06-18 13:51:02,375 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,376 Answer received: !yp
2024-06-18 13:51:02,376 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,376 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,376 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,376 Answer received: !ym
2024-06-18 13:51:02,377 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro275
e

2024-06-18 13:51:02,377 Answer received: !ybfalse
2024-06-18 13:51:02,377 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,378 Answer received: !yp
2024-06-18 13:51:02,379 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,379 Answer received: !yp
2024-06-18 13:51:02,379 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,379 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,379 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,380 Answer received: !ym
2024-06-18 13:51:02,380 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro275
e

2024-06-18 13:51:02,380 Answer received: !ybfalse
2024-06-18 13:51:02,380 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,382 Answer received: !yp
2024-06-18 13:51:02,382 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,382 Answer received: !yp
2024-06-18 13:51:02,382 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,382 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,383 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,383 Answer received: !ym
2024-06-18 13:51:02,383 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro275
e

2024-06-18 13:51:02,383 Answer received: !ybfalse
2024-06-18 13:51:02,383 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,385 Answer received: !yp
2024-06-18 13:51:02,385 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,386 Answer received: !yp
2024-06-18 13:51:02,386 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,386 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,386 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,386 Answer received: !ym
2024-06-18 13:51:02,386 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro275
e

2024-06-18 13:51:02,387 Answer received: !ybfalse
2024-06-18 13:51:02,387 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,388 Answer received: !yp
2024-06-18 13:51:02,388 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,389 Answer received: !yp
2024-06-18 13:51:02,389 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,389 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,389 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,389 Answer received: !ym
2024-06-18 13:51:02,389 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro275
e

2024-06-18 13:51:02,390 Answer received: !ybfalse
2024-06-18 13:51:02,390 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,392 Answer received: !yp
2024-06-18 13:51:02,392 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,392 Answer received: !yp
2024-06-18 13:51:02,392 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,392 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,393 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,393 Answer received: !ym
2024-06-18 13:51:02,393 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro275
e

2024-06-18 13:51:02,393 Answer received: !ybfalse
2024-06-18 13:51:02,393 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,395 Answer received: !yp
2024-06-18 13:51:02,395 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,395 Answer received: !yp
2024-06-18 13:51:02,395 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,395 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,396 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,396 Answer received: !ym
2024-06-18 13:51:02,396 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro275
e

2024-06-18 13:51:02,396 Answer received: !ybfalse
2024-06-18 13:51:02,396 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,398 Answer received: !yp
2024-06-18 13:51:02,398 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,398 Answer received: !yp
2024-06-18 13:51:02,398 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,398 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,399 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,399 Answer received: !ym
2024-06-18 13:51:02,399 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro275
e

2024-06-18 13:51:02,399 Answer received: !ybfalse
2024-06-18 13:51:02,399 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,401 Answer received: !yp
2024-06-18 13:51:02,401 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,401 Answer received: !yp
2024-06-18 13:51:02,401 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,401 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,402 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,402 Answer received: !ym
2024-06-18 13:51:02,402 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro275
e

2024-06-18 13:51:02,402 Answer received: !ybfalse
2024-06-18 13:51:02,402 Command to send: c
o275
getCause
e

2024-06-18 13:51:02,403 Answer received: !yro276
2024-06-18 13:51:02,403 Command to send: r
u
org
rj
e

2024-06-18 13:51:02,404 Answer received: !yp
2024-06-18 13:51:02,404 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:02,405 Answer received: !yp
2024-06-18 13:51:02,405 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:02,405 Answer received: !yp
2024-06-18 13:51:02,405 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:51:02,405 Answer received: !yp
2024-06-18 13:51:02,406 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:51:02,406 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:51:02,406 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:51:02,406 Answer received: !ym
2024-06-18 13:51:02,406 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro275
e

2024-06-18 13:51:02,407 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 57) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:02,407 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,409 Answer received: !yp
2024-06-18 13:51:02,409 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,409 Answer received: !yp
2024-06-18 13:51:02,409 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,410 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,410 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,410 Answer received: !ym
2024-06-18 13:51:02,410 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.api.python.PythonException
ro276
e

2024-06-18 13:51:02,410 Answer received: !ybfalse
2024-06-18 13:51:02,410 Command to send: c
o275
toString
e

2024-06-18 13:51:02,411 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 57) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:
2024-06-18 13:51:02,411 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,412 Answer received: !yp
2024-06-18 13:51:02,412 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,413 Answer received: !yp
2024-06-18 13:51:02,413 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,413 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,413 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,413 Answer received: !ym
2024-06-18 13:51:02,413 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro276
e

2024-06-18 13:51:02,414 Answer received: !ybfalse
2024-06-18 13:51:02,414 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,415 Answer received: !yp
2024-06-18 13:51:02,415 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,416 Answer received: !yp
2024-06-18 13:51:02,416 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,416 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,416 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,416 Answer received: !ym
2024-06-18 13:51:02,416 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro276
e

2024-06-18 13:51:02,417 Answer received: !ybfalse
2024-06-18 13:51:02,417 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,418 Answer received: !yp
2024-06-18 13:51:02,418 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,419 Answer received: !yp
2024-06-18 13:51:02,419 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,419 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,419 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,419 Answer received: !ym
2024-06-18 13:51:02,419 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro276
e

2024-06-18 13:51:02,420 Answer received: !ybfalse
2024-06-18 13:51:02,420 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,421 Answer received: !yp
2024-06-18 13:51:02,421 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,422 Answer received: !yp
2024-06-18 13:51:02,422 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,422 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,422 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,422 Answer received: !ym
2024-06-18 13:51:02,422 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro276
e

2024-06-18 13:51:02,423 Answer received: !ybfalse
2024-06-18 13:51:02,423 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,424 Answer received: !yp
2024-06-18 13:51:02,425 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,425 Answer received: !yp
2024-06-18 13:51:02,425 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,425 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,425 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,425 Answer received: !ym
2024-06-18 13:51:02,426 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro276
e

2024-06-18 13:51:02,426 Answer received: !ybfalse
2024-06-18 13:51:02,426 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,427 Answer received: !yp
2024-06-18 13:51:02,428 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,428 Answer received: !yp
2024-06-18 13:51:02,428 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,428 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,428 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,429 Answer received: !ym
2024-06-18 13:51:02,429 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro276
e

2024-06-18 13:51:02,429 Answer received: !ybfalse
2024-06-18 13:51:02,429 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,430 Answer received: !yp
2024-06-18 13:51:02,431 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,431 Answer received: !yp
2024-06-18 13:51:02,431 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,431 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,431 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,431 Answer received: !ym
2024-06-18 13:51:02,432 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArithmeticException
ro276
e

2024-06-18 13:51:02,432 Answer received: !ybfalse
2024-06-18 13:51:02,432 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,434 Answer received: !yp
2024-06-18 13:51:02,434 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,434 Answer received: !yp
2024-06-18 13:51:02,434 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,434 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,435 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,435 Answer received: !ym
2024-06-18 13:51:02,435 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.UnsupportedOperationException
ro276
e

2024-06-18 13:51:02,435 Answer received: !ybfalse
2024-06-18 13:51:02,435 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,437 Answer received: !yp
2024-06-18 13:51:02,437 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,437 Answer received: !yp
2024-06-18 13:51:02,437 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,438 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,438 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,438 Answer received: !ym
2024-06-18 13:51:02,438 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.ArrayIndexOutOfBoundsException
ro276
e

2024-06-18 13:51:02,438 Answer received: !ybfalse
2024-06-18 13:51:02,439 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,440 Answer received: !yp
2024-06-18 13:51:02,440 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,440 Answer received: !yp
2024-06-18 13:51:02,441 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,441 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,441 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,441 Answer received: !ym
2024-06-18 13:51:02,441 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.time.DateTimeException
ro276
e

2024-06-18 13:51:02,441 Answer received: !ybfalse
2024-06-18 13:51:02,442 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,443 Answer received: !yp
2024-06-18 13:51:02,443 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,444 Answer received: !yp
2024-06-18 13:51:02,444 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,444 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,444 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,444 Answer received: !ym
2024-06-18 13:51:02,444 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkRuntimeException
ro276
e

2024-06-18 13:51:02,445 Answer received: !ybfalse
2024-06-18 13:51:02,445 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:02,446 Answer received: !yp
2024-06-18 13:51:02,446 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:02,447 Answer received: !yp
2024-06-18 13:51:02,447 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:02,447 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:02,447 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:02,447 Answer received: !ym
2024-06-18 13:51:02,447 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.SparkUpgradeException
ro276
e

2024-06-18 13:51:02,448 Answer received: !ybfalse
2024-06-18 13:51:02,448 Command to send: c
o276
getCause
e

2024-06-18 13:51:02,448 Answer received: !yn
2024-06-18 13:51:02,448 Command to send: r
u
org
rj
e

2024-06-18 13:51:02,449 Answer received: !yp
2024-06-18 13:51:02,450 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:02,450 Answer received: !yp
2024-06-18 13:51:02,450 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:02,450 Answer received: !yp
2024-06-18 13:51:02,450 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:51:02,451 Answer received: !yp
2024-06-18 13:51:02,451 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:51:02,451 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:51:02,451 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:51:02,451 Answer received: !ym
2024-06-18 13:51:02,452 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro276
e

2024-06-18 13:51:02,452 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:02,452 Command to send: c
o276
toString
e

2024-06-18 13:51:02,452 Answer received: !ysorg.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.
2024-06-18 13:51:02,512 Command to send: p
ro275
e

2024-06-18 13:51:02,513 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 57) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:02,514 Command to send: p
ro275
e

2024-06-18 13:51:02,514 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 57) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:02,515 Command to send: p
ro275
e

2024-06-18 13:51:02,515 Answer received: !ysorg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 70.0 failed 1 times, most recent failure: Lost task 0.0 in stage 70.0 (TID 57) (LAPTOP-S1GNSTJG executor driver): org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n\nDriver stacktrace:\r\n	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n	at scala.Option.foreach(Option.scala:407)\r\n	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/d:/Documents/Data Science/Big_Data_Management/Project2/model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\r\n	at org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:780)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\r\n	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\r\n	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\r\n	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n	at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n	at org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:02,621 Command to send: m
d
o272
e

2024-06-18 13:51:02,621 Answer received: !yv
2024-06-18 13:51:02,621 Command to send: m
d
o271
e

2024-06-18 13:51:02,621 Answer received: !yv
2024-06-18 13:51:02,622 Command to send: m
d
o276
e

2024-06-18 13:51:02,622 Answer received: !yv
2024-06-18 13:51:30,730 Command to send: r
u
SparkConf
rj
e

2024-06-18 13:51:30,731 Answer received: !ycorg.apache.spark.SparkConf
2024-06-18 13:51:30,731 Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-06-18 13:51:30,732 Answer received: !yro277
2024-06-18 13:51:30,732 Command to send: c
o277
set
sspark.master
slocal
e

2024-06-18 13:51:30,732 Answer received: !yro278
2024-06-18 13:51:30,733 Command to send: c
o277
set
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:51:30,733 Answer received: !yro279
2024-06-18 13:51:30,733 Command to send: c
o277
getAll
e

2024-06-18 13:51:30,734 Answer received: !yto280
2024-06-18 13:51:30,734 Command to send: a
e
o280
e

2024-06-18 13:51:30,734 Answer received: !yi6
2024-06-18 13:51:30,734 Command to send: a
g
o280
i0
e

2024-06-18 13:51:30,735 Answer received: !yro281
2024-06-18 13:51:30,735 Command to send: c
o281
_1
e

2024-06-18 13:51:30,735 Answer received: !ysspark.master
2024-06-18 13:51:30,735 Command to send: c
o281
_2
e

2024-06-18 13:51:30,736 Answer received: !yslocal
2024-06-18 13:51:30,736 Command to send: a
e
o280
e

2024-06-18 13:51:30,736 Answer received: !yi6
2024-06-18 13:51:30,736 Command to send: a
g
o280
i1
e

2024-06-18 13:51:30,736 Answer received: !yro282
2024-06-18 13:51:30,737 Command to send: c
o282
_1
e

2024-06-18 13:51:30,737 Answer received: !ysspark.submit.pyFiles
2024-06-18 13:51:30,737 Command to send: c
o282
_2
e

2024-06-18 13:51:30,738 Answer received: !ys
2024-06-18 13:51:30,738 Command to send: a
e
o280
e

2024-06-18 13:51:30,738 Answer received: !yi6
2024-06-18 13:51:30,739 Command to send: a
g
o280
i2
e

2024-06-18 13:51:30,739 Answer received: !yro283
2024-06-18 13:51:30,739 Command to send: c
o283
_1
e

2024-06-18 13:51:30,739 Answer received: !ysspark.app.submitTime
2024-06-18 13:51:30,740 Command to send: c
o283
_2
e

2024-06-18 13:51:30,740 Answer received: !ys1718709747485
2024-06-18 13:51:30,740 Command to send: a
e
o280
e

2024-06-18 13:51:30,740 Answer received: !yi6
2024-06-18 13:51:30,740 Command to send: a
g
o280
i3
e

2024-06-18 13:51:30,741 Answer received: !yro284
2024-06-18 13:51:30,741 Command to send: c
o284
_1
e

2024-06-18 13:51:30,741 Answer received: !ysspark.submit.deployMode
2024-06-18 13:51:30,741 Command to send: c
o284
_2
e

2024-06-18 13:51:30,741 Answer received: !ysclient
2024-06-18 13:51:30,742 Command to send: a
e
o280
e

2024-06-18 13:51:30,742 Answer received: !yi6
2024-06-18 13:51:30,742 Command to send: a
g
o280
i4
e

2024-06-18 13:51:30,742 Answer received: !yro285
2024-06-18 13:51:30,742 Command to send: c
o285
_1
e

2024-06-18 13:51:30,743 Answer received: !ysspark.app.name
2024-06-18 13:51:30,743 Command to send: c
o285
_2
e

2024-06-18 13:51:30,743 Answer received: !ysFormatted zone loader
2024-06-18 13:51:30,743 Command to send: a
e
o280
e

2024-06-18 13:51:30,743 Answer received: !yi6
2024-06-18 13:51:30,744 Command to send: a
g
o280
i5
e

2024-06-18 13:51:30,744 Answer received: !yro286
2024-06-18 13:51:30,744 Command to send: c
o286
_1
e

2024-06-18 13:51:30,745 Answer received: !ysspark.ui.showConsoleProgress
2024-06-18 13:51:30,745 Command to send: c
o286
_2
e

2024-06-18 13:51:30,745 Answer received: !ystrue
2024-06-18 13:51:30,745 Command to send: a
e
o280
e

2024-06-18 13:51:30,745 Answer received: !yi6
2024-06-18 13:51:30,746 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:51:30,748 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:51:30,748 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:51:30,748 Answer received: !yro287
2024-06-18 13:51:30,748 Command to send: i
java.util.HashMap
e

2024-06-18 13:51:30,749 Answer received: !yao288
2024-06-18 13:51:30,749 Command to send: c
o288
put
sspark.master
slocal
e

2024-06-18 13:51:30,749 Answer received: !yn
2024-06-18 13:51:30,749 Command to send: c
o288
put
sspark.submit.pyFiles
s
e

2024-06-18 13:51:30,750 Answer received: !yn
2024-06-18 13:51:30,750 Command to send: c
o288
put
sspark.app.submitTime
s1718709747485
e

2024-06-18 13:51:30,750 Answer received: !yn
2024-06-18 13:51:30,751 Command to send: c
o288
put
sspark.submit.deployMode
sclient
e

2024-06-18 13:51:30,751 Answer received: !yn
2024-06-18 13:51:30,751 Command to send: c
o288
put
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:51:30,751 Answer received: !yn
2024-06-18 13:51:30,752 Command to send: c
o288
put
sspark.ui.showConsoleProgress
strue
e

2024-06-18 13:51:30,752 Answer received: !yn
2024-06-18 13:51:30,752 Command to send: c
o287
applyModifiableSettings
ro25
ro288
e

2024-06-18 13:51:30,752 Answer received: !yv
2024-06-18 13:51:30,753 Spark sesion correctly initialized
2024-06-18 13:51:30,753 Command to send: c
o25
read
e

2024-06-18 13:51:30,753 Answer received: !yro289
2024-06-18 13:51:30,754 Command to send: c
o289
option
smultiline
strue
e

2024-06-18 13:51:30,754 Answer received: !yro290
2024-06-18 13:51:30,754 Command to send: c
o290
format
sparquet
e

2024-06-18 13:51:30,754 Answer received: !yro291
2024-06-18 13:51:30,755 Command to send: c
o291
load
smodel_temp/20240614-2304-renda_familiar
e

2024-06-18 13:51:30,966 Answer received: !yro292
2024-06-18 13:51:30,966 Command to send: c
o25
read
e

2024-06-18 13:51:30,967 Answer received: !yro293
2024-06-18 13:51:30,967 Command to send: c
o293
option
smultiline
strue
e

2024-06-18 13:51:30,967 Answer received: !yro294
2024-06-18 13:51:30,968 Command to send: c
o294
format
sparquet
e

2024-06-18 13:51:30,968 Answer received: !yro295
2024-06-18 13:51:30,968 Command to send: c
o295
load
smodel_temp/20240614-2306-idealista
e

2024-06-18 13:51:31,064 Answer received: !yro296
2024-06-18 13:51:31,064 Command to send: c
o25
read
e

2024-06-18 13:51:31,065 Answer received: !yro297
2024-06-18 13:51:31,065 Command to send: c
o297
option
smultiline
strue
e

2024-06-18 13:51:31,065 Answer received: !yro298
2024-06-18 13:51:31,066 Command to send: c
o298
format
sparquet
e

2024-06-18 13:51:31,066 Answer received: !yro299
2024-06-18 13:51:31,066 Command to send: c
o299
load
smodel_temp/20240614-2308-lookup_renta_idealista
e

2024-06-18 13:51:31,161 Answer received: !yro300
2024-06-18 13:51:31,161 Command to send: c
o25
read
e

2024-06-18 13:51:31,162 Answer received: !yro301
2024-06-18 13:51:31,162 Command to send: c
o301
option
smultiline
strue
e

2024-06-18 13:51:31,163 Answer received: !yro302
2024-06-18 13:51:31,163 Command to send: c
o302
format
sparquet
e

2024-06-18 13:51:31,164 Answer received: !yro303
2024-06-18 13:51:31,164 Command to send: c
o303
load
smodel_temp/20240615-1328-hotels
e

2024-06-18 13:51:31,302 Answer received: !yro304
2024-06-18 13:51:31,637 Command to send: m
d
o278
e

2024-06-18 13:51:31,637 Answer received: !yv
2024-06-18 13:51:31,637 Command to send: m
d
o279
e

2024-06-18 13:51:31,638 Answer received: !yv
2024-06-18 13:51:31,638 Command to send: m
d
o280
e

2024-06-18 13:51:31,638 Answer received: !yv
2024-06-18 13:51:31,638 Command to send: m
d
o288
e

2024-06-18 13:51:31,638 Answer received: !yv
2024-06-18 13:51:32,694 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,696 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,696 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,696 Answer received: !ym
2024-06-18 13:51:32,697 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:51:32,697 Answer received: !yro305
2024-06-18 13:51:32,697 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,699 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,699 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,700 Answer received: !ym
2024-06-18 13:51:32,700 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:51:32,700 Answer received: !yro306
2024-06-18 13:51:32,700 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,702 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,702 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,702 Answer received: !ym
2024-06-18 13:51:32,702 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:51:32,703 Answer received: !yro307
2024-06-18 13:51:32,703 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:32,703 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:32,704 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:32,704 Answer received: !ym
2024-06-18 13:51:32,704 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:32,704 Answer received: !ylo308
2024-06-18 13:51:32,705 Command to send: c
o308
add
ro305
e

2024-06-18 13:51:32,705 Answer received: !ybtrue
2024-06-18 13:51:32,705 Command to send: c
o308
add
ro306
e

2024-06-18 13:51:32,705 Answer received: !ybtrue
2024-06-18 13:51:32,705 Command to send: c
o308
add
ro307
e

2024-06-18 13:51:32,706 Answer received: !ybtrue
2024-06-18 13:51:32,706 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro308
e

2024-06-18 13:51:32,706 Answer received: !yro309
2024-06-18 13:51:32,706 Command to send: c
o292
select
ro309
e

2024-06-18 13:51:32,713 Answer received: !yro310
2024-06-18 13:51:32,713 Command to send: c
o310
schema
e

2024-06-18 13:51:32,714 Answer received: !yro311
2024-06-18 13:51:32,714 Command to send: c
o311
treeString
e

2024-06-18 13:51:32,714 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:51:32,715 Command to send: c
o310
count
e

2024-06-18 13:51:32,812 Answer received: !yL811
2024-06-18 13:51:32,813 Command to send: c
o310
schema
e

2024-06-18 13:51:32,813 Answer received: !yro312
2024-06-18 13:51:32,813 Command to send: c
o312
json
e

2024-06-18 13:51:32,814 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:51:32,814 Command to send: c
o310
na
e

2024-06-18 13:51:32,815 Answer received: !yro313
2024-06-18 13:51:32,816 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:32,816 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:32,816 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:32,817 Answer received: !ym
2024-06-18 13:51:32,817 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:32,817 Answer received: !ylo314
2024-06-18 13:51:32,817 Command to send: c
o314
add
sNom_Districte
e

2024-06-18 13:51:32,818 Answer received: !ybtrue
2024-06-18 13:51:32,818 Command to send: c
o314
add
sCodi_Districte
e

2024-06-18 13:51:32,818 Answer received: !ybtrue
2024-06-18 13:51:32,818 Command to send: c
o314
add
sndex RFD Barcelona = 100
e

2024-06-18 13:51:32,819 Answer received: !ybtrue
2024-06-18 13:51:32,819 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro314
e

2024-06-18 13:51:32,819 Answer received: !yro315
2024-06-18 13:51:32,819 Command to send: c
o313
drop
i3
ro315
e

2024-06-18 13:51:32,822 Answer received: !yro316
2024-06-18 13:51:32,822 Command to send: c
o316
count
e

2024-06-18 13:51:32,973 Answer received: !yL811
2024-06-18 13:51:32,974 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,976 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,976 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,977 Answer received: !ym
2024-06-18 13:51:32,977 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:32,977 Answer received: !yro317
2024-06-18 13:51:32,977 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,979 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,979 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,980 Answer received: !ym
2024-06-18 13:51:32,980 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:51:32,980 Answer received: !yro318
2024-06-18 13:51:32,980 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,983 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,983 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,983 Answer received: !ym
2024-06-18 13:51:32,983 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:51:32,983 Answer received: !yro319
2024-06-18 13:51:32,983 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,984 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,985 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,985 Answer received: !ym
2024-06-18 13:51:32,985 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:51:32,985 Answer received: !yro320
2024-06-18 13:51:32,985 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,987 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,987 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,987 Answer received: !ym
2024-06-18 13:51:32,988 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:51:32,988 Answer received: !yro321
2024-06-18 13:51:32,989 Command to send: r
u
functions
rj
e

2024-06-18 13:51:32,990 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:32,990 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:32,991 Answer received: !ym
2024-06-18 13:51:32,991 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:51:32,991 Answer received: !yro322
2024-06-18 13:51:32,991 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:32,992 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:32,992 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:32,992 Answer received: !ym
2024-06-18 13:51:32,992 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:32,992 Answer received: !ylo323
2024-06-18 13:51:32,993 Command to send: c
o323
add
ro317
e

2024-06-18 13:51:32,993 Answer received: !ybtrue
2024-06-18 13:51:32,993 Command to send: c
o323
add
ro318
e

2024-06-18 13:51:32,993 Answer received: !ybtrue
2024-06-18 13:51:32,993 Command to send: c
o323
add
ro319
e

2024-06-18 13:51:32,993 Answer received: !ybtrue
2024-06-18 13:51:32,994 Command to send: c
o323
add
ro320
e

2024-06-18 13:51:32,994 Answer received: !ybtrue
2024-06-18 13:51:32,994 Command to send: c
o323
add
ro321
e

2024-06-18 13:51:32,994 Answer received: !ybtrue
2024-06-18 13:51:32,994 Command to send: c
o323
add
ro322
e

2024-06-18 13:51:32,994 Answer received: !ybtrue
2024-06-18 13:51:32,994 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro323
e

2024-06-18 13:51:32,995 Answer received: !yro324
2024-06-18 13:51:32,995 Command to send: c
o296
select
ro324
e

2024-06-18 13:51:33,000 Answer received: !yro325
2024-06-18 13:51:33,001 Command to send: c
o325
schema
e

2024-06-18 13:51:33,001 Answer received: !yro326
2024-06-18 13:51:33,001 Command to send: c
o326
treeString
e

2024-06-18 13:51:33,002 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:51:33,002 Command to send: c
o325
count
e

2024-06-18 13:51:33,108 Answer received: !yL20189
2024-06-18 13:51:33,108 Command to send: c
o325
schema
e

2024-06-18 13:51:33,108 Answer received: !yro327
2024-06-18 13:51:33,108 Command to send: c
o327
json
e

2024-06-18 13:51:33,109 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:51:33,109 Command to send: c
o325
na
e

2024-06-18 13:51:33,110 Answer received: !yro328
2024-06-18 13:51:33,110 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:33,111 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:33,111 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:33,112 Answer received: !ym
2024-06-18 13:51:33,112 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:33,112 Answer received: !ylo329
2024-06-18 13:51:33,113 Command to send: c
o329
add
sdistrict
e

2024-06-18 13:51:33,113 Answer received: !ybtrue
2024-06-18 13:51:33,113 Command to send: c
o329
add
snumPhotos
e

2024-06-18 13:51:33,113 Answer received: !ybtrue
2024-06-18 13:51:33,114 Command to send: c
o329
add
sprice
e

2024-06-18 13:51:33,114 Answer received: !ybtrue
2024-06-18 13:51:33,114 Command to send: c
o329
add
sfloor
e

2024-06-18 13:51:33,114 Answer received: !ybtrue
2024-06-18 13:51:33,115 Command to send: c
o329
add
ssize
e

2024-06-18 13:51:33,115 Answer received: !ybtrue
2024-06-18 13:51:33,115 Command to send: c
o329
add
sbathrooms
e

2024-06-18 13:51:33,115 Answer received: !ybtrue
2024-06-18 13:51:33,115 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro329
e

2024-06-18 13:51:33,116 Answer received: !yro330
2024-06-18 13:51:33,116 Command to send: c
o328
drop
i6
ro330
e

2024-06-18 13:51:33,119 Answer received: !yro331
2024-06-18 13:51:33,119 Command to send: c
o331
count
e

2024-06-18 13:51:33,299 Answer received: !yL15545
2024-06-18 13:51:33,299 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,303 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,303 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,303 Answer received: !ym
2024-06-18 13:51:33,304 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:33,304 Answer received: !yro332
2024-06-18 13:51:33,305 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,307 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,307 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,308 Answer received: !ym
2024-06-18 13:51:33,308 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:51:33,308 Answer received: !yro333
2024-06-18 13:51:33,309 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:33,310 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:33,310 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:33,310 Answer received: !ym
2024-06-18 13:51:33,311 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:33,311 Answer received: !ylo334
2024-06-18 13:51:33,311 Command to send: c
o334
add
ro332
e

2024-06-18 13:51:33,311 Answer received: !ybtrue
2024-06-18 13:51:33,311 Command to send: c
o334
add
ro333
e

2024-06-18 13:51:33,312 Answer received: !ybtrue
2024-06-18 13:51:33,312 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro334
e

2024-06-18 13:51:33,312 Answer received: !yro335
2024-06-18 13:51:33,312 Command to send: c
o300
select
ro335
e

2024-06-18 13:51:33,318 Answer received: !yro336
2024-06-18 13:51:33,319 Command to send: c
o336
schema
e

2024-06-18 13:51:33,319 Answer received: !yro337
2024-06-18 13:51:33,319 Command to send: c
o337
treeString
e

2024-06-18 13:51:33,319 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:51:33,320 Command to send: c
o336
count
e

2024-06-18 13:51:33,415 Answer received: !yL113
2024-06-18 13:51:33,415 Command to send: c
o336
schema
e

2024-06-18 13:51:33,415 Answer received: !yro338
2024-06-18 13:51:33,415 Command to send: c
o338
json
e

2024-06-18 13:51:33,416 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:51:33,416 Command to send: c
o336
na
e

2024-06-18 13:51:33,417 Answer received: !yro339
2024-06-18 13:51:33,417 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:33,417 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:33,417 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:33,418 Answer received: !ym
2024-06-18 13:51:33,418 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:33,418 Answer received: !ylo340
2024-06-18 13:51:33,418 Command to send: c
o340
add
sdistrict
e

2024-06-18 13:51:33,419 Answer received: !ybtrue
2024-06-18 13:51:33,419 Command to send: c
o340
add
sdistrict_id
e

2024-06-18 13:51:33,419 Answer received: !ybtrue
2024-06-18 13:51:33,419 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro340
e

2024-06-18 13:51:33,419 Answer received: !yro341
2024-06-18 13:51:33,419 Command to send: c
o339
drop
i2
ro341
e

2024-06-18 13:51:33,422 Answer received: !yro342
2024-06-18 13:51:33,422 Command to send: c
o342
count
e

2024-06-18 13:51:33,516 Answer received: !yL113
2024-06-18 13:51:33,516 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,518 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,518 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,519 Answer received: !ym
2024-06-18 13:51:33,519 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:51:33,519 Answer received: !yro343
2024-06-18 13:51:33,519 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,521 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,521 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,522 Answer received: !ym
2024-06-18 13:51:33,522 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:51:33,522 Answer received: !yro344
2024-06-18 13:51:33,522 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,523 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,524 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,524 Answer received: !ym
2024-06-18 13:51:33,524 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:51:33,524 Answer received: !yro345
2024-06-18 13:51:33,524 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,525 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,526 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,526 Answer received: !ym
2024-06-18 13:51:33,526 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:51:33,526 Answer received: !yro346
2024-06-18 13:51:33,526 Command to send: r
u
functions
rj
e

2024-06-18 13:51:33,527 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:33,527 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:33,528 Answer received: !ym
2024-06-18 13:51:33,528 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:51:33,528 Answer received: !yro347
2024-06-18 13:51:33,528 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:33,529 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:33,529 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:33,529 Answer received: !ym
2024-06-18 13:51:33,529 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:33,530 Answer received: !ylo348
2024-06-18 13:51:33,530 Command to send: c
o348
add
ro343
e

2024-06-18 13:51:33,530 Answer received: !ybtrue
2024-06-18 13:51:33,530 Command to send: c
o348
add
ro344
e

2024-06-18 13:51:33,531 Answer received: !ybtrue
2024-06-18 13:51:33,531 Command to send: c
o348
add
ro345
e

2024-06-18 13:51:33,531 Answer received: !ybtrue
2024-06-18 13:51:33,531 Command to send: c
o348
add
ro346
e

2024-06-18 13:51:33,531 Answer received: !ybtrue
2024-06-18 13:51:33,531 Command to send: c
o348
add
ro347
e

2024-06-18 13:51:33,531 Answer received: !ybtrue
2024-06-18 13:51:33,532 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro348
e

2024-06-18 13:51:33,532 Answer received: !yro349
2024-06-18 13:51:33,532 Command to send: c
o304
select
ro349
e

2024-06-18 13:51:33,536 Answer received: !xro350
2024-06-18 13:51:33,536 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:33,539 Answer received: !yp
2024-06-18 13:51:33,539 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:33,539 Answer received: !yp
2024-06-18 13:51:33,539 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:33,539 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:33,540 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:33,540 Answer received: !ym
2024-06-18 13:51:33,540 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro350
e

2024-06-18 13:51:33,540 Answer received: !ybfalse
2024-06-18 13:51:33,540 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:33,542 Answer received: !yp
2024-06-18 13:51:33,542 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:33,542 Answer received: !yp
2024-06-18 13:51:33,543 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:33,543 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:33,543 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:33,543 Answer received: !ym
2024-06-18 13:51:33,543 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro350
e

2024-06-18 13:51:33,543 Answer received: !ybtrue
2024-06-18 13:51:33,544 Command to send: c
o350
getMessage
e

2024-06-18 13:51:33,544 Answer received: !ys[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n
2024-06-18 13:51:33,544 Command to send: r
u
org
rj
e

2024-06-18 13:51:33,546 Answer received: !yp
2024-06-18 13:51:33,546 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:33,547 Answer received: !yp
2024-06-18 13:51:33,547 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:33,547 Answer received: !yp
2024-06-18 13:51:33,547 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:51:33,548 Answer received: !yp
2024-06-18 13:51:33,548 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:51:33,548 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:51:33,548 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:51:33,548 Answer received: !ym
2024-06-18 13:51:33,548 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro350
e

2024-06-18 13:51:33,549 Answer received: !ysorg.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)\r\n	at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4351)\r\n	at org.apache.spark.sql.Dataset.select(Dataset.scala:1540)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:33,549 Command to send: c
o350
getCause
e

2024-06-18 13:51:33,550 Answer received: !yn
2024-06-18 13:51:33,607 Command to send: r
u
org
rj
e

2024-06-18 13:51:33,609 Answer received: !yp
2024-06-18 13:51:33,609 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:33,609 Answer received: !yp
2024-06-18 13:51:33,609 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:33,610 Answer received: !yp
2024-06-18 13:51:33,610 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:33,610 Answer received: !yp
2024-06-18 13:51:33,610 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:33,611 Answer received: !yp
2024-06-18 13:51:33,611 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:33,611 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:33,611 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,612 Answer received: !ym
2024-06-18 13:51:33,612 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,612 Answer received: !yro351
2024-06-18 13:51:33,612 Command to send: c
o351
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:33,612 Answer received: !ybfalse
2024-06-18 13:51:33,614 Command to send: r
u
org
rj
e

2024-06-18 13:51:33,616 Answer received: !yp
2024-06-18 13:51:33,616 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:33,616 Answer received: !yp
2024-06-18 13:51:33,617 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:33,617 Answer received: !yp
2024-06-18 13:51:33,617 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:33,618 Answer received: !yp
2024-06-18 13:51:33,618 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:33,618 Answer received: !yp
2024-06-18 13:51:33,618 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:33,618 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:33,618 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,619 Answer received: !ym
2024-06-18 13:51:33,619 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,619 Answer received: !yro352
2024-06-18 13:51:33,619 Command to send: c
o352
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:33,620 Answer received: !ybfalse
2024-06-18 13:51:33,620 Command to send: r
u
org
rj
e

2024-06-18 13:51:33,622 Answer received: !yp
2024-06-18 13:51:33,622 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:33,623 Answer received: !yp
2024-06-18 13:51:33,623 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:33,623 Answer received: !yp
2024-06-18 13:51:33,623 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:33,624 Answer received: !yp
2024-06-18 13:51:33,624 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:33,624 Answer received: !yp
2024-06-18 13:51:33,625 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:33,625 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:33,625 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,625 Answer received: !ym
2024-06-18 13:51:33,625 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:33,625 Answer received: !yro353
2024-06-18 13:51:33,626 Command to send: c
o353
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:33,626 Answer received: !ybfalse
2024-06-18 13:51:33,640 Command to send: m
d
o281
e

2024-06-18 13:51:33,640 Answer received: !yv
2024-06-18 13:51:33,640 Command to send: m
d
o282
e

2024-06-18 13:51:33,640 Answer received: !yv
2024-06-18 13:51:33,640 Command to send: m
d
o283
e

2024-06-18 13:51:33,641 Answer received: !yv
2024-06-18 13:51:33,641 Command to send: m
d
o284
e

2024-06-18 13:51:33,641 Answer received: !yv
2024-06-18 13:51:33,641 Command to send: m
d
o285
e

2024-06-18 13:51:33,641 Answer received: !yv
2024-06-18 13:51:33,641 Command to send: m
d
o286
e

2024-06-18 13:51:33,642 Answer received: !yv
2024-06-18 13:51:33,642 Command to send: m
d
o287
e

2024-06-18 13:51:33,642 Answer received: !yv
2024-06-18 13:51:33,642 Command to send: m
d
o289
e

2024-06-18 13:51:33,642 Answer received: !yv
2024-06-18 13:51:33,642 Command to send: m
d
o290
e

2024-06-18 13:51:33,643 Answer received: !yv
2024-06-18 13:51:33,643 Command to send: m
d
o291
e

2024-06-18 13:51:33,643 Answer received: !yv
2024-06-18 13:51:33,643 Command to send: m
d
o293
e

2024-06-18 13:51:33,643 Answer received: !yv
2024-06-18 13:51:33,643 Command to send: m
d
o294
e

2024-06-18 13:51:33,644 Answer received: !yv
2024-06-18 13:51:33,644 Command to send: m
d
o295
e

2024-06-18 13:51:33,644 Answer received: !yv
2024-06-18 13:51:33,644 Command to send: m
d
o297
e

2024-06-18 13:51:33,644 Answer received: !yv
2024-06-18 13:51:33,644 Command to send: m
d
o298
e

2024-06-18 13:51:33,645 Answer received: !yv
2024-06-18 13:51:33,645 Command to send: m
d
o299
e

2024-06-18 13:51:33,645 Answer received: !yv
2024-06-18 13:51:33,645 Command to send: m
d
o301
e

2024-06-18 13:51:33,645 Answer received: !yv
2024-06-18 13:51:33,646 Command to send: m
d
o302
e

2024-06-18 13:51:33,646 Answer received: !yv
2024-06-18 13:51:33,646 Command to send: m
d
o303
e

2024-06-18 13:51:33,646 Answer received: !yv
2024-06-18 13:51:33,646 Command to send: m
d
o308
e

2024-06-18 13:51:33,647 Answer received: !yv
2024-06-18 13:51:33,647 Command to send: m
d
o314
e

2024-06-18 13:51:33,647 Answer received: !yv
2024-06-18 13:51:33,647 Command to send: m
d
o323
e

2024-06-18 13:51:33,647 Answer received: !yv
2024-06-18 13:51:33,647 Command to send: m
d
o329
e

2024-06-18 13:51:33,648 Answer received: !yv
2024-06-18 13:51:33,648 Command to send: m
d
o334
e

2024-06-18 13:51:33,648 Answer received: !yv
2024-06-18 13:51:33,648 Command to send: m
d
o340
e

2024-06-18 13:51:33,648 Answer received: !yv
2024-06-18 13:51:33,648 Command to send: m
d
o348
e

2024-06-18 13:51:33,649 Answer received: !yv
2024-06-18 13:51:33,649 Command to send: m
d
o305
e

2024-06-18 13:51:33,649 Answer received: !yv
2024-06-18 13:51:33,649 Command to send: m
d
o306
e

2024-06-18 13:51:33,649 Answer received: !yv
2024-06-18 13:51:33,649 Command to send: m
d
o307
e

2024-06-18 13:51:33,650 Answer received: !yv
2024-06-18 13:51:33,650 Command to send: m
d
o309
e

2024-06-18 13:51:33,650 Answer received: !yv
2024-06-18 13:51:33,650 Command to send: m
d
o310
e

2024-06-18 13:51:33,650 Answer received: !yv
2024-06-18 13:51:33,650 Command to send: m
d
o311
e

2024-06-18 13:51:33,651 Answer received: !yv
2024-06-18 13:51:33,651 Command to send: m
d
o312
e

2024-06-18 13:51:33,651 Answer received: !yv
2024-06-18 13:51:33,651 Command to send: m
d
o313
e

2024-06-18 13:51:33,651 Answer received: !yv
2024-06-18 13:51:33,652 Command to send: m
d
o315
e

2024-06-18 13:51:33,652 Answer received: !yv
2024-06-18 13:51:33,652 Command to send: m
d
o317
e

2024-06-18 13:51:33,652 Answer received: !yv
2024-06-18 13:51:33,652 Command to send: m
d
o318
e

2024-06-18 13:51:33,652 Answer received: !yv
2024-06-18 13:51:33,652 Command to send: m
d
o319
e

2024-06-18 13:51:33,653 Answer received: !yv
2024-06-18 13:51:33,653 Command to send: m
d
o320
e

2024-06-18 13:51:33,653 Answer received: !yv
2024-06-18 13:51:33,653 Command to send: m
d
o321
e

2024-06-18 13:51:33,654 Answer received: !yv
2024-06-18 13:51:33,654 Command to send: m
d
o322
e

2024-06-18 13:51:33,654 Answer received: !yv
2024-06-18 13:51:33,654 Command to send: m
d
o324
e

2024-06-18 13:51:33,654 Answer received: !yv
2024-06-18 13:51:33,654 Command to send: m
d
o325
e

2024-06-18 13:51:33,655 Answer received: !yv
2024-06-18 13:51:33,655 Command to send: m
d
o326
e

2024-06-18 13:51:33,655 Answer received: !yv
2024-06-18 13:51:33,655 Command to send: m
d
o327
e

2024-06-18 13:51:33,655 Answer received: !yv
2024-06-18 13:51:33,655 Command to send: m
d
o328
e

2024-06-18 13:51:33,656 Answer received: !yv
2024-06-18 13:51:33,656 Command to send: m
d
o330
e

2024-06-18 13:51:33,656 Answer received: !yv
2024-06-18 13:51:33,656 Command to send: m
d
o332
e

2024-06-18 13:51:33,656 Answer received: !yv
2024-06-18 13:51:33,657 Command to send: m
d
o333
e

2024-06-18 13:51:33,657 Answer received: !yv
2024-06-18 13:51:33,657 Command to send: m
d
o335
e

2024-06-18 13:51:33,657 Answer received: !yv
2024-06-18 13:51:33,657 Command to send: m
d
o336
e

2024-06-18 13:51:33,658 Answer received: !yv
2024-06-18 13:51:33,658 Command to send: m
d
o337
e

2024-06-18 13:51:33,658 Answer received: !yv
2024-06-18 13:51:33,658 Command to send: m
d
o338
e

2024-06-18 13:51:33,658 Answer received: !yv
2024-06-18 13:51:33,658 Command to send: m
d
o339
e

2024-06-18 13:51:33,659 Answer received: !yv
2024-06-18 13:51:33,659 Command to send: m
d
o341
e

2024-06-18 13:51:33,659 Answer received: !yv
2024-06-18 13:51:33,659 Command to send: m
d
o343
e

2024-06-18 13:51:33,659 Answer received: !yv
2024-06-18 13:51:33,659 Command to send: m
d
o344
e

2024-06-18 13:51:33,660 Answer received: !yv
2024-06-18 13:51:33,660 Command to send: m
d
o345
e

2024-06-18 13:51:33,660 Answer received: !yv
2024-06-18 13:51:33,660 Command to send: m
d
o346
e

2024-06-18 13:51:33,660 Answer received: !yv
2024-06-18 13:51:33,660 Command to send: m
d
o347
e

2024-06-18 13:51:33,661 Answer received: !yv
2024-06-18 13:51:33,661 Command to send: m
d
o296
e

2024-06-18 13:51:33,661 Answer received: !yv
2024-06-18 13:51:33,661 Command to send: m
d
o300
e

2024-06-18 13:51:33,661 Answer received: !yv
2024-06-18 13:51:33,662 Command to send: m
d
o292
e

2024-06-18 13:51:33,662 Answer received: !yv
2024-06-18 13:51:35,664 Command to send: m
d
o351
e

2024-06-18 13:51:35,664 Answer received: !yv
2024-06-18 13:51:35,664 Command to send: m
d
o352
e

2024-06-18 13:51:35,665 Answer received: !yv
2024-06-18 13:51:35,665 Command to send: m
d
o353
e

2024-06-18 13:51:35,665 Answer received: !yv
2024-06-18 13:51:45,351 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,353 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,353 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,353 Answer received: !ym
2024-06-18 13:51:45,353 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:51:45,354 Answer received: !yro354
2024-06-18 13:51:45,354 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,355 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,355 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,355 Answer received: !ym
2024-06-18 13:51:45,356 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:51:45,356 Answer received: !yro355
2024-06-18 13:51:45,356 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,357 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,357 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,358 Answer received: !ym
2024-06-18 13:51:45,358 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:51:45,358 Answer received: !yro356
2024-06-18 13:51:45,358 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,359 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,359 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,359 Answer received: !ym
2024-06-18 13:51:45,360 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,360 Answer received: !ylo357
2024-06-18 13:51:45,360 Command to send: c
o357
add
ro354
e

2024-06-18 13:51:45,360 Answer received: !ybtrue
2024-06-18 13:51:45,360 Command to send: c
o357
add
ro355
e

2024-06-18 13:51:45,361 Answer received: !ybtrue
2024-06-18 13:51:45,361 Command to send: c
o357
add
ro356
e

2024-06-18 13:51:45,361 Answer received: !ybtrue
2024-06-18 13:51:45,361 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro357
e

2024-06-18 13:51:45,362 Answer received: !yro358
2024-06-18 13:51:45,362 Command to send: c
o316
select
ro358
e

2024-06-18 13:51:45,365 Answer received: !yro359
2024-06-18 13:51:45,365 Command to send: c
o359
schema
e

2024-06-18 13:51:45,366 Answer received: !yro360
2024-06-18 13:51:45,366 Command to send: c
o360
treeString
e

2024-06-18 13:51:45,366 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:51:45,366 Command to send: c
o359
count
e

2024-06-18 13:51:45,471 Answer received: !yL811
2024-06-18 13:51:45,471 Command to send: c
o359
schema
e

2024-06-18 13:51:45,472 Answer received: !yro361
2024-06-18 13:51:45,472 Command to send: c
o361
json
e

2024-06-18 13:51:45,473 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:51:45,473 Command to send: c
o359
na
e

2024-06-18 13:51:45,474 Answer received: !yro362
2024-06-18 13:51:45,474 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,475 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,475 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,475 Answer received: !ym
2024-06-18 13:51:45,475 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,476 Answer received: !ylo363
2024-06-18 13:51:45,476 Command to send: c
o363
add
sNom_Districte
e

2024-06-18 13:51:45,476 Answer received: !ybtrue
2024-06-18 13:51:45,476 Command to send: c
o363
add
sCodi_Districte
e

2024-06-18 13:51:45,477 Answer received: !ybtrue
2024-06-18 13:51:45,477 Command to send: c
o363
add
sndex RFD Barcelona = 100
e

2024-06-18 13:51:45,477 Answer received: !ybtrue
2024-06-18 13:51:45,477 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro363
e

2024-06-18 13:51:45,477 Answer received: !yro364
2024-06-18 13:51:45,478 Command to send: c
o362
drop
i3
ro364
e

2024-06-18 13:51:45,480 Answer received: !yro365
2024-06-18 13:51:45,480 Command to send: c
o365
count
e

2024-06-18 13:51:45,576 Answer received: !yL811
2024-06-18 13:51:45,576 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,578 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,578 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,579 Answer received: !ym
2024-06-18 13:51:45,579 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:45,580 Answer received: !yro366
2024-06-18 13:51:45,580 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,581 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,581 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,582 Answer received: !ym
2024-06-18 13:51:45,582 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:51:45,582 Answer received: !yro367
2024-06-18 13:51:45,582 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,583 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,583 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,584 Answer received: !ym
2024-06-18 13:51:45,584 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:51:45,584 Answer received: !yro368
2024-06-18 13:51:45,584 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,586 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,586 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,586 Answer received: !ym
2024-06-18 13:51:45,586 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:51:45,587 Answer received: !yro369
2024-06-18 13:51:45,587 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,588 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,588 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,588 Answer received: !ym
2024-06-18 13:51:45,589 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:51:45,589 Answer received: !yro370
2024-06-18 13:51:45,589 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,590 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,590 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,591 Answer received: !ym
2024-06-18 13:51:45,591 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:51:45,591 Answer received: !yro371
2024-06-18 13:51:45,591 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,592 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,592 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,592 Answer received: !ym
2024-06-18 13:51:45,592 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,593 Answer received: !ylo372
2024-06-18 13:51:45,593 Command to send: c
o372
add
ro366
e

2024-06-18 13:51:45,593 Answer received: !ybtrue
2024-06-18 13:51:45,593 Command to send: c
o372
add
ro367
e

2024-06-18 13:51:45,594 Answer received: !ybtrue
2024-06-18 13:51:45,594 Command to send: c
o372
add
ro368
e

2024-06-18 13:51:45,594 Answer received: !ybtrue
2024-06-18 13:51:45,595 Command to send: c
o372
add
ro369
e

2024-06-18 13:51:45,595 Answer received: !ybtrue
2024-06-18 13:51:45,595 Command to send: c
o372
add
ro370
e

2024-06-18 13:51:45,595 Answer received: !ybtrue
2024-06-18 13:51:45,596 Command to send: c
o372
add
ro371
e

2024-06-18 13:51:45,596 Answer received: !ybtrue
2024-06-18 13:51:45,596 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro372
e

2024-06-18 13:51:45,596 Answer received: !yro373
2024-06-18 13:51:45,596 Command to send: c
o331
select
ro373
e

2024-06-18 13:51:45,600 Answer received: !yro374
2024-06-18 13:51:45,600 Command to send: c
o374
schema
e

2024-06-18 13:51:45,601 Answer received: !yro375
2024-06-18 13:51:45,601 Command to send: c
o375
treeString
e

2024-06-18 13:51:45,601 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:51:45,601 Command to send: c
o374
count
e

2024-06-18 13:51:45,670 Command to send: m
d
o357
e

2024-06-18 13:51:45,670 Answer received: !yv
2024-06-18 13:51:45,670 Command to send: m
d
o363
e

2024-06-18 13:51:45,670 Answer received: !yv
2024-06-18 13:51:45,670 Command to send: m
d
o372
e

2024-06-18 13:51:45,671 Answer received: !yv
2024-06-18 13:51:45,716 Answer received: !yL15545
2024-06-18 13:51:45,716 Command to send: c
o374
schema
e

2024-06-18 13:51:45,716 Answer received: !yro376
2024-06-18 13:51:45,716 Command to send: c
o376
json
e

2024-06-18 13:51:45,717 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:51:45,717 Command to send: c
o374
na
e

2024-06-18 13:51:45,719 Answer received: !yro377
2024-06-18 13:51:45,719 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,720 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,720 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,720 Answer received: !ym
2024-06-18 13:51:45,720 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,721 Answer received: !ylo378
2024-06-18 13:51:45,721 Command to send: c
o378
add
sdistrict
e

2024-06-18 13:51:45,721 Answer received: !ybtrue
2024-06-18 13:51:45,721 Command to send: c
o378
add
snumPhotos
e

2024-06-18 13:51:45,722 Answer received: !ybtrue
2024-06-18 13:51:45,722 Command to send: c
o378
add
sprice
e

2024-06-18 13:51:45,722 Answer received: !ybtrue
2024-06-18 13:51:45,723 Command to send: c
o378
add
sfloor
e

2024-06-18 13:51:45,723 Answer received: !ybtrue
2024-06-18 13:51:45,723 Command to send: c
o378
add
ssize
e

2024-06-18 13:51:45,724 Answer received: !ybtrue
2024-06-18 13:51:45,724 Command to send: c
o378
add
sbathrooms
e

2024-06-18 13:51:45,724 Answer received: !ybtrue
2024-06-18 13:51:45,725 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro378
e

2024-06-18 13:51:45,725 Answer received: !yro379
2024-06-18 13:51:45,725 Command to send: c
o377
drop
i6
ro379
e

2024-06-18 13:51:45,730 Answer received: !yro380
2024-06-18 13:51:45,730 Command to send: c
o380
count
e

2024-06-18 13:51:45,853 Answer received: !yL15545
2024-06-18 13:51:45,854 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,855 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,855 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,856 Answer received: !ym
2024-06-18 13:51:45,856 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:45,856 Answer received: !yro381
2024-06-18 13:51:45,856 Command to send: r
u
functions
rj
e

2024-06-18 13:51:45,858 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:45,858 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:45,858 Answer received: !ym
2024-06-18 13:51:45,858 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:51:45,859 Answer received: !yro382
2024-06-18 13:51:45,859 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,859 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,859 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,860 Answer received: !ym
2024-06-18 13:51:45,860 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,860 Answer received: !ylo383
2024-06-18 13:51:45,860 Command to send: c
o383
add
ro381
e

2024-06-18 13:51:45,860 Answer received: !ybtrue
2024-06-18 13:51:45,861 Command to send: c
o383
add
ro382
e

2024-06-18 13:51:45,861 Answer received: !ybtrue
2024-06-18 13:51:45,861 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro383
e

2024-06-18 13:51:45,862 Answer received: !yro384
2024-06-18 13:51:45,862 Command to send: c
o342
select
ro384
e

2024-06-18 13:51:45,865 Answer received: !yro385
2024-06-18 13:51:45,865 Command to send: c
o385
schema
e

2024-06-18 13:51:45,866 Answer received: !yro386
2024-06-18 13:51:45,866 Command to send: c
o386
treeString
e

2024-06-18 13:51:45,866 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:51:45,866 Command to send: c
o385
count
e

2024-06-18 13:51:45,956 Answer received: !yL113
2024-06-18 13:51:45,956 Command to send: c
o385
schema
e

2024-06-18 13:51:45,957 Answer received: !yro387
2024-06-18 13:51:45,957 Command to send: c
o387
json
e

2024-06-18 13:51:45,957 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:51:45,957 Command to send: c
o385
na
e

2024-06-18 13:51:45,958 Answer received: !yro388
2024-06-18 13:51:45,958 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:45,959 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:45,959 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:45,960 Answer received: !ym
2024-06-18 13:51:45,960 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:45,960 Answer received: !ylo389
2024-06-18 13:51:45,961 Command to send: c
o389
add
sdistrict
e

2024-06-18 13:51:45,961 Answer received: !ybtrue
2024-06-18 13:51:45,961 Command to send: c
o389
add
sdistrict_id
e

2024-06-18 13:51:45,961 Answer received: !ybtrue
2024-06-18 13:51:45,962 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro389
e

2024-06-18 13:51:45,962 Answer received: !yro390
2024-06-18 13:51:45,962 Command to send: c
o388
drop
i2
ro390
e

2024-06-18 13:51:45,964 Answer received: !yro391
2024-06-18 13:51:45,964 Command to send: c
o391
count
e

2024-06-18 13:51:46,052 Answer received: !yL113
2024-06-18 13:51:46,052 Command to send: r
u
functions
rj
e

2024-06-18 13:51:46,054 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:46,054 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:46,054 Answer received: !ym
2024-06-18 13:51:46,054 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:51:46,055 Answer received: !yro392
2024-06-18 13:51:46,055 Command to send: r
u
functions
rj
e

2024-06-18 13:51:46,056 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:46,056 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:46,056 Answer received: !ym
2024-06-18 13:51:46,056 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:51:46,057 Answer received: !yro393
2024-06-18 13:51:46,057 Command to send: r
u
functions
rj
e

2024-06-18 13:51:46,058 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:46,058 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:46,058 Answer received: !ym
2024-06-18 13:51:46,058 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:51:46,059 Answer received: !yro394
2024-06-18 13:51:46,059 Command to send: r
u
functions
rj
e

2024-06-18 13:51:46,060 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:46,060 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:46,060 Answer received: !ym
2024-06-18 13:51:46,061 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:51:46,061 Answer received: !yro395
2024-06-18 13:51:46,061 Command to send: r
u
functions
rj
e

2024-06-18 13:51:46,062 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:46,063 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:46,063 Answer received: !ym
2024-06-18 13:51:46,063 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:51:46,063 Answer received: !yro396
2024-06-18 13:51:46,063 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:46,064 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:46,064 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:46,064 Answer received: !ym
2024-06-18 13:51:46,064 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:46,064 Answer received: !ylo397
2024-06-18 13:51:46,065 Command to send: c
o397
add
ro392
e

2024-06-18 13:51:46,065 Answer received: !ybtrue
2024-06-18 13:51:46,065 Command to send: c
o397
add
ro393
e

2024-06-18 13:51:46,065 Answer received: !ybtrue
2024-06-18 13:51:46,065 Command to send: c
o397
add
ro394
e

2024-06-18 13:51:46,065 Answer received: !ybtrue
2024-06-18 13:51:46,066 Command to send: c
o397
add
ro395
e

2024-06-18 13:51:46,066 Answer received: !ybtrue
2024-06-18 13:51:46,066 Command to send: c
o397
add
ro396
e

2024-06-18 13:51:46,066 Answer received: !ybtrue
2024-06-18 13:51:46,066 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro397
e

2024-06-18 13:51:46,066 Answer received: !yro398
2024-06-18 13:51:46,067 Command to send: c
o304
select
ro398
e

2024-06-18 13:51:46,071 Answer received: !xro399
2024-06-18 13:51:46,071 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:46,072 Answer received: !yp
2024-06-18 13:51:46,073 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:46,073 Answer received: !yp
2024-06-18 13:51:46,073 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:46,074 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:46,074 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:46,074 Answer received: !ym
2024-06-18 13:51:46,074 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro399
e

2024-06-18 13:51:46,075 Answer received: !ybfalse
2024-06-18 13:51:46,075 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:46,077 Answer received: !yp
2024-06-18 13:51:46,077 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:46,078 Answer received: !yp
2024-06-18 13:51:46,079 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:46,079 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:46,079 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:46,079 Answer received: !ym
2024-06-18 13:51:46,080 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro399
e

2024-06-18 13:51:46,080 Answer received: !ybtrue
2024-06-18 13:51:46,080 Command to send: c
o399
getMessage
e

2024-06-18 13:51:46,080 Answer received: !ys[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n
2024-06-18 13:51:46,081 Command to send: r
u
org
rj
e

2024-06-18 13:51:46,082 Answer received: !yp
2024-06-18 13:51:46,082 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:46,083 Answer received: !yp
2024-06-18 13:51:46,083 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:46,083 Answer received: !yp
2024-06-18 13:51:46,083 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:51:46,084 Answer received: !yp
2024-06-18 13:51:46,084 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:51:46,084 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:51:46,084 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:51:46,085 Answer received: !ym
2024-06-18 13:51:46,085 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro399
e

2024-06-18 13:51:46,085 Answer received: !ysorg.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)\r\n	at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4351)\r\n	at org.apache.spark.sql.Dataset.select(Dataset.scala:1540)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:46,086 Command to send: c
o399
getCause
e

2024-06-18 13:51:46,086 Answer received: !yn
2024-06-18 13:51:46,148 Command to send: r
u
org
rj
e

2024-06-18 13:51:46,150 Answer received: !yp
2024-06-18 13:51:46,150 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:46,151 Answer received: !yp
2024-06-18 13:51:46,151 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:46,151 Answer received: !yp
2024-06-18 13:51:46,151 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:46,152 Answer received: !yp
2024-06-18 13:51:46,152 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:46,153 Answer received: !yp
2024-06-18 13:51:46,153 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:46,153 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:46,153 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,154 Answer received: !ym
2024-06-18 13:51:46,154 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,154 Answer received: !yro400
2024-06-18 13:51:46,154 Command to send: c
o400
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:46,154 Answer received: !ybfalse
2024-06-18 13:51:46,155 Command to send: r
u
org
rj
e

2024-06-18 13:51:46,157 Answer received: !yp
2024-06-18 13:51:46,157 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:46,158 Answer received: !yp
2024-06-18 13:51:46,158 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:46,158 Answer received: !yp
2024-06-18 13:51:46,158 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:46,159 Answer received: !yp
2024-06-18 13:51:46,159 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:46,159 Answer received: !yp
2024-06-18 13:51:46,160 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:46,160 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:46,160 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,160 Answer received: !ym
2024-06-18 13:51:46,160 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,161 Answer received: !yro401
2024-06-18 13:51:46,161 Command to send: c
o401
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:46,161 Answer received: !ybfalse
2024-06-18 13:51:46,162 Command to send: r
u
org
rj
e

2024-06-18 13:51:46,164 Answer received: !yp
2024-06-18 13:51:46,164 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:46,164 Answer received: !yp
2024-06-18 13:51:46,165 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:46,165 Answer received: !yp
2024-06-18 13:51:46,165 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:46,166 Answer received: !yp
2024-06-18 13:51:46,166 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:46,166 Answer received: !yp
2024-06-18 13:51:46,166 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:46,167 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:46,167 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,167 Answer received: !ym
2024-06-18 13:51:46,167 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:46,167 Answer received: !yro402
2024-06-18 13:51:46,168 Command to send: c
o402
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:46,168 Answer received: !ybfalse
2024-06-18 13:51:46,672 Command to send: m
d
o378
e

2024-06-18 13:51:46,672 Answer received: !yv
2024-06-18 13:51:46,672 Command to send: m
d
o383
e

2024-06-18 13:51:46,673 Answer received: !yv
2024-06-18 13:51:46,673 Command to send: m
d
o389
e

2024-06-18 13:51:46,673 Answer received: !yv
2024-06-18 13:51:46,673 Command to send: m
d
o397
e

2024-06-18 13:51:46,673 Answer received: !yv
2024-06-18 13:51:46,674 Command to send: m
d
o354
e

2024-06-18 13:51:46,674 Answer received: !yv
2024-06-18 13:51:46,674 Command to send: m
d
o355
e

2024-06-18 13:51:46,674 Answer received: !yv
2024-06-18 13:51:46,674 Command to send: m
d
o356
e

2024-06-18 13:51:46,675 Answer received: !yv
2024-06-18 13:51:46,675 Command to send: m
d
o358
e

2024-06-18 13:51:46,675 Answer received: !yv
2024-06-18 13:51:46,675 Command to send: m
d
o359
e

2024-06-18 13:51:46,675 Answer received: !yv
2024-06-18 13:51:46,675 Command to send: m
d
o360
e

2024-06-18 13:51:46,676 Answer received: !yv
2024-06-18 13:51:46,676 Command to send: m
d
o361
e

2024-06-18 13:51:46,676 Answer received: !yv
2024-06-18 13:51:46,676 Command to send: m
d
o362
e

2024-06-18 13:51:46,676 Answer received: !yv
2024-06-18 13:51:46,677 Command to send: m
d
o364
e

2024-06-18 13:51:46,677 Answer received: !yv
2024-06-18 13:51:46,677 Command to send: m
d
o366
e

2024-06-18 13:51:46,677 Answer received: !yv
2024-06-18 13:51:46,677 Command to send: m
d
o367
e

2024-06-18 13:51:46,677 Answer received: !yv
2024-06-18 13:51:46,677 Command to send: m
d
o368
e

2024-06-18 13:51:46,678 Answer received: !yv
2024-06-18 13:51:46,678 Command to send: m
d
o369
e

2024-06-18 13:51:46,678 Answer received: !yv
2024-06-18 13:51:46,678 Command to send: m
d
o370
e

2024-06-18 13:51:46,678 Answer received: !yv
2024-06-18 13:51:46,679 Command to send: m
d
o371
e

2024-06-18 13:51:46,679 Answer received: !yv
2024-06-18 13:51:46,679 Command to send: m
d
o373
e

2024-06-18 13:51:46,679 Answer received: !yv
2024-06-18 13:51:46,679 Command to send: m
d
o374
e

2024-06-18 13:51:46,680 Answer received: !yv
2024-06-18 13:51:46,680 Command to send: m
d
o375
e

2024-06-18 13:51:46,680 Answer received: !yv
2024-06-18 13:51:46,680 Command to send: m
d
o376
e

2024-06-18 13:51:46,680 Answer received: !yv
2024-06-18 13:51:46,681 Command to send: m
d
o377
e

2024-06-18 13:51:46,681 Answer received: !yv
2024-06-18 13:51:46,681 Command to send: m
d
o379
e

2024-06-18 13:51:46,681 Answer received: !yv
2024-06-18 13:51:46,681 Command to send: m
d
o381
e

2024-06-18 13:51:46,682 Answer received: !yv
2024-06-18 13:51:46,682 Command to send: m
d
o382
e

2024-06-18 13:51:46,682 Answer received: !yv
2024-06-18 13:51:46,682 Command to send: m
d
o384
e

2024-06-18 13:51:46,682 Answer received: !yv
2024-06-18 13:51:46,682 Command to send: m
d
o385
e

2024-06-18 13:51:46,683 Answer received: !yv
2024-06-18 13:51:46,683 Command to send: m
d
o386
e

2024-06-18 13:51:46,683 Answer received: !yv
2024-06-18 13:51:46,683 Command to send: m
d
o387
e

2024-06-18 13:51:46,683 Answer received: !yv
2024-06-18 13:51:46,684 Command to send: m
d
o388
e

2024-06-18 13:51:46,684 Answer received: !yv
2024-06-18 13:51:46,684 Command to send: m
d
o390
e

2024-06-18 13:51:46,684 Answer received: !yv
2024-06-18 13:51:46,684 Command to send: m
d
o392
e

2024-06-18 13:51:46,684 Answer received: !yv
2024-06-18 13:51:46,685 Command to send: m
d
o393
e

2024-06-18 13:51:46,685 Answer received: !yv
2024-06-18 13:51:46,685 Command to send: m
d
o394
e

2024-06-18 13:51:46,685 Answer received: !yv
2024-06-18 13:51:46,685 Command to send: m
d
o395
e

2024-06-18 13:51:46,686 Answer received: !yv
2024-06-18 13:51:46,686 Command to send: m
d
o396
e

2024-06-18 13:51:46,686 Answer received: !yv
2024-06-18 13:51:49,688 Command to send: m
d
o400
e

2024-06-18 13:51:49,688 Answer received: !yv
2024-06-18 13:51:49,688 Command to send: m
d
o401
e

2024-06-18 13:51:49,689 Answer received: !yv
2024-06-18 13:51:49,689 Command to send: m
d
o402
e

2024-06-18 13:51:49,689 Answer received: !yv
2024-06-18 13:51:56,689 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,690 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,690 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,691 Answer received: !ym
2024-06-18 13:51:56,691 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:51:56,691 Answer received: !yro403
2024-06-18 13:51:56,691 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,693 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,693 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,694 Answer received: !ym
2024-06-18 13:51:56,694 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:51:56,695 Answer received: !yro404
2024-06-18 13:51:56,695 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,697 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,697 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,697 Answer received: !ym
2024-06-18 13:51:56,698 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:51:56,698 Answer received: !yro405
2024-06-18 13:51:56,698 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:56,699 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:56,699 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:56,700 Answer received: !ym
2024-06-18 13:51:56,700 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:56,700 Answer received: !ylo406
2024-06-18 13:51:56,700 Command to send: c
o406
add
ro403
e

2024-06-18 13:51:56,701 Answer received: !ybtrue
2024-06-18 13:51:56,701 Command to send: c
o406
add
ro404
e

2024-06-18 13:51:56,701 Answer received: !ybtrue
2024-06-18 13:51:56,701 Command to send: c
o406
add
ro405
e

2024-06-18 13:51:56,702 Answer received: !ybtrue
2024-06-18 13:51:56,702 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro406
e

2024-06-18 13:51:56,702 Answer received: !yro407
2024-06-18 13:51:56,702 Command to send: c
o365
select
ro407
e

2024-06-18 13:51:56,705 Answer received: !yro408
2024-06-18 13:51:56,706 Command to send: c
o408
schema
e

2024-06-18 13:51:56,706 Answer received: !yro409
2024-06-18 13:51:56,706 Command to send: c
o409
treeString
e

2024-06-18 13:51:56,707 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:51:56,707 Command to send: c
o408
count
e

2024-06-18 13:51:56,794 Answer received: !yL811
2024-06-18 13:51:56,794 Command to send: c
o408
schema
e

2024-06-18 13:51:56,794 Answer received: !yro410
2024-06-18 13:51:56,794 Command to send: c
o410
json
e

2024-06-18 13:51:56,795 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:51:56,795 Command to send: c
o408
na
e

2024-06-18 13:51:56,796 Answer received: !yro411
2024-06-18 13:51:56,796 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:56,797 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:56,797 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:56,797 Answer received: !ym
2024-06-18 13:51:56,798 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:56,798 Answer received: !ylo412
2024-06-18 13:51:56,798 Command to send: c
o412
add
sNom_Districte
e

2024-06-18 13:51:56,798 Answer received: !ybtrue
2024-06-18 13:51:56,799 Command to send: c
o412
add
sCodi_Districte
e

2024-06-18 13:51:56,799 Answer received: !ybtrue
2024-06-18 13:51:56,799 Command to send: c
o412
add
sndex RFD Barcelona = 100
e

2024-06-18 13:51:56,799 Answer received: !ybtrue
2024-06-18 13:51:56,800 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro412
e

2024-06-18 13:51:56,800 Answer received: !yro413
2024-06-18 13:51:56,800 Command to send: c
o411
drop
i3
ro413
e

2024-06-18 13:51:56,803 Answer received: !yro414
2024-06-18 13:51:56,803 Command to send: c
o414
count
e

2024-06-18 13:51:56,897 Answer received: !yL811
2024-06-18 13:51:56,897 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,899 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,899 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,899 Answer received: !ym
2024-06-18 13:51:56,899 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:56,900 Answer received: !yro415
2024-06-18 13:51:56,900 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,901 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,901 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,902 Answer received: !ym
2024-06-18 13:51:56,902 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:51:56,902 Answer received: !yro416
2024-06-18 13:51:56,902 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,904 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,904 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,904 Answer received: !ym
2024-06-18 13:51:56,904 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:51:56,905 Answer received: !yro417
2024-06-18 13:51:56,905 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,907 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,907 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,907 Answer received: !ym
2024-06-18 13:51:56,907 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:51:56,908 Answer received: !yro418
2024-06-18 13:51:56,908 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,909 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,909 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,910 Answer received: !ym
2024-06-18 13:51:56,910 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:51:56,910 Answer received: !yro419
2024-06-18 13:51:56,910 Command to send: r
u
functions
rj
e

2024-06-18 13:51:56,911 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:56,911 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:56,912 Answer received: !ym
2024-06-18 13:51:56,912 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:51:56,912 Answer received: !yro420
2024-06-18 13:51:56,912 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:56,913 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:56,913 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:56,913 Answer received: !ym
2024-06-18 13:51:56,914 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:56,914 Answer received: !ylo421
2024-06-18 13:51:56,915 Command to send: c
o421
add
ro415
e

2024-06-18 13:51:56,915 Answer received: !ybtrue
2024-06-18 13:51:56,915 Command to send: c
o421
add
ro416
e

2024-06-18 13:51:56,915 Answer received: !ybtrue
2024-06-18 13:51:56,916 Command to send: c
o421
add
ro417
e

2024-06-18 13:51:56,916 Answer received: !ybtrue
2024-06-18 13:51:56,916 Command to send: c
o421
add
ro418
e

2024-06-18 13:51:56,916 Answer received: !ybtrue
2024-06-18 13:51:56,916 Command to send: c
o421
add
ro419
e

2024-06-18 13:51:56,917 Answer received: !ybtrue
2024-06-18 13:51:56,917 Command to send: c
o421
add
ro420
e

2024-06-18 13:51:56,917 Answer received: !ybtrue
2024-06-18 13:51:56,917 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro421
e

2024-06-18 13:51:56,917 Answer received: !yro422
2024-06-18 13:51:56,918 Command to send: c
o380
select
ro422
e

2024-06-18 13:51:56,922 Answer received: !yro423
2024-06-18 13:51:56,922 Command to send: c
o423
schema
e

2024-06-18 13:51:56,923 Answer received: !yro424
2024-06-18 13:51:56,923 Command to send: c
o424
treeString
e

2024-06-18 13:51:56,923 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:51:56,923 Command to send: c
o423
count
e

2024-06-18 13:51:57,024 Answer received: !yL15545
2024-06-18 13:51:57,024 Command to send: c
o423
schema
e

2024-06-18 13:51:57,024 Answer received: !yro425
2024-06-18 13:51:57,024 Command to send: c
o425
json
e

2024-06-18 13:51:57,025 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:51:57,025 Command to send: c
o423
na
e

2024-06-18 13:51:57,027 Answer received: !yro426
2024-06-18 13:51:57,027 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:57,028 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:57,028 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:57,028 Answer received: !ym
2024-06-18 13:51:57,029 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:57,029 Answer received: !ylo427
2024-06-18 13:51:57,029 Command to send: c
o427
add
sdistrict
e

2024-06-18 13:51:57,029 Answer received: !ybtrue
2024-06-18 13:51:57,030 Command to send: c
o427
add
snumPhotos
e

2024-06-18 13:51:57,030 Answer received: !ybtrue
2024-06-18 13:51:57,030 Command to send: c
o427
add
sprice
e

2024-06-18 13:51:57,030 Answer received: !ybtrue
2024-06-18 13:51:57,030 Command to send: c
o427
add
sfloor
e

2024-06-18 13:51:57,031 Answer received: !ybtrue
2024-06-18 13:51:57,031 Command to send: c
o427
add
ssize
e

2024-06-18 13:51:57,031 Answer received: !ybtrue
2024-06-18 13:51:57,031 Command to send: c
o427
add
sbathrooms
e

2024-06-18 13:51:57,031 Answer received: !ybtrue
2024-06-18 13:51:57,032 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro427
e

2024-06-18 13:51:57,032 Answer received: !yro428
2024-06-18 13:51:57,032 Command to send: c
o426
drop
i6
ro428
e

2024-06-18 13:51:57,035 Answer received: !yro429
2024-06-18 13:51:57,035 Command to send: c
o429
count
e

2024-06-18 13:51:57,167 Answer received: !yL15545
2024-06-18 13:51:57,167 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,170 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,170 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,170 Answer received: !ym
2024-06-18 13:51:57,171 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:51:57,171 Answer received: !yro430
2024-06-18 13:51:57,171 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,172 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,173 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,173 Answer received: !ym
2024-06-18 13:51:57,173 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:51:57,173 Answer received: !yro431
2024-06-18 13:51:57,174 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:57,174 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:57,174 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:57,175 Answer received: !ym
2024-06-18 13:51:57,175 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:57,175 Answer received: !ylo432
2024-06-18 13:51:57,175 Command to send: c
o432
add
ro430
e

2024-06-18 13:51:57,176 Answer received: !ybtrue
2024-06-18 13:51:57,176 Command to send: c
o432
add
ro431
e

2024-06-18 13:51:57,176 Answer received: !ybtrue
2024-06-18 13:51:57,176 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro432
e

2024-06-18 13:51:57,176 Answer received: !yro433
2024-06-18 13:51:57,177 Command to send: c
o391
select
ro433
e

2024-06-18 13:51:57,194 Answer received: !yro434
2024-06-18 13:51:57,194 Command to send: c
o434
schema
e

2024-06-18 13:51:57,195 Answer received: !yro435
2024-06-18 13:51:57,195 Command to send: c
o435
treeString
e

2024-06-18 13:51:57,195 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:51:57,195 Command to send: c
o434
count
e

2024-06-18 13:51:57,293 Answer received: !yL113
2024-06-18 13:51:57,293 Command to send: c
o434
schema
e

2024-06-18 13:51:57,293 Answer received: !yro436
2024-06-18 13:51:57,294 Command to send: c
o436
json
e

2024-06-18 13:51:57,294 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:51:57,294 Command to send: c
o434
na
e

2024-06-18 13:51:57,295 Answer received: !yro437
2024-06-18 13:51:57,296 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:57,296 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:57,296 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:57,297 Answer received: !ym
2024-06-18 13:51:57,297 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:57,298 Answer received: !ylo438
2024-06-18 13:51:57,298 Command to send: c
o438
add
sdistrict
e

2024-06-18 13:51:57,298 Answer received: !ybtrue
2024-06-18 13:51:57,298 Command to send: c
o438
add
sdistrict_id
e

2024-06-18 13:51:57,299 Answer received: !ybtrue
2024-06-18 13:51:57,299 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro438
e

2024-06-18 13:51:57,299 Answer received: !yro439
2024-06-18 13:51:57,299 Command to send: c
o437
drop
i2
ro439
e

2024-06-18 13:51:57,302 Answer received: !yro440
2024-06-18 13:51:57,302 Command to send: c
o440
count
e

2024-06-18 13:51:57,387 Answer received: !yL113
2024-06-18 13:51:57,388 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,389 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,390 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,390 Answer received: !ym
2024-06-18 13:51:57,390 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:51:57,391 Answer received: !yro441
2024-06-18 13:51:57,391 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,392 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,392 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,392 Answer received: !ym
2024-06-18 13:51:57,392 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:51:57,393 Answer received: !yro442
2024-06-18 13:51:57,393 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,395 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,395 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,396 Answer received: !ym
2024-06-18 13:51:57,396 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:51:57,396 Answer received: !yro443
2024-06-18 13:51:57,397 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,398 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,398 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,398 Answer received: !ym
2024-06-18 13:51:57,399 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:51:57,399 Answer received: !yro444
2024-06-18 13:51:57,399 Command to send: r
u
functions
rj
e

2024-06-18 13:51:57,400 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:51:57,400 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:51:57,401 Answer received: !ym
2024-06-18 13:51:57,401 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:51:57,401 Answer received: !yro445
2024-06-18 13:51:57,401 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:51:57,402 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:51:57,402 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:51:57,403 Answer received: !ym
2024-06-18 13:51:57,403 Command to send: i
java.util.ArrayList
e

2024-06-18 13:51:57,403 Answer received: !ylo446
2024-06-18 13:51:57,403 Command to send: c
o446
add
ro441
e

2024-06-18 13:51:57,404 Answer received: !ybtrue
2024-06-18 13:51:57,404 Command to send: c
o446
add
ro442
e

2024-06-18 13:51:57,404 Answer received: !ybtrue
2024-06-18 13:51:57,404 Command to send: c
o446
add
ro443
e

2024-06-18 13:51:57,405 Answer received: !ybtrue
2024-06-18 13:51:57,405 Command to send: c
o446
add
ro444
e

2024-06-18 13:51:57,405 Answer received: !ybtrue
2024-06-18 13:51:57,405 Command to send: c
o446
add
ro445
e

2024-06-18 13:51:57,406 Answer received: !ybtrue
2024-06-18 13:51:57,406 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro446
e

2024-06-18 13:51:57,406 Answer received: !yro447
2024-06-18 13:51:57,406 Command to send: c
o304
select
ro447
e

2024-06-18 13:51:57,411 Answer received: !xro448
2024-06-18 13:51:57,411 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:57,413 Answer received: !yp
2024-06-18 13:51:57,413 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:57,414 Answer received: !yp
2024-06-18 13:51:57,414 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:57,414 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:57,414 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:57,415 Answer received: !ym
2024-06-18 13:51:57,415 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro448
e

2024-06-18 13:51:57,415 Answer received: !ybfalse
2024-06-18 13:51:57,415 Command to send: r
u
py4j
rj
e

2024-06-18 13:51:57,417 Answer received: !yp
2024-06-18 13:51:57,417 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:51:57,418 Answer received: !yp
2024-06-18 13:51:57,418 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:51:57,418 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:51:57,418 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:51:57,418 Answer received: !ym
2024-06-18 13:51:57,418 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro448
e

2024-06-18 13:51:57,419 Answer received: !ybtrue
2024-06-18 13:51:57,419 Command to send: c
o448
getMessage
e

2024-06-18 13:51:57,420 Answer received: !ys[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n
2024-06-18 13:51:57,420 Command to send: r
u
org
rj
e

2024-06-18 13:51:57,422 Answer received: !yp
2024-06-18 13:51:57,422 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:57,422 Answer received: !yp
2024-06-18 13:51:57,422 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:57,423 Answer received: !yp
2024-06-18 13:51:57,423 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:51:57,423 Answer received: !yp
2024-06-18 13:51:57,423 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:51:57,424 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:51:57,424 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:51:57,424 Answer received: !ym
2024-06-18 13:51:57,424 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro448
e

2024-06-18 13:51:57,425 Answer received: !ysorg.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1024,N_hotels#1025L,Avg_lat#1026,Avg_long#1027,Avg_Index_RFD#1028,numPhotos#1029L,price#1030,floor#1031,size#1032,bathrooms#1033L] parquet\n\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)\r\n	at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4351)\r\n	at org.apache.spark.sql.Dataset.select(Dataset.scala:1540)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:51:57,425 Command to send: c
o448
getCause
e

2024-06-18 13:51:57,426 Answer received: !yn
2024-06-18 13:51:57,483 Command to send: r
u
org
rj
e

2024-06-18 13:51:57,485 Answer received: !yp
2024-06-18 13:51:57,486 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:57,486 Answer received: !yp
2024-06-18 13:51:57,486 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:57,487 Answer received: !yp
2024-06-18 13:51:57,487 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:57,487 Answer received: !yp
2024-06-18 13:51:57,487 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:57,489 Answer received: !yp
2024-06-18 13:51:57,489 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:57,489 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:57,489 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,489 Answer received: !ym
2024-06-18 13:51:57,489 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,489 Answer received: !yro449
2024-06-18 13:51:57,489 Command to send: c
o449
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:57,490 Answer received: !ybfalse
2024-06-18 13:51:57,491 Command to send: r
u
org
rj
e

2024-06-18 13:51:57,493 Answer received: !yp
2024-06-18 13:51:57,493 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:57,494 Answer received: !yp
2024-06-18 13:51:57,494 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:57,494 Answer received: !yp
2024-06-18 13:51:57,494 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:57,495 Answer received: !yp
2024-06-18 13:51:57,495 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:57,496 Answer received: !yp
2024-06-18 13:51:57,496 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:57,496 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:57,496 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,496 Answer received: !ym
2024-06-18 13:51:57,496 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,497 Answer received: !yro450
2024-06-18 13:51:57,497 Command to send: c
o450
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:57,497 Answer received: !ybfalse
2024-06-18 13:51:57,498 Command to send: r
u
org
rj
e

2024-06-18 13:51:57,500 Answer received: !yp
2024-06-18 13:51:57,500 Command to send: r
u
org.apache
rj
e

2024-06-18 13:51:57,501 Answer received: !yp
2024-06-18 13:51:57,501 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:51:57,501 Answer received: !yp
2024-06-18 13:51:57,502 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:51:57,502 Answer received: !yp
2024-06-18 13:51:57,502 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:51:57,503 Answer received: !yp
2024-06-18 13:51:57,503 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:51:57,503 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:51:57,503 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,504 Answer received: !ym
2024-06-18 13:51:57,504 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:51:57,504 Answer received: !yro451
2024-06-18 13:51:57,504 Command to send: c
o451
pysparkJVMStacktraceEnabled
e

2024-06-18 13:51:57,504 Answer received: !ybfalse
2024-06-18 13:51:57,693 Command to send: m
d
o406
e

2024-06-18 13:51:57,693 Answer received: !yv
2024-06-18 13:51:57,693 Command to send: m
d
o412
e

2024-06-18 13:51:57,693 Answer received: !yv
2024-06-18 13:51:57,694 Command to send: m
d
o421
e

2024-06-18 13:51:57,694 Answer received: !yv
2024-06-18 13:51:57,694 Command to send: m
d
o427
e

2024-06-18 13:51:57,694 Answer received: !yv
2024-06-18 13:51:57,694 Command to send: m
d
o432
e

2024-06-18 13:51:57,695 Answer received: !yv
2024-06-18 13:51:57,695 Command to send: m
d
o438
e

2024-06-18 13:51:57,695 Answer received: !yv
2024-06-18 13:51:57,695 Command to send: m
d
o446
e

2024-06-18 13:51:57,695 Answer received: !yv
2024-06-18 13:51:57,695 Command to send: m
d
o403
e

2024-06-18 13:51:57,696 Answer received: !yv
2024-06-18 13:51:57,696 Command to send: m
d
o404
e

2024-06-18 13:51:57,696 Answer received: !yv
2024-06-18 13:51:57,696 Command to send: m
d
o405
e

2024-06-18 13:51:57,696 Answer received: !yv
2024-06-18 13:51:57,697 Command to send: m
d
o407
e

2024-06-18 13:51:57,697 Answer received: !yv
2024-06-18 13:51:57,697 Command to send: m
d
o408
e

2024-06-18 13:51:57,697 Answer received: !yv
2024-06-18 13:51:57,697 Command to send: m
d
o409
e

2024-06-18 13:51:57,698 Answer received: !yv
2024-06-18 13:51:57,698 Command to send: m
d
o410
e

2024-06-18 13:51:57,698 Answer received: !yv
2024-06-18 13:51:57,698 Command to send: m
d
o411
e

2024-06-18 13:51:57,698 Answer received: !yv
2024-06-18 13:51:57,698 Command to send: m
d
o413
e

2024-06-18 13:51:57,699 Answer received: !yv
2024-06-18 13:51:57,699 Command to send: m
d
o415
e

2024-06-18 13:51:57,699 Answer received: !yv
2024-06-18 13:51:57,699 Command to send: m
d
o416
e

2024-06-18 13:51:57,699 Answer received: !yv
2024-06-18 13:51:57,700 Command to send: m
d
o417
e

2024-06-18 13:51:57,700 Answer received: !yv
2024-06-18 13:51:57,700 Command to send: m
d
o418
e

2024-06-18 13:51:57,701 Answer received: !yv
2024-06-18 13:51:57,701 Command to send: m
d
o419
e

2024-06-18 13:51:57,701 Answer received: !yv
2024-06-18 13:51:57,701 Command to send: m
d
o420
e

2024-06-18 13:51:57,701 Answer received: !yv
2024-06-18 13:51:57,702 Command to send: m
d
o422
e

2024-06-18 13:51:57,702 Answer received: !yv
2024-06-18 13:51:57,702 Command to send: m
d
o423
e

2024-06-18 13:51:57,702 Answer received: !yv
2024-06-18 13:51:57,702 Command to send: m
d
o424
e

2024-06-18 13:51:57,703 Answer received: !yv
2024-06-18 13:51:57,703 Command to send: m
d
o425
e

2024-06-18 13:51:57,703 Answer received: !yv
2024-06-18 13:51:57,703 Command to send: m
d
o426
e

2024-06-18 13:51:57,703 Answer received: !yv
2024-06-18 13:51:57,703 Command to send: m
d
o428
e

2024-06-18 13:51:57,703 Answer received: !yv
2024-06-18 13:51:57,703 Command to send: m
d
o430
e

2024-06-18 13:51:57,704 Answer received: !yv
2024-06-18 13:51:57,704 Command to send: m
d
o431
e

2024-06-18 13:51:57,704 Answer received: !yv
2024-06-18 13:51:57,704 Command to send: m
d
o433
e

2024-06-18 13:51:57,704 Answer received: !yv
2024-06-18 13:51:57,705 Command to send: m
d
o434
e

2024-06-18 13:51:57,705 Answer received: !yv
2024-06-18 13:51:57,705 Command to send: m
d
o435
e

2024-06-18 13:51:57,705 Answer received: !yv
2024-06-18 13:51:57,705 Command to send: m
d
o436
e

2024-06-18 13:51:57,706 Answer received: !yv
2024-06-18 13:51:57,706 Command to send: m
d
o437
e

2024-06-18 13:51:57,706 Answer received: !yv
2024-06-18 13:51:57,706 Command to send: m
d
o439
e

2024-06-18 13:51:57,706 Answer received: !yv
2024-06-18 13:51:57,706 Command to send: m
d
o441
e

2024-06-18 13:51:57,707 Answer received: !yv
2024-06-18 13:51:57,707 Command to send: m
d
o442
e

2024-06-18 13:51:57,707 Answer received: !yv
2024-06-18 13:51:57,707 Command to send: m
d
o443
e

2024-06-18 13:51:57,707 Answer received: !yv
2024-06-18 13:51:57,708 Command to send: m
d
o444
e

2024-06-18 13:51:57,708 Answer received: !yv
2024-06-18 13:51:57,708 Command to send: m
d
o445
e

2024-06-18 13:51:57,709 Answer received: !yv
2024-06-18 13:51:57,709 Command to send: m
d
o365
e

2024-06-18 13:51:57,709 Answer received: !yv
2024-06-18 13:51:57,709 Command to send: m
d
o380
e

2024-06-18 13:51:57,709 Answer received: !yv
2024-06-18 13:51:57,709 Command to send: m
d
o391
e

2024-06-18 13:51:57,710 Answer received: !yv
2024-06-18 13:52:16,745 Command to send: r
u
SparkConf
rj
e

2024-06-18 13:52:16,746 Answer received: !ycorg.apache.spark.SparkConf
2024-06-18 13:52:16,746 Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-06-18 13:52:16,747 Answer received: !yro452
2024-06-18 13:52:16,747 Command to send: c
o452
set
sspark.master
slocal
e

2024-06-18 13:52:16,747 Answer received: !yro453
2024-06-18 13:52:16,748 Command to send: c
o452
set
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:52:16,748 Answer received: !yro454
2024-06-18 13:52:16,749 Command to send: c
o452
getAll
e

2024-06-18 13:52:16,749 Answer received: !yto455
2024-06-18 13:52:16,749 Command to send: a
e
o455
e

2024-06-18 13:52:16,750 Answer received: !yi6
2024-06-18 13:52:16,750 Command to send: a
g
o455
i0
e

2024-06-18 13:52:16,750 Answer received: !yro456
2024-06-18 13:52:16,750 Command to send: c
o456
_1
e

2024-06-18 13:52:16,751 Answer received: !ysspark.master
2024-06-18 13:52:16,751 Command to send: c
o456
_2
e

2024-06-18 13:52:16,751 Answer received: !yslocal
2024-06-18 13:52:16,751 Command to send: a
e
o455
e

2024-06-18 13:52:16,752 Answer received: !yi6
2024-06-18 13:52:16,752 Command to send: a
g
o455
i1
e

2024-06-18 13:52:16,752 Answer received: !yro457
2024-06-18 13:52:16,752 Command to send: c
o457
_1
e

2024-06-18 13:52:16,753 Answer received: !ysspark.submit.pyFiles
2024-06-18 13:52:16,753 Command to send: c
o457
_2
e

2024-06-18 13:52:16,753 Answer received: !ys
2024-06-18 13:52:16,753 Command to send: a
e
o455
e

2024-06-18 13:52:16,754 Answer received: !yi6
2024-06-18 13:52:16,754 Command to send: a
g
o455
i2
e

2024-06-18 13:52:16,754 Answer received: !yro458
2024-06-18 13:52:16,754 Command to send: c
o458
_1
e

2024-06-18 13:52:16,754 Answer received: !ysspark.app.submitTime
2024-06-18 13:52:16,755 Command to send: c
o458
_2
e

2024-06-18 13:52:16,755 Answer received: !ys1718709747485
2024-06-18 13:52:16,755 Command to send: a
e
o455
e

2024-06-18 13:52:16,755 Answer received: !yi6
2024-06-18 13:52:16,755 Command to send: a
g
o455
i3
e

2024-06-18 13:52:16,756 Answer received: !yro459
2024-06-18 13:52:16,756 Command to send: c
o459
_1
e

2024-06-18 13:52:16,756 Answer received: !ysspark.submit.deployMode
2024-06-18 13:52:16,756 Command to send: c
o459
_2
e

2024-06-18 13:52:16,757 Answer received: !ysclient
2024-06-18 13:52:16,757 Command to send: a
e
o455
e

2024-06-18 13:52:16,757 Answer received: !yi6
2024-06-18 13:52:16,757 Command to send: a
g
o455
i4
e

2024-06-18 13:52:16,758 Answer received: !yro460
2024-06-18 13:52:16,758 Command to send: c
o460
_1
e

2024-06-18 13:52:16,758 Answer received: !ysspark.app.name
2024-06-18 13:52:16,758 Command to send: c
o460
_2
e

2024-06-18 13:52:16,759 Answer received: !ysFormatted zone loader
2024-06-18 13:52:16,759 Command to send: a
e
o455
e

2024-06-18 13:52:16,759 Answer received: !yi6
2024-06-18 13:52:16,759 Command to send: a
g
o455
i5
e

2024-06-18 13:52:16,760 Answer received: !yro461
2024-06-18 13:52:16,760 Command to send: c
o461
_1
e

2024-06-18 13:52:16,760 Answer received: !ysspark.ui.showConsoleProgress
2024-06-18 13:52:16,760 Command to send: c
o461
_2
e

2024-06-18 13:52:16,761 Answer received: !ystrue
2024-06-18 13:52:16,761 Command to send: a
e
o455
e

2024-06-18 13:52:16,761 Answer received: !yi6
2024-06-18 13:52:16,762 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:52:16,764 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:52:16,764 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:52:16,765 Answer received: !yro462
2024-06-18 13:52:16,765 Command to send: i
java.util.HashMap
e

2024-06-18 13:52:16,765 Answer received: !yao463
2024-06-18 13:52:16,765 Command to send: c
o463
put
sspark.master
slocal
e

2024-06-18 13:52:16,766 Answer received: !yn
2024-06-18 13:52:16,766 Command to send: c
o463
put
sspark.submit.pyFiles
s
e

2024-06-18 13:52:16,766 Answer received: !yn
2024-06-18 13:52:16,767 Command to send: c
o463
put
sspark.app.submitTime
s1718709747485
e

2024-06-18 13:52:16,767 Answer received: !yn
2024-06-18 13:52:16,767 Command to send: c
o463
put
sspark.submit.deployMode
sclient
e

2024-06-18 13:52:16,767 Answer received: !yn
2024-06-18 13:52:16,768 Command to send: c
o463
put
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:52:16,768 Answer received: !yn
2024-06-18 13:52:16,768 Command to send: c
o463
put
sspark.ui.showConsoleProgress
strue
e

2024-06-18 13:52:16,768 Answer received: !yn
2024-06-18 13:52:16,768 Command to send: c
o462
applyModifiableSettings
ro25
ro463
e

2024-06-18 13:52:16,769 Answer received: !yv
2024-06-18 13:52:16,769 Spark sesion correctly initialized
2024-06-18 13:52:16,770 Command to send: c
o25
read
e

2024-06-18 13:52:16,770 Answer received: !yro464
2024-06-18 13:52:16,770 Command to send: c
o464
option
smultiline
strue
e

2024-06-18 13:52:16,771 Answer received: !yro465
2024-06-18 13:52:16,771 Command to send: c
o465
format
sparquet
e

2024-06-18 13:52:16,771 Answer received: !yro466
2024-06-18 13:52:16,771 Command to send: c
o466
load
smodel_temp/20240614-2304-renda_familiar
e

2024-06-18 13:52:16,892 Answer received: !yro467
2024-06-18 13:52:16,892 Command to send: c
o25
read
e

2024-06-18 13:52:16,893 Answer received: !yro468
2024-06-18 13:52:16,893 Command to send: c
o468
option
smultiline
strue
e

2024-06-18 13:52:16,893 Answer received: !yro469
2024-06-18 13:52:16,893 Command to send: c
o469
format
sparquet
e

2024-06-18 13:52:16,894 Answer received: !yro470
2024-06-18 13:52:16,894 Command to send: c
o470
load
smodel_temp/20240614-2306-idealista
e

2024-06-18 13:52:17,012 Answer received: !yro471
2024-06-18 13:52:17,013 Command to send: c
o25
read
e

2024-06-18 13:52:17,013 Answer received: !yro472
2024-06-18 13:52:17,013 Command to send: c
o472
option
smultiline
strue
e

2024-06-18 13:52:17,014 Answer received: !yro473
2024-06-18 13:52:17,014 Command to send: c
o473
format
sparquet
e

2024-06-18 13:52:17,014 Answer received: !yro474
2024-06-18 13:52:17,014 Command to send: c
o474
load
smodel_temp/20240614-2308-lookup_renta_idealista
e

2024-06-18 13:52:17,108 Answer received: !yro475
2024-06-18 13:52:17,108 Command to send: c
o25
read
e

2024-06-18 13:52:17,108 Answer received: !yro476
2024-06-18 13:52:17,109 Command to send: c
o476
option
smultiline
strue
e

2024-06-18 13:52:17,109 Answer received: !yro477
2024-06-18 13:52:17,109 Command to send: c
o477
format
sparquet
e

2024-06-18 13:52:17,109 Answer received: !yro478
2024-06-18 13:52:17,110 Command to send: c
o478
load
smodel_temp/20240615-1328-hotels
e

2024-06-18 13:52:17,203 Answer received: !yro479
2024-06-18 13:52:17,721 Command to send: m
d
o453
e

2024-06-18 13:52:17,721 Answer received: !yv
2024-06-18 13:52:17,721 Command to send: m
d
o454
e

2024-06-18 13:52:17,721 Answer received: !yv
2024-06-18 13:52:17,722 Command to send: m
d
o455
e

2024-06-18 13:52:17,722 Answer received: !yv
2024-06-18 13:52:17,722 Command to send: m
d
o463
e

2024-06-18 13:52:17,722 Answer received: !yv
2024-06-18 13:52:18,526 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,527 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,528 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,528 Answer received: !ym
2024-06-18 13:52:18,528 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:52:18,529 Answer received: !yro480
2024-06-18 13:52:18,529 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,530 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,531 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,531 Answer received: !ym
2024-06-18 13:52:18,531 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:52:18,532 Answer received: !yro481
2024-06-18 13:52:18,532 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,533 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,533 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,534 Answer received: !ym
2024-06-18 13:52:18,534 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:52:18,534 Answer received: !yro482
2024-06-18 13:52:18,534 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:18,535 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:18,535 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:18,535 Answer received: !ym
2024-06-18 13:52:18,535 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:18,535 Answer received: !ylo483
2024-06-18 13:52:18,536 Command to send: c
o483
add
ro480
e

2024-06-18 13:52:18,536 Answer received: !ybtrue
2024-06-18 13:52:18,536 Command to send: c
o483
add
ro481
e

2024-06-18 13:52:18,536 Answer received: !ybtrue
2024-06-18 13:52:18,536 Command to send: c
o483
add
ro482
e

2024-06-18 13:52:18,537 Answer received: !ybtrue
2024-06-18 13:52:18,537 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro483
e

2024-06-18 13:52:18,537 Answer received: !yro484
2024-06-18 13:52:18,537 Command to send: c
o467
select
ro484
e

2024-06-18 13:52:18,541 Answer received: !yro485
2024-06-18 13:52:18,542 Command to send: c
o485
schema
e

2024-06-18 13:52:18,542 Answer received: !yro486
2024-06-18 13:52:18,542 Command to send: c
o486
treeString
e

2024-06-18 13:52:18,543 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:52:18,543 Command to send: c
o485
count
e

2024-06-18 13:52:18,634 Answer received: !yL811
2024-06-18 13:52:18,634 Command to send: c
o485
schema
e

2024-06-18 13:52:18,635 Answer received: !yro487
2024-06-18 13:52:18,635 Command to send: c
o487
json
e

2024-06-18 13:52:18,635 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:52:18,636 Command to send: c
o485
na
e

2024-06-18 13:52:18,637 Answer received: !yro488
2024-06-18 13:52:18,637 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:18,638 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:18,638 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:18,638 Answer received: !ym
2024-06-18 13:52:18,638 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:18,638 Answer received: !ylo489
2024-06-18 13:52:18,639 Command to send: c
o489
add
sNom_Districte
e

2024-06-18 13:52:18,639 Answer received: !ybtrue
2024-06-18 13:52:18,639 Command to send: c
o489
add
sCodi_Districte
e

2024-06-18 13:52:18,639 Answer received: !ybtrue
2024-06-18 13:52:18,640 Command to send: c
o489
add
sndex RFD Barcelona = 100
e

2024-06-18 13:52:18,640 Answer received: !ybtrue
2024-06-18 13:52:18,640 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro489
e

2024-06-18 13:52:18,641 Answer received: !yro490
2024-06-18 13:52:18,641 Command to send: c
o488
drop
i3
ro490
e

2024-06-18 13:52:18,643 Answer received: !yro491
2024-06-18 13:52:18,643 Command to send: c
o491
count
e

2024-06-18 13:52:18,723 Command to send: m
d
o449
e

2024-06-18 13:52:18,723 Answer received: !yv
2024-06-18 13:52:18,723 Command to send: m
d
o450
e

2024-06-18 13:52:18,723 Answer received: !yv
2024-06-18 13:52:18,724 Command to send: m
d
o451
e

2024-06-18 13:52:18,724 Answer received: !yv
2024-06-18 13:52:18,724 Command to send: m
d
o456
e

2024-06-18 13:52:18,724 Answer received: !yv
2024-06-18 13:52:18,724 Command to send: m
d
o457
e

2024-06-18 13:52:18,725 Answer received: !yv
2024-06-18 13:52:18,725 Command to send: m
d
o458
e

2024-06-18 13:52:18,725 Answer received: !yv
2024-06-18 13:52:18,725 Command to send: m
d
o459
e

2024-06-18 13:52:18,725 Answer received: !yv
2024-06-18 13:52:18,725 Command to send: m
d
o460
e

2024-06-18 13:52:18,726 Answer received: !yv
2024-06-18 13:52:18,726 Command to send: m
d
o461
e

2024-06-18 13:52:18,726 Answer received: !yv
2024-06-18 13:52:18,726 Command to send: m
d
o462
e

2024-06-18 13:52:18,726 Answer received: !yv
2024-06-18 13:52:18,727 Command to send: m
d
o464
e

2024-06-18 13:52:18,727 Answer received: !yv
2024-06-18 13:52:18,727 Command to send: m
d
o465
e

2024-06-18 13:52:18,727 Answer received: !yv
2024-06-18 13:52:18,727 Command to send: m
d
o466
e

2024-06-18 13:52:18,728 Answer received: !yv
2024-06-18 13:52:18,728 Command to send: m
d
o468
e

2024-06-18 13:52:18,728 Answer received: !yv
2024-06-18 13:52:18,728 Command to send: m
d
o469
e

2024-06-18 13:52:18,728 Answer received: !yv
2024-06-18 13:52:18,729 Command to send: m
d
o470
e

2024-06-18 13:52:18,729 Answer received: !yv
2024-06-18 13:52:18,729 Command to send: m
d
o472
e

2024-06-18 13:52:18,729 Answer received: !yv
2024-06-18 13:52:18,729 Command to send: m
d
o473
e

2024-06-18 13:52:18,729 Answer received: !yv
2024-06-18 13:52:18,730 Command to send: m
d
o474
e

2024-06-18 13:52:18,730 Answer received: !yv
2024-06-18 13:52:18,730 Command to send: m
d
o476
e

2024-06-18 13:52:18,730 Answer received: !yv
2024-06-18 13:52:18,730 Command to send: m
d
o477
e

2024-06-18 13:52:18,731 Answer received: !yv
2024-06-18 13:52:18,731 Command to send: m
d
o478
e

2024-06-18 13:52:18,731 Answer received: !yv
2024-06-18 13:52:18,731 Command to send: m
d
o483
e

2024-06-18 13:52:18,732 Answer received: !yv
2024-06-18 13:52:18,732 Command to send: m
d
o489
e

2024-06-18 13:52:18,732 Answer received: !yv
2024-06-18 13:52:18,747 Answer received: !yL811
2024-06-18 13:52:18,747 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,750 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,750 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,750 Answer received: !ym
2024-06-18 13:52:18,751 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:52:18,751 Answer received: !yro492
2024-06-18 13:52:18,751 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,752 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,752 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,753 Answer received: !ym
2024-06-18 13:52:18,753 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:52:18,753 Answer received: !yro493
2024-06-18 13:52:18,753 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,755 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,755 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,755 Answer received: !ym
2024-06-18 13:52:18,755 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:52:18,756 Answer received: !yro494
2024-06-18 13:52:18,756 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,758 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,758 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,758 Answer received: !ym
2024-06-18 13:52:18,758 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:52:18,759 Answer received: !yro495
2024-06-18 13:52:18,759 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,760 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,760 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,760 Answer received: !ym
2024-06-18 13:52:18,761 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:52:18,761 Answer received: !yro496
2024-06-18 13:52:18,761 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,762 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,762 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,762 Answer received: !ym
2024-06-18 13:52:18,763 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:52:18,763 Answer received: !yro497
2024-06-18 13:52:18,763 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:18,764 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:18,764 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:18,764 Answer received: !ym
2024-06-18 13:52:18,764 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:18,764 Answer received: !ylo498
2024-06-18 13:52:18,765 Command to send: c
o498
add
ro492
e

2024-06-18 13:52:18,765 Answer received: !ybtrue
2024-06-18 13:52:18,765 Command to send: c
o498
add
ro493
e

2024-06-18 13:52:18,765 Answer received: !ybtrue
2024-06-18 13:52:18,766 Command to send: c
o498
add
ro494
e

2024-06-18 13:52:18,766 Answer received: !ybtrue
2024-06-18 13:52:18,766 Command to send: c
o498
add
ro495
e

2024-06-18 13:52:18,766 Answer received: !ybtrue
2024-06-18 13:52:18,766 Command to send: c
o498
add
ro496
e

2024-06-18 13:52:18,767 Answer received: !ybtrue
2024-06-18 13:52:18,767 Command to send: c
o498
add
ro497
e

2024-06-18 13:52:18,767 Answer received: !ybtrue
2024-06-18 13:52:18,767 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro498
e

2024-06-18 13:52:18,767 Answer received: !yro499
2024-06-18 13:52:18,767 Command to send: c
o471
select
ro499
e

2024-06-18 13:52:18,771 Answer received: !yro500
2024-06-18 13:52:18,772 Command to send: c
o500
schema
e

2024-06-18 13:52:18,772 Answer received: !yro501
2024-06-18 13:52:18,772 Command to send: c
o501
treeString
e

2024-06-18 13:52:18,772 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:52:18,773 Command to send: c
o500
count
e

2024-06-18 13:52:18,863 Answer received: !yL20189
2024-06-18 13:52:18,863 Command to send: c
o500
schema
e

2024-06-18 13:52:18,864 Answer received: !yro502
2024-06-18 13:52:18,864 Command to send: c
o502
json
e

2024-06-18 13:52:18,864 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:52:18,865 Command to send: c
o500
na
e

2024-06-18 13:52:18,867 Answer received: !yro503
2024-06-18 13:52:18,867 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:18,868 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:18,868 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:18,869 Answer received: !ym
2024-06-18 13:52:18,869 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:18,869 Answer received: !ylo504
2024-06-18 13:52:18,869 Command to send: c
o504
add
sdistrict
e

2024-06-18 13:52:18,869 Answer received: !ybtrue
2024-06-18 13:52:18,870 Command to send: c
o504
add
snumPhotos
e

2024-06-18 13:52:18,870 Answer received: !ybtrue
2024-06-18 13:52:18,870 Command to send: c
o504
add
sprice
e

2024-06-18 13:52:18,870 Answer received: !ybtrue
2024-06-18 13:52:18,870 Command to send: c
o504
add
sfloor
e

2024-06-18 13:52:18,871 Answer received: !ybtrue
2024-06-18 13:52:18,871 Command to send: c
o504
add
ssize
e

2024-06-18 13:52:18,871 Answer received: !ybtrue
2024-06-18 13:52:18,871 Command to send: c
o504
add
sbathrooms
e

2024-06-18 13:52:18,872 Answer received: !ybtrue
2024-06-18 13:52:18,872 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro504
e

2024-06-18 13:52:18,872 Answer received: !yro505
2024-06-18 13:52:18,872 Command to send: c
o503
drop
i6
ro505
e

2024-06-18 13:52:18,875 Answer received: !yro506
2024-06-18 13:52:18,876 Command to send: c
o506
count
e

2024-06-18 13:52:18,985 Answer received: !yL15545
2024-06-18 13:52:18,986 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,987 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,988 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,988 Answer received: !ym
2024-06-18 13:52:18,988 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:52:18,988 Answer received: !yro507
2024-06-18 13:52:18,989 Command to send: r
u
functions
rj
e

2024-06-18 13:52:18,991 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:18,991 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:18,991 Answer received: !ym
2024-06-18 13:52:18,991 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:52:18,992 Answer received: !yro508
2024-06-18 13:52:18,992 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:18,993 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:18,993 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:18,994 Answer received: !ym
2024-06-18 13:52:18,994 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:18,994 Answer received: !ylo509
2024-06-18 13:52:18,994 Command to send: c
o509
add
ro507
e

2024-06-18 13:52:18,994 Answer received: !ybtrue
2024-06-18 13:52:18,995 Command to send: c
o509
add
ro508
e

2024-06-18 13:52:18,995 Answer received: !ybtrue
2024-06-18 13:52:18,995 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro509
e

2024-06-18 13:52:18,995 Answer received: !yro510
2024-06-18 13:52:18,995 Command to send: c
o475
select
ro510
e

2024-06-18 13:52:19,000 Answer received: !yro511
2024-06-18 13:52:19,000 Command to send: c
o511
schema
e

2024-06-18 13:52:19,000 Answer received: !yro512
2024-06-18 13:52:19,001 Command to send: c
o512
treeString
e

2024-06-18 13:52:19,001 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:52:19,001 Command to send: c
o511
count
e

2024-06-18 13:52:19,104 Answer received: !yL113
2024-06-18 13:52:19,104 Command to send: c
o511
schema
e

2024-06-18 13:52:19,104 Answer received: !yro513
2024-06-18 13:52:19,105 Command to send: c
o513
json
e

2024-06-18 13:52:19,105 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:52:19,105 Command to send: c
o511
na
e

2024-06-18 13:52:19,107 Answer received: !yro514
2024-06-18 13:52:19,107 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:19,108 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:19,109 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:19,109 Answer received: !ym
2024-06-18 13:52:19,109 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:19,109 Answer received: !ylo515
2024-06-18 13:52:19,110 Command to send: c
o515
add
sdistrict
e

2024-06-18 13:52:19,110 Answer received: !ybtrue
2024-06-18 13:52:19,110 Command to send: c
o515
add
sdistrict_id
e

2024-06-18 13:52:19,111 Answer received: !ybtrue
2024-06-18 13:52:19,111 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro515
e

2024-06-18 13:52:19,111 Answer received: !yro516
2024-06-18 13:52:19,112 Command to send: c
o514
drop
i2
ro516
e

2024-06-18 13:52:19,114 Answer received: !yro517
2024-06-18 13:52:19,115 Command to send: c
o517
count
e

2024-06-18 13:52:19,213 Answer received: !yL113
2024-06-18 13:52:19,213 Command to send: r
u
functions
rj
e

2024-06-18 13:52:19,215 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:19,215 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:19,215 Answer received: !ym
2024-06-18 13:52:19,215 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:52:19,216 Answer received: !yro518
2024-06-18 13:52:19,216 Command to send: r
u
functions
rj
e

2024-06-18 13:52:19,217 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:19,217 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:19,218 Answer received: !ym
2024-06-18 13:52:19,218 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:52:19,218 Answer received: !yro519
2024-06-18 13:52:19,218 Command to send: r
u
functions
rj
e

2024-06-18 13:52:19,219 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:19,219 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:19,220 Answer received: !ym
2024-06-18 13:52:19,220 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:52:19,220 Answer received: !yro520
2024-06-18 13:52:19,220 Command to send: r
u
functions
rj
e

2024-06-18 13:52:19,221 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:19,222 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:19,222 Answer received: !ym
2024-06-18 13:52:19,222 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:52:19,222 Answer received: !yro521
2024-06-18 13:52:19,222 Command to send: r
u
functions
rj
e

2024-06-18 13:52:19,224 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:52:19,224 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:52:19,225 Answer received: !ym
2024-06-18 13:52:19,225 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:52:19,225 Answer received: !yro522
2024-06-18 13:52:19,225 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:52:19,226 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:52:19,226 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:52:19,227 Answer received: !ym
2024-06-18 13:52:19,227 Command to send: i
java.util.ArrayList
e

2024-06-18 13:52:19,227 Answer received: !ylo523
2024-06-18 13:52:19,227 Command to send: c
o523
add
ro518
e

2024-06-18 13:52:19,228 Answer received: !ybtrue
2024-06-18 13:52:19,228 Command to send: c
o523
add
ro519
e

2024-06-18 13:52:19,228 Answer received: !ybtrue
2024-06-18 13:52:19,228 Command to send: c
o523
add
ro520
e

2024-06-18 13:52:19,228 Answer received: !ybtrue
2024-06-18 13:52:19,229 Command to send: c
o523
add
ro521
e

2024-06-18 13:52:19,229 Answer received: !ybtrue
2024-06-18 13:52:19,229 Command to send: c
o523
add
ro522
e

2024-06-18 13:52:19,229 Answer received: !ybtrue
2024-06-18 13:52:19,229 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro523
e

2024-06-18 13:52:19,230 Answer received: !yro524
2024-06-18 13:52:19,230 Command to send: c
o479
select
ro524
e

2024-06-18 13:52:19,233 Answer received: !xro525
2024-06-18 13:52:19,234 Command to send: r
u
py4j
rj
e

2024-06-18 13:52:19,236 Answer received: !yp
2024-06-18 13:52:19,236 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:52:19,236 Answer received: !yp
2024-06-18 13:52:19,236 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:52:19,237 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:52:19,237 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:52:19,237 Answer received: !ym
2024-06-18 13:52:19,237 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro525
e

2024-06-18 13:52:19,237 Answer received: !ybfalse
2024-06-18 13:52:19,238 Command to send: r
u
py4j
rj
e

2024-06-18 13:52:19,239 Answer received: !yp
2024-06-18 13:52:19,240 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:52:19,240 Answer received: !yp
2024-06-18 13:52:19,240 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:52:19,241 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:52:19,241 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:52:19,241 Answer received: !ym
2024-06-18 13:52:19,241 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro525
e

2024-06-18 13:52:19,242 Answer received: !ybtrue
2024-06-18 13:52:19,242 Command to send: c
o525
getMessage
e

2024-06-18 13:52:19,242 Answer received: !ys[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1376,N_hotels#1377L,Avg_lat#1378,Avg_long#1379,Avg_Index_RFD#1380,numPhotos#1381L,price#1382,floor#1383,size#1384,bathrooms#1385L] parquet\n
2024-06-18 13:52:19,242 Command to send: r
u
org
rj
e

2024-06-18 13:52:19,244 Answer received: !yp
2024-06-18 13:52:19,244 Command to send: r
u
org.apache
rj
e

2024-06-18 13:52:19,245 Answer received: !yp
2024-06-18 13:52:19,245 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:52:19,245 Answer received: !yp
2024-06-18 13:52:19,246 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:52:19,246 Answer received: !yp
2024-06-18 13:52:19,246 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:52:19,246 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:52:19,247 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:52:19,247 Answer received: !ym
2024-06-18 13:52:19,247 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro525
e

2024-06-18 13:52:19,248 Answer received: !ysorg.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `addresses_district_id` cannot be resolved. Did you mean one of the following? [`Avg_stars`, `price`, `Avg_lat`, `N_hotels`, `size`].;\n'Project ['addresses_district_id, 'name, 'secondary_filters_name, 'geo_epgs_4326_lat, 'geo_epgs_4326_lon]\n+- Relation [Avg_stars#1376,N_hotels#1377L,Avg_lat#1378,Avg_long#1379,Avg_Index_RFD#1380,numPhotos#1381L,price#1382,floor#1383,size#1384,bathrooms#1385L] parquet\n\r\n	at org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:307)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:147)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:266)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$5$adapted(CheckAnalysis.scala:264)\r\n	at scala.collection.immutable.Stream.foreach(Stream.scala:533)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:264)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:244)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:182)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:164)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:160)\r\n	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:150)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:211)\r\n	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\r\n	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:208)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:77)\r\n	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\r\n	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\r\n	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:91)\r\n	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89)\r\n	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:4351)\r\n	at org.apache.spark.sql.Dataset.select(Dataset.scala:1540)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:52:19,248 Command to send: c
o525
getCause
e

2024-06-18 13:52:19,249 Answer received: !yn
2024-06-18 13:52:19,305 Command to send: r
u
org
rj
e

2024-06-18 13:52:19,307 Answer received: !yp
2024-06-18 13:52:19,307 Command to send: r
u
org.apache
rj
e

2024-06-18 13:52:19,308 Answer received: !yp
2024-06-18 13:52:19,308 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:52:19,308 Answer received: !yp
2024-06-18 13:52:19,309 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:52:19,309 Answer received: !yp
2024-06-18 13:52:19,309 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:52:19,309 Answer received: !yp
2024-06-18 13:52:19,310 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:52:19,310 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:52:19,310 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,310 Answer received: !ym
2024-06-18 13:52:19,310 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,311 Answer received: !yro526
2024-06-18 13:52:19,311 Command to send: c
o526
pysparkJVMStacktraceEnabled
e

2024-06-18 13:52:19,311 Answer received: !ybfalse
2024-06-18 13:52:19,312 Command to send: r
u
org
rj
e

2024-06-18 13:52:19,314 Answer received: !yp
2024-06-18 13:52:19,314 Command to send: r
u
org.apache
rj
e

2024-06-18 13:52:19,315 Answer received: !yp
2024-06-18 13:52:19,315 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:52:19,315 Answer received: !yp
2024-06-18 13:52:19,315 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:52:19,316 Answer received: !yp
2024-06-18 13:52:19,316 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:52:19,316 Answer received: !yp
2024-06-18 13:52:19,316 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:52:19,317 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:52:19,317 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,317 Answer received: !ym
2024-06-18 13:52:19,317 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,317 Answer received: !yro527
2024-06-18 13:52:19,318 Command to send: c
o527
pysparkJVMStacktraceEnabled
e

2024-06-18 13:52:19,318 Answer received: !ybfalse
2024-06-18 13:52:19,319 Command to send: r
u
org
rj
e

2024-06-18 13:52:19,320 Answer received: !yp
2024-06-18 13:52:19,320 Command to send: r
u
org.apache
rj
e

2024-06-18 13:52:19,321 Answer received: !yp
2024-06-18 13:52:19,321 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:52:19,321 Answer received: !yp
2024-06-18 13:52:19,321 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:52:19,323 Answer received: !yp
2024-06-18 13:52:19,323 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:52:19,323 Answer received: !yp
2024-06-18 13:52:19,323 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:52:19,323 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:52:19,323 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,325 Answer received: !ym
2024-06-18 13:52:19,325 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:52:19,325 Answer received: !yro528
2024-06-18 13:52:19,325 Command to send: c
o528
pysparkJVMStacktraceEnabled
e

2024-06-18 13:52:19,325 Answer received: !ybfalse
2024-06-18 13:52:19,733 Command to send: m
d
o498
e

2024-06-18 13:52:19,733 Answer received: !yv
2024-06-18 13:52:19,733 Command to send: m
d
o504
e

2024-06-18 13:52:19,733 Answer received: !yv
2024-06-18 13:52:19,734 Command to send: m
d
o509
e

2024-06-18 13:52:19,734 Answer received: !yv
2024-06-18 13:52:19,734 Command to send: m
d
o515
e

2024-06-18 13:52:19,734 Answer received: !yv
2024-06-18 13:52:19,734 Command to send: m
d
o523
e

2024-06-18 13:52:19,735 Answer received: !yv
2024-06-18 13:52:19,735 Command to send: m
d
o480
e

2024-06-18 13:52:19,735 Answer received: !yv
2024-06-18 13:52:19,735 Command to send: m
d
o481
e

2024-06-18 13:52:19,735 Answer received: !yv
2024-06-18 13:52:19,735 Command to send: m
d
o482
e

2024-06-18 13:52:19,736 Answer received: !yv
2024-06-18 13:52:19,736 Command to send: m
d
o484
e

2024-06-18 13:52:19,736 Answer received: !yv
2024-06-18 13:52:19,736 Command to send: m
d
o485
e

2024-06-18 13:52:19,737 Answer received: !yv
2024-06-18 13:52:19,737 Command to send: m
d
o486
e

2024-06-18 13:52:19,737 Answer received: !yv
2024-06-18 13:52:19,737 Command to send: m
d
o487
e

2024-06-18 13:52:19,737 Answer received: !yv
2024-06-18 13:52:19,737 Command to send: m
d
o488
e

2024-06-18 13:52:19,738 Answer received: !yv
2024-06-18 13:52:19,738 Command to send: m
d
o490
e

2024-06-18 13:52:19,738 Answer received: !yv
2024-06-18 13:52:19,738 Command to send: m
d
o492
e

2024-06-18 13:52:19,738 Answer received: !yv
2024-06-18 13:52:19,738 Command to send: m
d
o493
e

2024-06-18 13:52:19,739 Answer received: !yv
2024-06-18 13:52:19,739 Command to send: m
d
o494
e

2024-06-18 13:52:19,739 Answer received: !yv
2024-06-18 13:52:19,739 Command to send: m
d
o495
e

2024-06-18 13:52:19,739 Answer received: !yv
2024-06-18 13:52:19,739 Command to send: m
d
o496
e

2024-06-18 13:52:19,740 Answer received: !yv
2024-06-18 13:52:19,740 Command to send: m
d
o497
e

2024-06-18 13:52:19,740 Answer received: !yv
2024-06-18 13:52:19,740 Command to send: m
d
o499
e

2024-06-18 13:52:19,740 Answer received: !yv
2024-06-18 13:52:19,741 Command to send: m
d
o500
e

2024-06-18 13:52:19,741 Answer received: !yv
2024-06-18 13:52:19,741 Command to send: m
d
o501
e

2024-06-18 13:52:19,741 Answer received: !yv
2024-06-18 13:52:19,741 Command to send: m
d
o502
e

2024-06-18 13:52:19,742 Answer received: !yv
2024-06-18 13:52:19,742 Command to send: m
d
o503
e

2024-06-18 13:52:19,742 Answer received: !yv
2024-06-18 13:52:19,742 Command to send: m
d
o505
e

2024-06-18 13:52:19,742 Answer received: !yv
2024-06-18 13:52:19,742 Command to send: m
d
o507
e

2024-06-18 13:52:19,743 Answer received: !yv
2024-06-18 13:52:19,743 Command to send: m
d
o508
e

2024-06-18 13:52:19,743 Answer received: !yv
2024-06-18 13:52:19,743 Command to send: m
d
o510
e

2024-06-18 13:52:19,743 Answer received: !yv
2024-06-18 13:52:19,743 Command to send: m
d
o511
e

2024-06-18 13:52:19,744 Answer received: !yv
2024-06-18 13:52:19,744 Command to send: m
d
o512
e

2024-06-18 13:52:19,744 Answer received: !yv
2024-06-18 13:52:19,744 Command to send: m
d
o513
e

2024-06-18 13:52:19,744 Answer received: !yv
2024-06-18 13:52:19,744 Command to send: m
d
o514
e

2024-06-18 13:52:19,745 Answer received: !yv
2024-06-18 13:52:19,745 Command to send: m
d
o516
e

2024-06-18 13:52:19,745 Answer received: !yv
2024-06-18 13:52:19,745 Command to send: m
d
o518
e

2024-06-18 13:52:19,745 Answer received: !yv
2024-06-18 13:52:19,745 Command to send: m
d
o519
e

2024-06-18 13:52:19,746 Answer received: !yv
2024-06-18 13:52:19,746 Command to send: m
d
o520
e

2024-06-18 13:52:19,746 Answer received: !yv
2024-06-18 13:52:19,746 Command to send: m
d
o521
e

2024-06-18 13:52:19,746 Answer received: !yv
2024-06-18 13:52:19,746 Command to send: m
d
o522
e

2024-06-18 13:52:19,746 Answer received: !yv
2024-06-18 13:52:23,749 Command to send: m
d
o526
e

2024-06-18 13:52:23,749 Answer received: !yv
2024-06-18 13:52:23,749 Command to send: m
d
o527
e

2024-06-18 13:52:23,750 Answer received: !yv
2024-06-18 13:52:23,750 Command to send: m
d
o528
e

2024-06-18 13:52:23,750 Answer received: !yv
2024-06-18 13:53:13,395 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:53:13,396 Listing 'user/bdm/Formatted_zone/hotels'.
2024-06-18 13:53:13,396 Resolved path '/' to '/'.
2024-06-18 13:53:13,398 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:53:13,413 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:53:13,413 Updated root to '/user/bdm'.
2024-06-18 13:53:13,414 Resolved path 'user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:53:13,414 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:53:13,423 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,423 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:53:13,423 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels'.
2024-06-18 13:53:13,431 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,440 Listing 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:53:13,440 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:53:13,441 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels'.
2024-06-18 13:53:13,449 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,450 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to 'model_temp/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,450 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,450 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' (depth 0).
2024-06-18 13:53:13,450 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,451 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,451 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,494 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,494 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,494 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\._SUCCESS.crc'.
2024-06-18 13:53:13,495 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,495 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,504 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,505 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:53:13,528 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:53:13,528 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc'.
2024-06-18 13:53:13,529 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\._SUCCESS.crc' complete.
2024-06-18 13:53:13,529 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to 'model_temp/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,529 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,530 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:53:13,530 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,530 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,530 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,540 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,540 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,540 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,541 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,541 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,549 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,552 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,568 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 656
2024-06-18 13:53:13,568 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,569 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\.part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet.crc' complete.
2024-06-18 13:53:13,569 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to 'model_temp/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,569 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,570 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' (depth 0).
2024-06-18 13:53:13,570 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,570 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,570 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,580 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,580 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,580 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\_SUCCESS'.
2024-06-18 13:53:13,580 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,580 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,589 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,591 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,605 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:53:13,605 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS'.
2024-06-18 13:53:13,606 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\_SUCCESS' complete.
2024-06-18 13:53:13,606 Downloading 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to 'model_temp/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,606 Resolved path 'user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,607 Walking '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' (depth 0).
2024-06-18 13:53:13,607 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,607 Fetching status for '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,607 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,616 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,616 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,617 Downloading '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,619 Reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,619 Resolved path '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,628 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,630 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,649 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 82434
2024-06-18 13:53:13,679 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet'.
2024-06-18 13:53:13,680 Download of /user/bdm/user/bdm/Formatted_zone/hotels/20240615-1328-hotels/part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240615-1328-hotels\\part-00000-3156af16-7102-418e-8f75-613ab9a57fc7-c000.snappy.parquet' complete.
2024-06-18 13:53:13,680 File 20240615-1328-hotels downloaded correctly
2024-06-18 13:53:13,680 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:53:13,680 Listing 'user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:53:13,680 Resolved path '/' to '/'.
2024-06-18 13:53:13,682 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:53:13,723 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:53:13,725 Updated root to '/user/bdm'.
2024-06-18 13:53:13,725 Resolved path 'user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:53:13,725 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:53:13,736 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,737 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:53:13,737 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar'.
2024-06-18 13:53:13,745 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,746 Listing 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:53:13,746 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:53:13,746 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar'.
2024-06-18 13:53:13,754 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,755 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to 'model_temp/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,755 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,755 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' (depth 0).
2024-06-18 13:53:13,756 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,756 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,756 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,764 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,765 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,765 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\._SUCCESS.crc'.
2024-06-18 13:53:13,765 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,765 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,773 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,775 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:53:13,794 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:53:13,794 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc'.
2024-06-18 13:53:13,795 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\._SUCCESS.crc' complete.
2024-06-18 13:53:13,795 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to 'model_temp/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,795 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,796 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:53:13,796 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,796 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,796 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,806 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,807 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,807 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,807 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,808 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,817 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,819 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,836 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 136
2024-06-18 13:53:13,836 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc'.
2024-06-18 13:53:13,836 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\.part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet.crc' complete.
2024-06-18 13:53:13,836 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to 'model_temp/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,837 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,837 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' (depth 0).
2024-06-18 13:53:13,837 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,837 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,838 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,848 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,848 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,848 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\_SUCCESS'.
2024-06-18 13:53:13,850 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,850 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,858 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,860 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,876 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:53:13,876 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS'.
2024-06-18 13:53:13,876 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\_SUCCESS' complete.
2024-06-18 13:53:13,876 Downloading 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to 'model_temp/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,876 Resolved path 'user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,877 Walking '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' (depth 0).
2024-06-18 13:53:13,877 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,877 Fetching status for '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,877 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,887 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,887 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,888 Downloading '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,889 Reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,889 Resolved path '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,898 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,900 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:13,916 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 16241
2024-06-18 13:53:13,922 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet'.
2024-06-18 13:53:13,922 Download of /user/bdm/user/bdm/Formatted_zone/renda_familiar/20240614-2304-renda_familiar/part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2304-renda_familiar\\part-00000-e9324cc8-1621-43c9-8e01-04fdda7bcf5a-c000.snappy.parquet' complete.
2024-06-18 13:53:13,922 File 20240614-2304-renda_familiar downloaded correctly
2024-06-18 13:53:13,922 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:53:13,922 Listing 'user/bdm/Formatted_zone/idealista'.
2024-06-18 13:53:13,923 Resolved path '/' to '/'.
2024-06-18 13:53:13,924 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:53:13,942 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:53:13,943 Updated root to '/user/bdm'.
2024-06-18 13:53:13,943 Resolved path 'user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:53:13,943 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:53:13,951 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,951 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:53:13,951 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista'.
2024-06-18 13:53:13,960 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,961 Listing 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:53:13,961 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:53:13,961 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista'.
2024-06-18 13:53:13,970 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,970 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to 'model_temp/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,970 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,971 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' (depth 0).
2024-06-18 13:53:13,971 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,971 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,971 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,980 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:13,980 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:13,980 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\._SUCCESS.crc'.
2024-06-18 13:53:13,981 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,981 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:13,991 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:13,993 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:53:14,007 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:53:14,008 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,008 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\._SUCCESS.crc' complete.
2024-06-18 13:53:14,008 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to 'model_temp/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,008 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,009 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:53:14,009 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,009 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,009 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,017 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,018 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,018 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,023 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,023 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,032 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,034 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,049 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 10236
2024-06-18 13:53:14,050 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,050 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\.part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet.crc' complete.
2024-06-18 13:53:14,050 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to 'model_temp/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,050 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,051 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' (depth 0).
2024-06-18 13:53:14,051 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,051 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,052 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,061 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,061 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,061 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\_SUCCESS'.
2024-06-18 13:53:14,062 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,062 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,070 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,072 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,086 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:53:14,086 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS'.
2024-06-18 13:53:14,087 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\_SUCCESS' complete.
2024-06-18 13:53:14,087 Downloading 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to 'model_temp/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,087 Resolved path 'user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,087 Walking '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' (depth 0).
2024-06-18 13:53:14,088 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,088 Fetching status for '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,088 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,096 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,096 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,097 Downloading '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,098 Reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,098 Resolved path '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,107 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,109 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,126 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 1308973
2024-06-18 13:53:14,361 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet'.
2024-06-18 13:53:14,362 Download of /user/bdm/user/bdm/Formatted_zone/idealista/20240614-2306-idealista/part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2306-idealista\\part-00000-5ede5d9f-67df-4807-809e-9cca684dd7f7-c000.snappy.parquet' complete.
2024-06-18 13:53:14,362 File 20240614-2306-idealista downloaded correctly
2024-06-18 13:53:14,362 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:53:14,362 Listing 'user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:53:14,362 Resolved path '/' to '/'.
2024-06-18 13:53:14,364 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:53:14,404 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:53:14,405 Updated root to '/user/bdm'.
2024-06-18 13:53:14,405 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:53:14,405 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:53:14,415 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,415 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:53:14,415 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista'.
2024-06-18 13:53:14,424 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,425 Listing 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:53:14,425 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:53:14,425 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista'.
2024-06-18 13:53:14,436 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista?user.name=bdm&op=LISTSTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,437 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to 'model_temp/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,437 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,438 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' (depth 0).
2024-06-18 13:53:14,438 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,438 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,438 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,445 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,446 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,446 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\._SUCCESS.crc'.
2024-06-18 13:53:14,447 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,448 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,458 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,460 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:53:14,476 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 8
2024-06-18 13:53:14,477 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc'.
2024-06-18 13:53:14,477 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/._SUCCESS.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\._SUCCESS.crc' complete.
2024-06-18 13:53:14,477 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to 'model_temp/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,477 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,478 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' (depth 0).
2024-06-18 13:53:14,478 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,478 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,478 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,490 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,490 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,490 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,491 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,491 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,500 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,502 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,517 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 88
2024-06-18 13:53:14,517 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc'.
2024-06-18 13:53:14,518 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\.part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet.crc' complete.
2024-06-18 13:53:14,518 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to 'model_temp/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,518 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,519 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' (depth 0).
2024-06-18 13:53:14,519 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,519 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,519 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,529 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,529 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,529 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\_SUCCESS'.
2024-06-18 13:53:14,530 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,530 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,538 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,540 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,555 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 0
2024-06-18 13:53:14,555 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS'.
2024-06-18 13:53:14,556 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/_SUCCESS to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\_SUCCESS' complete.
2024-06-18 13:53:14,556 Downloading 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to 'model_temp/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,556 Resolved path 'user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,557 Walking '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' (depth 0).
2024-06-18 13:53:14,557 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,557 Fetching status for '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,557 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,567 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?user.name=bdm&op=GETFILESTATUS HTTP/1.1" 200 None
2024-06-18 13:53:14,567 Downloading 1 files using 1 thread(s).
2024-06-18 13:53:14,567 Downloading '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,569 Reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,569 Resolved path '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' to '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,577 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?user.name=bdm&offset=0&op=OPEN HTTP/1.1" 307 0
2024-06-18 13:53:14,579 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:53:14,624 http://charmander.fib.upc.es:9864 "GET /webhdfs/v1/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet?op=OPEN&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&offset=0 HTTP/1.1" 200 9934
2024-06-18 13:53:14,625 Closed response for reading file '/user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet'.
2024-06-18 13:53:14,625 Download of /user/bdm/user/bdm/Formatted_zone/lookup_renta_idealista/20240614-2308-lookup_renta_idealista/part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet to 'D:\\Documents\\Data Science\\Big_Data_Management\\Project2\\model_temp\\20240614-2308-lookup_renta_idealista\\part-00000-33079e31-e817-436a-b4e2-ecb0e385aa74-c000.snappy.parquet' complete.
2024-06-18 13:53:14,625 File 20240614-2308-lookup_renta_idealista downloaded correctly
2024-06-18 13:53:16,496 Command to send: r
u
SparkConf
rj
e

2024-06-18 13:53:16,497 Answer received: !ycorg.apache.spark.SparkConf
2024-06-18 13:53:16,497 Command to send: i
org.apache.spark.SparkConf
bTrue
e

2024-06-18 13:53:16,498 Answer received: !yro529
2024-06-18 13:53:16,499 Command to send: c
o529
set
sspark.master
slocal
e

2024-06-18 13:53:16,499 Answer received: !yro530
2024-06-18 13:53:16,499 Command to send: c
o529
set
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:53:16,500 Answer received: !yro531
2024-06-18 13:53:16,500 Command to send: c
o529
getAll
e

2024-06-18 13:53:16,501 Answer received: !yto532
2024-06-18 13:53:16,501 Command to send: a
e
o532
e

2024-06-18 13:53:16,502 Answer received: !yi6
2024-06-18 13:53:16,502 Command to send: a
g
o532
i0
e

2024-06-18 13:53:16,502 Answer received: !yro533
2024-06-18 13:53:16,502 Command to send: c
o533
_1
e

2024-06-18 13:53:16,503 Answer received: !ysspark.master
2024-06-18 13:53:16,503 Command to send: c
o533
_2
e

2024-06-18 13:53:16,503 Answer received: !yslocal
2024-06-18 13:53:16,503 Command to send: a
e
o532
e

2024-06-18 13:53:16,504 Answer received: !yi6
2024-06-18 13:53:16,504 Command to send: a
g
o532
i1
e

2024-06-18 13:53:16,504 Answer received: !yro534
2024-06-18 13:53:16,504 Command to send: c
o534
_1
e

2024-06-18 13:53:16,504 Answer received: !ysspark.submit.pyFiles
2024-06-18 13:53:16,505 Command to send: c
o534
_2
e

2024-06-18 13:53:16,505 Answer received: !ys
2024-06-18 13:53:16,505 Command to send: a
e
o532
e

2024-06-18 13:53:16,505 Answer received: !yi6
2024-06-18 13:53:16,505 Command to send: a
g
o532
i2
e

2024-06-18 13:53:16,506 Answer received: !yro535
2024-06-18 13:53:16,506 Command to send: c
o535
_1
e

2024-06-18 13:53:16,506 Answer received: !ysspark.app.submitTime
2024-06-18 13:53:16,506 Command to send: c
o535
_2
e

2024-06-18 13:53:16,507 Answer received: !ys1718709747485
2024-06-18 13:53:16,507 Command to send: a
e
o532
e

2024-06-18 13:53:16,507 Answer received: !yi6
2024-06-18 13:53:16,507 Command to send: a
g
o532
i3
e

2024-06-18 13:53:16,508 Answer received: !yro536
2024-06-18 13:53:16,508 Command to send: c
o536
_1
e

2024-06-18 13:53:16,508 Answer received: !ysspark.submit.deployMode
2024-06-18 13:53:16,508 Command to send: c
o536
_2
e

2024-06-18 13:53:16,509 Answer received: !ysclient
2024-06-18 13:53:16,509 Command to send: a
e
o532
e

2024-06-18 13:53:16,510 Answer received: !yi6
2024-06-18 13:53:16,510 Command to send: a
g
o532
i4
e

2024-06-18 13:53:16,510 Answer received: !yro537
2024-06-18 13:53:16,510 Command to send: c
o537
_1
e

2024-06-18 13:53:16,510 Answer received: !ysspark.app.name
2024-06-18 13:53:16,511 Command to send: c
o537
_2
e

2024-06-18 13:53:16,511 Answer received: !ysFormatted zone loader
2024-06-18 13:53:16,511 Command to send: a
e
o532
e

2024-06-18 13:53:16,511 Answer received: !yi6
2024-06-18 13:53:16,511 Command to send: a
g
o532
i5
e

2024-06-18 13:53:16,512 Answer received: !yro538
2024-06-18 13:53:16,512 Command to send: c
o538
_1
e

2024-06-18 13:53:16,512 Answer received: !ysspark.ui.showConsoleProgress
2024-06-18 13:53:16,512 Command to send: c
o538
_2
e

2024-06-18 13:53:16,513 Answer received: !ystrue
2024-06-18 13:53:16,513 Command to send: a
e
o532
e

2024-06-18 13:53:16,513 Answer received: !yi6
2024-06-18 13:53:16,513 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:53:16,515 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:53:16,515 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:53:16,515 Answer received: !yro539
2024-06-18 13:53:16,515 Command to send: i
java.util.HashMap
e

2024-06-18 13:53:16,516 Answer received: !yao540
2024-06-18 13:53:16,516 Command to send: c
o540
put
sspark.master
slocal
e

2024-06-18 13:53:16,517 Answer received: !yn
2024-06-18 13:53:16,517 Command to send: c
o540
put
sspark.submit.pyFiles
s
e

2024-06-18 13:53:16,517 Answer received: !yn
2024-06-18 13:53:16,518 Command to send: c
o540
put
sspark.app.submitTime
s1718709747485
e

2024-06-18 13:53:16,518 Answer received: !yn
2024-06-18 13:53:16,518 Command to send: c
o540
put
sspark.submit.deployMode
sclient
e

2024-06-18 13:53:16,518 Answer received: !yn
2024-06-18 13:53:16,519 Command to send: c
o540
put
sspark.app.name
sFormatted zone loader
e

2024-06-18 13:53:16,519 Answer received: !yn
2024-06-18 13:53:16,519 Command to send: c
o540
put
sspark.ui.showConsoleProgress
strue
e

2024-06-18 13:53:16,519 Answer received: !yn
2024-06-18 13:53:16,520 Command to send: c
o539
applyModifiableSettings
ro25
ro540
e

2024-06-18 13:53:16,520 Answer received: !yv
2024-06-18 13:53:16,520 Spark sesion correctly initialized
2024-06-18 13:53:16,521 Command to send: c
o25
read
e

2024-06-18 13:53:16,521 Answer received: !yro541
2024-06-18 13:53:16,521 Command to send: c
o541
option
smultiline
strue
e

2024-06-18 13:53:16,522 Answer received: !yro542
2024-06-18 13:53:16,522 Command to send: c
o542
format
sparquet
e

2024-06-18 13:53:16,522 Answer received: !yro543
2024-06-18 13:53:16,522 Command to send: c
o543
load
smodel_temp/20240614-2304-renda_familiar
e

2024-06-18 13:53:16,685 Answer received: !yro544
2024-06-18 13:53:16,685 Command to send: c
o25
read
e

2024-06-18 13:53:16,686 Answer received: !yro545
2024-06-18 13:53:16,686 Command to send: c
o545
option
smultiline
strue
e

2024-06-18 13:53:16,686 Answer received: !yro546
2024-06-18 13:53:16,687 Command to send: c
o546
format
sparquet
e

2024-06-18 13:53:16,687 Answer received: !yro547
2024-06-18 13:53:16,687 Command to send: c
o547
load
smodel_temp/20240614-2306-idealista
e

2024-06-18 13:53:16,781 Command to send: m
d
o530
e

2024-06-18 13:53:16,781 Answer received: !yv
2024-06-18 13:53:16,781 Command to send: m
d
o531
e

2024-06-18 13:53:16,781 Answer received: !yv
2024-06-18 13:53:16,782 Command to send: m
d
o532
e

2024-06-18 13:53:16,782 Answer received: !yv
2024-06-18 13:53:16,782 Command to send: m
d
o540
e

2024-06-18 13:53:16,782 Answer received: !yv
2024-06-18 13:53:16,839 Answer received: !yro548
2024-06-18 13:53:16,839 Command to send: c
o25
read
e

2024-06-18 13:53:16,840 Answer received: !yro549
2024-06-18 13:53:16,840 Command to send: c
o549
option
smultiline
strue
e

2024-06-18 13:53:16,840 Answer received: !yro550
2024-06-18 13:53:16,841 Command to send: c
o550
format
sparquet
e

2024-06-18 13:53:16,841 Answer received: !yro551
2024-06-18 13:53:16,841 Command to send: c
o551
load
smodel_temp/20240614-2308-lookup_renta_idealista
e

2024-06-18 13:53:16,943 Answer received: !yro552
2024-06-18 13:53:16,943 Command to send: c
o25
read
e

2024-06-18 13:53:16,944 Answer received: !yro553
2024-06-18 13:53:16,944 Command to send: c
o553
option
smultiline
strue
e

2024-06-18 13:53:16,944 Answer received: !yro554
2024-06-18 13:53:16,944 Command to send: c
o554
format
sparquet
e

2024-06-18 13:53:16,945 Answer received: !yro555
2024-06-18 13:53:16,945 Command to send: c
o555
load
smodel_temp/20240615-1328-hotels
e

2024-06-18 13:53:17,051 Answer received: !yro556
2024-06-18 13:53:24,949 Command to send: r
u
functions
rj
e

2024-06-18 13:53:24,951 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:24,951 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:24,952 Answer received: !ym
2024-06-18 13:53:24,952 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:53:24,952 Answer received: !yro557
2024-06-18 13:53:24,953 Command to send: r
u
functions
rj
e

2024-06-18 13:53:24,954 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:24,954 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:24,954 Answer received: !ym
2024-06-18 13:53:24,955 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:53:24,956 Answer received: !yro558
2024-06-18 13:53:24,956 Command to send: r
u
functions
rj
e

2024-06-18 13:53:24,958 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:24,958 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:24,959 Answer received: !ym
2024-06-18 13:53:24,959 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:53:24,959 Answer received: !yro559
2024-06-18 13:53:24,959 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:24,960 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:24,961 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:24,961 Answer received: !ym
2024-06-18 13:53:24,961 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:24,962 Answer received: !ylo560
2024-06-18 13:53:24,962 Command to send: c
o560
add
ro557
e

2024-06-18 13:53:24,962 Answer received: !ybtrue
2024-06-18 13:53:24,962 Command to send: c
o560
add
ro558
e

2024-06-18 13:53:24,962 Answer received: !ybtrue
2024-06-18 13:53:24,963 Command to send: c
o560
add
ro559
e

2024-06-18 13:53:24,963 Answer received: !ybtrue
2024-06-18 13:53:24,963 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro560
e

2024-06-18 13:53:24,963 Answer received: !yro561
2024-06-18 13:53:24,964 Command to send: c
o544
select
ro561
e

2024-06-18 13:53:24,968 Answer received: !yro562
2024-06-18 13:53:24,968 Command to send: c
o562
schema
e

2024-06-18 13:53:24,968 Answer received: !yro563
2024-06-18 13:53:24,968 Command to send: c
o563
treeString
e

2024-06-18 13:53:24,969 Answer received: !ysroot\n |-- Nom_Districte: string (nullable = true)\n |-- Codi_Districte: string (nullable = true)\n |-- ndex RFD Barcelona = 100: string (nullable = true)\n
2024-06-18 13:53:24,969 Command to send: c
o562
count
e

2024-06-18 13:53:25,052 Answer received: !yL811
2024-06-18 13:53:25,052 Command to send: c
o562
schema
e

2024-06-18 13:53:25,052 Answer received: !yro564
2024-06-18 13:53:25,052 Command to send: c
o564
json
e

2024-06-18 13:53:25,052 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"ndex RFD Barcelona = 100","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}}]}
2024-06-18 13:53:25,052 Command to send: c
o562
na
e

2024-06-18 13:53:25,053 Answer received: !yro565
2024-06-18 13:53:25,053 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,054 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,054 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,055 Answer received: !ym
2024-06-18 13:53:25,055 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,055 Answer received: !ylo566
2024-06-18 13:53:25,055 Command to send: c
o566
add
sNom_Districte
e

2024-06-18 13:53:25,056 Answer received: !ybtrue
2024-06-18 13:53:25,056 Command to send: c
o566
add
sCodi_Districte
e

2024-06-18 13:53:25,056 Answer received: !ybtrue
2024-06-18 13:53:25,056 Command to send: c
o566
add
sndex RFD Barcelona = 100
e

2024-06-18 13:53:25,057 Answer received: !ybtrue
2024-06-18 13:53:25,057 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro566
e

2024-06-18 13:53:25,058 Answer received: !yro567
2024-06-18 13:53:25,058 Command to send: c
o565
drop
i3
ro567
e

2024-06-18 13:53:25,060 Answer received: !yro568
2024-06-18 13:53:25,060 Command to send: c
o568
count
e

2024-06-18 13:53:25,149 Answer received: !yL811
2024-06-18 13:53:25,150 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,151 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,151 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,152 Answer received: !ym
2024-06-18 13:53:25,152 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:53:25,152 Answer received: !yro569
2024-06-18 13:53:25,152 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,153 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,153 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,154 Answer received: !ym
2024-06-18 13:53:25,154 Command to send: c
z:org.apache.spark.sql.functions
col
snumPhotos
e

2024-06-18 13:53:25,154 Answer received: !yro570
2024-06-18 13:53:25,154 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,156 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,156 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,156 Answer received: !ym
2024-06-18 13:53:25,156 Command to send: c
z:org.apache.spark.sql.functions
col
sprice
e

2024-06-18 13:53:25,157 Answer received: !yro571
2024-06-18 13:53:25,157 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,158 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,158 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,159 Answer received: !ym
2024-06-18 13:53:25,159 Command to send: c
z:org.apache.spark.sql.functions
col
sfloor
e

2024-06-18 13:53:25,159 Answer received: !yro572
2024-06-18 13:53:25,159 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,160 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,161 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,161 Answer received: !ym
2024-06-18 13:53:25,161 Command to send: c
z:org.apache.spark.sql.functions
col
ssize
e

2024-06-18 13:53:25,161 Answer received: !yro573
2024-06-18 13:53:25,161 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,163 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,163 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,163 Answer received: !ym
2024-06-18 13:53:25,163 Command to send: c
z:org.apache.spark.sql.functions
col
sbathrooms
e

2024-06-18 13:53:25,163 Answer received: !yro574
2024-06-18 13:53:25,164 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,164 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,164 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,165 Answer received: !ym
2024-06-18 13:53:25,165 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,165 Answer received: !ylo575
2024-06-18 13:53:25,165 Command to send: c
o575
add
ro569
e

2024-06-18 13:53:25,166 Answer received: !ybtrue
2024-06-18 13:53:25,166 Command to send: c
o575
add
ro570
e

2024-06-18 13:53:25,166 Answer received: !ybtrue
2024-06-18 13:53:25,166 Command to send: c
o575
add
ro571
e

2024-06-18 13:53:25,166 Answer received: !ybtrue
2024-06-18 13:53:25,167 Command to send: c
o575
add
ro572
e

2024-06-18 13:53:25,167 Answer received: !ybtrue
2024-06-18 13:53:25,167 Command to send: c
o575
add
ro573
e

2024-06-18 13:53:25,167 Answer received: !ybtrue
2024-06-18 13:53:25,167 Command to send: c
o575
add
ro574
e

2024-06-18 13:53:25,168 Answer received: !ybtrue
2024-06-18 13:53:25,168 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro575
e

2024-06-18 13:53:25,168 Answer received: !yro576
2024-06-18 13:53:25,168 Command to send: c
o548
select
ro576
e

2024-06-18 13:53:25,172 Answer received: !yro577
2024-06-18 13:53:25,172 Command to send: c
o577
schema
e

2024-06-18 13:53:25,173 Answer received: !yro578
2024-06-18 13:53:25,173 Command to send: c
o578
treeString
e

2024-06-18 13:53:25,173 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- numPhotos: long (nullable = true)\n |-- price: double (nullable = true)\n |-- floor: string (nullable = true)\n |-- size: double (nullable = true)\n |-- bathrooms: long (nullable = true)\n
2024-06-18 13:53:25,173 Command to send: c
o577
count
e

2024-06-18 13:53:25,268 Answer received: !yL20189
2024-06-18 13:53:25,268 Command to send: c
o577
schema
e

2024-06-18 13:53:25,268 Answer received: !yro579
2024-06-18 13:53:25,268 Command to send: c
o579
json
e

2024-06-18 13:53:25,269 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:53:25,269 Command to send: c
o577
na
e

2024-06-18 13:53:25,270 Answer received: !yro580
2024-06-18 13:53:25,270 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,271 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,271 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,272 Answer received: !ym
2024-06-18 13:53:25,272 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,272 Answer received: !ylo581
2024-06-18 13:53:25,272 Command to send: c
o581
add
sdistrict
e

2024-06-18 13:53:25,273 Answer received: !ybtrue
2024-06-18 13:53:25,273 Command to send: c
o581
add
snumPhotos
e

2024-06-18 13:53:25,273 Answer received: !ybtrue
2024-06-18 13:53:25,273 Command to send: c
o581
add
sprice
e

2024-06-18 13:53:25,274 Answer received: !ybtrue
2024-06-18 13:53:25,274 Command to send: c
o581
add
sfloor
e

2024-06-18 13:53:25,274 Answer received: !ybtrue
2024-06-18 13:53:25,274 Command to send: c
o581
add
ssize
e

2024-06-18 13:53:25,274 Answer received: !ybtrue
2024-06-18 13:53:25,275 Command to send: c
o581
add
sbathrooms
e

2024-06-18 13:53:25,275 Answer received: !ybtrue
2024-06-18 13:53:25,275 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro581
e

2024-06-18 13:53:25,275 Answer received: !yro582
2024-06-18 13:53:25,276 Command to send: c
o580
drop
i6
ro582
e

2024-06-18 13:53:25,278 Answer received: !yro583
2024-06-18 13:53:25,278 Command to send: c
o583
count
e

2024-06-18 13:53:25,372 Answer received: !yL15545
2024-06-18 13:53:25,372 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,374 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,374 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,375 Answer received: !ym
2024-06-18 13:53:25,375 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict
e

2024-06-18 13:53:25,375 Answer received: !yro584
2024-06-18 13:53:25,375 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,377 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,377 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,377 Answer received: !ym
2024-06-18 13:53:25,377 Command to send: c
z:org.apache.spark.sql.functions
col
sdistrict_id
e

2024-06-18 13:53:25,377 Answer received: !yro585
2024-06-18 13:53:25,378 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,378 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,378 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,379 Answer received: !ym
2024-06-18 13:53:25,379 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,379 Answer received: !ylo586
2024-06-18 13:53:25,379 Command to send: c
o586
add
ro584
e

2024-06-18 13:53:25,380 Answer received: !ybtrue
2024-06-18 13:53:25,380 Command to send: c
o586
add
ro585
e

2024-06-18 13:53:25,380 Answer received: !ybtrue
2024-06-18 13:53:25,380 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro586
e

2024-06-18 13:53:25,381 Answer received: !yro587
2024-06-18 13:53:25,381 Command to send: c
o552
select
ro587
e

2024-06-18 13:53:25,384 Answer received: !yro588
2024-06-18 13:53:25,384 Command to send: c
o588
schema
e

2024-06-18 13:53:25,384 Answer received: !yro589
2024-06-18 13:53:25,384 Command to send: c
o589
treeString
e

2024-06-18 13:53:25,385 Answer received: !ysroot\n |-- district: string (nullable = true)\n |-- district_id: string (nullable = true)\n
2024-06-18 13:53:25,385 Command to send: c
o588
count
e

2024-06-18 13:53:25,471 Answer received: !yL113
2024-06-18 13:53:25,471 Command to send: c
o588
schema
e

2024-06-18 13:53:25,471 Answer received: !yro590
2024-06-18 13:53:25,471 Command to send: c
o590
json
e

2024-06-18 13:53:25,472 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:53:25,472 Command to send: c
o588
na
e

2024-06-18 13:53:25,473 Answer received: !yro591
2024-06-18 13:53:25,473 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,474 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,474 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,475 Answer received: !ym
2024-06-18 13:53:25,475 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,475 Answer received: !ylo592
2024-06-18 13:53:25,475 Command to send: c
o592
add
sdistrict
e

2024-06-18 13:53:25,476 Answer received: !ybtrue
2024-06-18 13:53:25,476 Command to send: c
o592
add
sdistrict_id
e

2024-06-18 13:53:25,476 Answer received: !ybtrue
2024-06-18 13:53:25,476 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro592
e

2024-06-18 13:53:25,477 Answer received: !yro593
2024-06-18 13:53:25,477 Command to send: c
o591
drop
i2
ro593
e

2024-06-18 13:53:25,479 Answer received: !yro594
2024-06-18 13:53:25,479 Command to send: c
o594
count
e

2024-06-18 13:53:25,564 Answer received: !yL113
2024-06-18 13:53:25,565 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,567 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,567 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,567 Answer received: !ym
2024-06-18 13:53:25,567 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:53:25,567 Answer received: !yro595
2024-06-18 13:53:25,568 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,569 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,569 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,569 Answer received: !ym
2024-06-18 13:53:25,569 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:53:25,570 Answer received: !yro596
2024-06-18 13:53:25,570 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,571 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,571 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,571 Answer received: !ym
2024-06-18 13:53:25,572 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:53:25,572 Answer received: !yro597
2024-06-18 13:53:25,572 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,574 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,574 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,574 Answer received: !ym
2024-06-18 13:53:25,574 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:53:25,575 Answer received: !yro598
2024-06-18 13:53:25,575 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,576 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,576 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,576 Answer received: !ym
2024-06-18 13:53:25,576 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:53:25,577 Answer received: !yro599
2024-06-18 13:53:25,577 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,577 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,578 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,578 Answer received: !ym
2024-06-18 13:53:25,578 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,578 Answer received: !ylo600
2024-06-18 13:53:25,578 Command to send: c
o600
add
ro595
e

2024-06-18 13:53:25,579 Answer received: !ybtrue
2024-06-18 13:53:25,579 Command to send: c
o600
add
ro596
e

2024-06-18 13:53:25,579 Answer received: !ybtrue
2024-06-18 13:53:25,579 Command to send: c
o600
add
ro597
e

2024-06-18 13:53:25,579 Answer received: !ybtrue
2024-06-18 13:53:25,580 Command to send: c
o600
add
ro598
e

2024-06-18 13:53:25,580 Answer received: !ybtrue
2024-06-18 13:53:25,580 Command to send: c
o600
add
ro599
e

2024-06-18 13:53:25,580 Answer received: !ybtrue
2024-06-18 13:53:25,580 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro600
e

2024-06-18 13:53:25,581 Answer received: !yro601
2024-06-18 13:53:25,581 Command to send: c
o556
select
ro601
e

2024-06-18 13:53:25,585 Answer received: !yro602
2024-06-18 13:53:25,585 Command to send: c
o602
schema
e

2024-06-18 13:53:25,585 Answer received: !yro603
2024-06-18 13:53:25,586 Command to send: c
o603
treeString
e

2024-06-18 13:53:25,586 Answer received: !ysroot\n |-- addresses_district_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- secondary_filters_name: string (nullable = true)\n |-- geo_epgs_4326_lat: string (nullable = true)\n |-- geo_epgs_4326_lon: string (nullable = true)\n
2024-06-18 13:53:25,586 Command to send: c
o602
count
e

2024-06-18 13:53:25,692 Answer received: !yL444
2024-06-18 13:53:25,692 Command to send: c
o602
schema
e

2024-06-18 13:53:25,693 Answer received: !yro604
2024-06-18 13:53:25,693 Command to send: c
o604
json
e

2024-06-18 13:53:25,694 Answer received: !ys{"type":"struct","fields":[{"name":"addresses_district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"name","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"secondary_filters_name","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"geo_epgs_4326_lat","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}},{"name":"geo_epgs_4326_lon","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240615-1328-","Description":"Information about hotels in neighbourhoods"}}]}
2024-06-18 13:53:25,694 Command to send: c
o602
na
e

2024-06-18 13:53:25,697 Answer received: !yro605
2024-06-18 13:53:25,697 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,699 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,699 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,700 Answer received: !ym
2024-06-18 13:53:25,700 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,701 Answer received: !ylo606
2024-06-18 13:53:25,701 Command to send: c
o606
add
saddresses_district_id
e

2024-06-18 13:53:25,701 Answer received: !ybtrue
2024-06-18 13:53:25,702 Command to send: c
o606
add
sname
e

2024-06-18 13:53:25,702 Answer received: !ybtrue
2024-06-18 13:53:25,702 Command to send: c
o606
add
ssecondary_filters_name
e

2024-06-18 13:53:25,702 Answer received: !ybtrue
2024-06-18 13:53:25,703 Command to send: c
o606
add
sgeo_epgs_4326_lat
e

2024-06-18 13:53:25,703 Answer received: !ybtrue
2024-06-18 13:53:25,703 Command to send: c
o606
add
sgeo_epgs_4326_lon
e

2024-06-18 13:53:25,704 Answer received: !ybtrue
2024-06-18 13:53:25,704 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro606
e

2024-06-18 13:53:25,704 Answer received: !yro607
2024-06-18 13:53:25,704 Command to send: c
o605
drop
i5
ro607
e

2024-06-18 13:53:25,709 Answer received: !yro608
2024-06-18 13:53:25,710 Command to send: c
o608
count
e

2024-06-18 13:53:25,787 Command to send: m
d
o533
e

2024-06-18 13:53:25,787 Answer received: !yv
2024-06-18 13:53:25,787 Command to send: m
d
o534
e

2024-06-18 13:53:25,787 Answer received: !yv
2024-06-18 13:53:25,788 Command to send: m
d
o535
e

2024-06-18 13:53:25,788 Answer received: !yv
2024-06-18 13:53:25,788 Command to send: m
d
o536
e

2024-06-18 13:53:25,788 Answer received: !yv
2024-06-18 13:53:25,789 Command to send: m
d
o537
e

2024-06-18 13:53:25,789 Answer received: !yv
2024-06-18 13:53:25,789 Command to send: m
d
o538
e

2024-06-18 13:53:25,789 Answer received: !yv
2024-06-18 13:53:25,790 Command to send: m
d
o539
e

2024-06-18 13:53:25,790 Answer received: !yv
2024-06-18 13:53:25,790 Command to send: m
d
o541
e

2024-06-18 13:53:25,790 Answer received: !yv
2024-06-18 13:53:25,791 Command to send: m
d
o542
e

2024-06-18 13:53:25,791 Answer received: !yv
2024-06-18 13:53:25,791 Command to send: m
d
o543
e

2024-06-18 13:53:25,791 Answer received: !yv
2024-06-18 13:53:25,791 Command to send: m
d
o545
e

2024-06-18 13:53:25,791 Answer received: !yv
2024-06-18 13:53:25,792 Command to send: m
d
o546
e

2024-06-18 13:53:25,792 Answer received: !yv
2024-06-18 13:53:25,792 Command to send: m
d
o547
e

2024-06-18 13:53:25,792 Answer received: !yv
2024-06-18 13:53:25,792 Command to send: m
d
o549
e

2024-06-18 13:53:25,793 Answer received: !yv
2024-06-18 13:53:25,793 Command to send: m
d
o550
e

2024-06-18 13:53:25,793 Answer received: !yv
2024-06-18 13:53:25,793 Command to send: m
d
o551
e

2024-06-18 13:53:25,793 Answer received: !yv
2024-06-18 13:53:25,793 Command to send: m
d
o553
e

2024-06-18 13:53:25,794 Answer received: !yv
2024-06-18 13:53:25,794 Command to send: m
d
o554
e

2024-06-18 13:53:25,794 Answer received: !yv
2024-06-18 13:53:25,794 Command to send: m
d
o555
e

2024-06-18 13:53:25,794 Answer received: !yv
2024-06-18 13:53:25,795 Command to send: m
d
o452
e

2024-06-18 13:53:25,795 Answer received: !yv
2024-06-18 13:53:25,795 Command to send: m
d
o471
e

2024-06-18 13:53:25,795 Answer received: !yv
2024-06-18 13:53:25,795 Command to send: m
d
o475
e

2024-06-18 13:53:25,795 Answer received: !yv
2024-06-18 13:53:25,796 Command to send: m
d
o467
e

2024-06-18 13:53:25,796 Answer received: !yv
2024-06-18 13:53:25,796 Command to send: m
d
o491
e

2024-06-18 13:53:25,796 Answer received: !yv
2024-06-18 13:53:25,796 Command to send: m
d
o506
e

2024-06-18 13:53:25,796 Answer received: !yv
2024-06-18 13:53:25,797 Command to send: m
d
o517
e

2024-06-18 13:53:25,797 Answer received: !yv
2024-06-18 13:53:25,797 Command to send: m
d
o525
e

2024-06-18 13:53:25,797 Answer received: !yv
2024-06-18 13:53:25,797 Command to send: m
d
o524
e

2024-06-18 13:53:25,798 Answer received: !yv
2024-06-18 13:53:25,798 Command to send: m
d
o560
e

2024-06-18 13:53:25,798 Answer received: !yv
2024-06-18 13:53:25,798 Command to send: m
d
o566
e

2024-06-18 13:53:25,798 Answer received: !yv
2024-06-18 13:53:25,798 Command to send: m
d
o575
e

2024-06-18 13:53:25,799 Answer received: !yv
2024-06-18 13:53:25,799 Command to send: m
d
o581
e

2024-06-18 13:53:25,799 Answer received: !yv
2024-06-18 13:53:25,799 Command to send: m
d
o586
e

2024-06-18 13:53:25,799 Answer received: !yv
2024-06-18 13:53:25,799 Command to send: m
d
o592
e

2024-06-18 13:53:25,800 Answer received: !yv
2024-06-18 13:53:25,800 Command to send: m
d
o600
e

2024-06-18 13:53:25,800 Answer received: !yv
2024-06-18 13:53:25,800 Command to send: m
d
o606
e

2024-06-18 13:53:25,800 Answer received: !yv
2024-06-18 13:53:25,828 Answer received: !yL443
2024-06-18 13:53:25,829 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,831 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,831 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,832 Answer received: !ym
2024-06-18 13:53:25,832 Command to send: c
z:org.apache.spark.sql.functions
col
ssecondary_filters_name
e

2024-06-18 13:53:25,832 Answer received: !yro609
2024-06-18 13:53:25,832 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,834 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,834 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:53:25,834 Answer received: !ym
2024-06-18 13:53:25,834 Command to send: c
z:org.apache.spark.sql.functions
split
ro609
s 
i-1
e

2024-06-18 13:53:25,835 Answer received: !yro610
2024-06-18 13:53:25,835 Command to send: c
o610
cast
sarray<int>
e

2024-06-18 13:53:25,835 Answer received: !yro611
2024-06-18 13:53:25,836 Command to send: c
o608
withColumn
sstars
ro611
e

2024-06-18 13:53:25,842 Answer received: !yro612
2024-06-18 13:53:25,842 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,843 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,844 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,844 Answer received: !ym
2024-06-18 13:53:25,844 Command to send: c
z:org.apache.spark.sql.functions
col
sstars
e

2024-06-18 13:53:25,844 Answer received: !yro613
2024-06-18 13:53:25,845 Command to send: c
o613
apply
i1
e

2024-06-18 13:53:25,845 Answer received: !yro614
2024-06-18 13:53:25,845 Command to send: c
o612
withColumn
sstars
ro614
e

2024-06-18 13:53:25,850 Answer received: !yro615
2024-06-18 13:53:25,851 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,852 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,852 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,852 Answer received: !ym
2024-06-18 13:53:25,852 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,852 Answer received: !ylo616
2024-06-18 13:53:25,853 Command to send: c
o616
add
ssecondary_filters_name
e

2024-06-18 13:53:25,853 Answer received: !ybtrue
2024-06-18 13:53:25,853 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro616
e

2024-06-18 13:53:25,853 Answer received: !yro617
2024-06-18 13:53:25,854 Command to send: c
o615
drop
ro617
e

2024-06-18 13:53:25,858 Answer received: !yro618
2024-06-18 13:53:25,858 Command to send: c
o618
apply
saddresses_district_id
e

2024-06-18 13:53:25,859 Answer received: !yro619
2024-06-18 13:53:25,859 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,860 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,861 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,862 Answer received: !ym
2024-06-18 13:53:25,862 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,862 Answer received: !yro620
2024-06-18 13:53:25,862 Command to send: c
o620
isDefined
e

2024-06-18 13:53:25,862 Answer received: !ybtrue
2024-06-18 13:53:25,862 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,864 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,864 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,865 Answer received: !ym
2024-06-18 13:53:25,865 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,865 Answer received: !yro621
2024-06-18 13:53:25,865 Command to send: c
o621
get
e

2024-06-18 13:53:25,865 Answer received: !yro622
2024-06-18 13:53:25,866 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:53:25,867 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:53:25,867 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:53:25,867 Answer received: !yro623
2024-06-18 13:53:25,868 Command to send: i
java.util.HashMap
e

2024-06-18 13:53:25,868 Answer received: !yao624
2024-06-18 13:53:25,868 Command to send: c
o623
applyModifiableSettings
ro622
ro624
e

2024-06-18 13:53:25,868 Answer received: !yv
2024-06-18 13:53:25,869 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:53:25,869 Answer received: !yro625
2024-06-18 13:53:25,869 Command to send: c
o619
cast
ro625
e

2024-06-18 13:53:25,869 Answer received: !yro626
2024-06-18 13:53:25,869 Command to send: c
o618
withColumn
saddresses_district_id
ro626
e

2024-06-18 13:53:25,874 Answer received: !yro627
2024-06-18 13:53:25,875 Command to send: c
o618
apply
sgeo_epgs_4326_lat
e

2024-06-18 13:53:25,875 Answer received: !yro628
2024-06-18 13:53:25,875 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,877 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,877 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,877 Answer received: !ym
2024-06-18 13:53:25,877 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,877 Answer received: !yro629
2024-06-18 13:53:25,877 Command to send: c
o629
isDefined
e

2024-06-18 13:53:25,878 Answer received: !ybtrue
2024-06-18 13:53:25,878 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,879 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,879 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,879 Answer received: !ym
2024-06-18 13:53:25,879 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,879 Answer received: !yro630
2024-06-18 13:53:25,879 Command to send: c
o630
get
e

2024-06-18 13:53:25,880 Answer received: !yro631
2024-06-18 13:53:25,880 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:53:25,881 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:53:25,882 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:53:25,882 Answer received: !yro632
2024-06-18 13:53:25,882 Command to send: i
java.util.HashMap
e

2024-06-18 13:53:25,882 Answer received: !yao633
2024-06-18 13:53:25,882 Command to send: c
o632
applyModifiableSettings
ro631
ro633
e

2024-06-18 13:53:25,883 Answer received: !yv
2024-06-18 13:53:25,883 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:53:25,883 Answer received: !yro634
2024-06-18 13:53:25,883 Command to send: c
o628
cast
ro634
e

2024-06-18 13:53:25,883 Answer received: !yro635
2024-06-18 13:53:25,884 Command to send: c
o627
withColumn
sgeo_epgs_4326_lat
ro635
e

2024-06-18 13:53:25,887 Answer received: !yro636
2024-06-18 13:53:25,888 Command to send: c
o618
apply
sgeo_epgs_4326_lon
e

2024-06-18 13:53:25,888 Answer received: !yro637
2024-06-18 13:53:25,888 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,890 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,890 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,890 Answer received: !ym
2024-06-18 13:53:25,890 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,891 Answer received: !yro638
2024-06-18 13:53:25,891 Command to send: c
o638
isDefined
e

2024-06-18 13:53:25,891 Answer received: !ybtrue
2024-06-18 13:53:25,891 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,892 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,892 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,893 Answer received: !ym
2024-06-18 13:53:25,893 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,893 Answer received: !yro639
2024-06-18 13:53:25,893 Command to send: c
o639
get
e

2024-06-18 13:53:25,893 Answer received: !yro640
2024-06-18 13:53:25,893 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:53:25,894 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:53:25,895 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:53:25,895 Answer received: !yro641
2024-06-18 13:53:25,895 Command to send: i
java.util.HashMap
e

2024-06-18 13:53:25,895 Answer received: !yao642
2024-06-18 13:53:25,895 Command to send: c
o641
applyModifiableSettings
ro640
ro642
e

2024-06-18 13:53:25,896 Answer received: !yv
2024-06-18 13:53:25,896 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:53:25,896 Answer received: !yro643
2024-06-18 13:53:25,896 Command to send: c
o637
cast
ro643
e

2024-06-18 13:53:25,897 Answer received: !yro644
2024-06-18 13:53:25,897 Command to send: c
o636
withColumn
sgeo_epgs_4326_lon
ro644
e

2024-06-18 13:53:25,901 Answer received: !yro645
2024-06-18 13:53:25,902 Command to send: c
o583
apply
sfloor
e

2024-06-18 13:53:25,902 Answer received: !yro646
2024-06-18 13:53:25,902 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,904 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,904 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,904 Answer received: !ym
2024-06-18 13:53:25,904 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,904 Answer received: !yro647
2024-06-18 13:53:25,905 Command to send: c
o647
isDefined
e

2024-06-18 13:53:25,905 Answer received: !ybtrue
2024-06-18 13:53:25,905 Command to send: r
u
SparkSession
rj
e

2024-06-18 13:53:25,907 Answer received: !ycorg.apache.spark.sql.SparkSession
2024-06-18 13:53:25,907 Command to send: r
m
org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,907 Answer received: !ym
2024-06-18 13:53:25,907 Command to send: c
z:org.apache.spark.sql.SparkSession
getActiveSession
e

2024-06-18 13:53:25,907 Answer received: !yro648
2024-06-18 13:53:25,908 Command to send: c
o648
get
e

2024-06-18 13:53:25,908 Answer received: !yro649
2024-06-18 13:53:25,908 Command to send: r
u
SparkSession$
rj
e

2024-06-18 13:53:25,909 Answer received: !ycorg.apache.spark.sql.SparkSession$
2024-06-18 13:53:25,909 Command to send: r
m
org.apache.spark.sql.SparkSession$
MODULE$
e

2024-06-18 13:53:25,909 Answer received: !yro650
2024-06-18 13:53:25,910 Command to send: i
java.util.HashMap
e

2024-06-18 13:53:25,910 Answer received: !yao651
2024-06-18 13:53:25,910 Command to send: c
o650
applyModifiableSettings
ro649
ro651
e

2024-06-18 13:53:25,910 Answer received: !yv
2024-06-18 13:53:25,910 Command to send: c
o25
parseDataType
s"integer"
e

2024-06-18 13:53:25,911 Answer received: !yro652
2024-06-18 13:53:25,911 Command to send: c
o646
cast
ro652
e

2024-06-18 13:53:25,911 Answer received: !yro653
2024-06-18 13:53:25,911 Command to send: c
o583
withColumn
sfloor
ro653
e

2024-06-18 13:53:25,916 Answer received: !yro654
2024-06-18 13:53:25,916 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,918 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,918 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,918 Answer received: !ym
2024-06-18 13:53:25,918 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:53:25,919 Answer received: !yro655
2024-06-18 13:53:25,919 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,920 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,920 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:53:25,920 Answer received: !ym
2024-06-18 13:53:25,920 Command to send: c
z:org.apache.spark.sql.functions
split
ro655
s"
i-1
e

2024-06-18 13:53:25,921 Answer received: !yro656
2024-06-18 13:53:25,921 Command to send: c
o568
withColumn
sNom_Districte
ro656
e

2024-06-18 13:53:25,925 Answer received: !yro657
2024-06-18 13:53:25,925 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,927 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,927 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,927 Answer received: !ym
2024-06-18 13:53:25,927 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:53:25,927 Answer received: !yro658
2024-06-18 13:53:25,928 Command to send: c
o658
apply
i1
e

2024-06-18 13:53:25,928 Answer received: !yro659
2024-06-18 13:53:25,928 Command to send: c
o657
withColumn
sNom_Districte
ro659
e

2024-06-18 13:53:25,932 Answer received: !yro660
2024-06-18 13:53:25,932 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,933 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,934 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,934 Answer received: !ym
2024-06-18 13:53:25,934 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:53:25,935 Answer received: !yro661
2024-06-18 13:53:25,935 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,936 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,936 Command to send: r
m
org.apache.spark.sql.functions
split
e

2024-06-18 13:53:25,936 Answer received: !ym
2024-06-18 13:53:25,937 Command to send: c
z:org.apache.spark.sql.functions
split
ro661
s"
i-1
e

2024-06-18 13:53:25,937 Answer received: !yro662
2024-06-18 13:53:25,937 Command to send: c
o660
withColumn
sndex RFD Barcelona = 100
ro662
e

2024-06-18 13:53:25,941 Answer received: !yro663
2024-06-18 13:53:25,941 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,943 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,943 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,943 Answer received: !ym
2024-06-18 13:53:25,944 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:53:25,944 Answer received: !yro664
2024-06-18 13:53:25,944 Command to send: c
o664
apply
i1
e

2024-06-18 13:53:25,944 Answer received: !yro665
2024-06-18 13:53:25,945 Command to send: c
o663
withColumn
sndex RFD Barcelona = 100
ro665
e

2024-06-18 13:53:25,948 Answer received: !yro666
2024-06-18 13:53:25,949 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,950 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,951 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,951 Answer received: !ym
2024-06-18 13:53:25,951 Command to send: c
z:org.apache.spark.sql.functions
col
saddresses_district_id
e

2024-06-18 13:53:25,951 Answer received: !yro667
2024-06-18 13:53:25,951 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,952 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,952 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,952 Answer received: !ym
2024-06-18 13:53:25,953 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,953 Answer received: !ylo668
2024-06-18 13:53:25,953 Command to send: c
o668
add
ro667
e

2024-06-18 13:53:25,953 Answer received: !ybtrue
2024-06-18 13:53:25,953 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro668
e

2024-06-18 13:53:25,954 Answer received: !yro669
2024-06-18 13:53:25,954 Command to send: c
o645
groupBy
ro669
e

2024-06-18 13:53:25,955 Answer received: !yro670
2024-06-18 13:53:25,955 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,957 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,957 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,957 Answer received: !ym
2024-06-18 13:53:25,958 Command to send: c
z:org.apache.spark.sql.functions
col
sstars
e

2024-06-18 13:53:25,958 Answer received: !yro671
2024-06-18 13:53:25,958 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,959 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,959 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:53:25,960 Answer received: !ym
2024-06-18 13:53:25,960 Command to send: c
z:org.apache.spark.sql.functions
avg
ro671
e

2024-06-18 13:53:25,960 Answer received: !yro672
2024-06-18 13:53:25,961 Command to send: c
o672
as
sAvg_stars
e

2024-06-18 13:53:25,961 Answer received: !yro673
2024-06-18 13:53:25,961 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,962 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,962 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,963 Answer received: !ym
2024-06-18 13:53:25,963 Command to send: c
z:org.apache.spark.sql.functions
col
sname
e

2024-06-18 13:53:25,963 Answer received: !yro674
2024-06-18 13:53:25,963 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,964 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,965 Command to send: r
m
org.apache.spark.sql.functions
count
e

2024-06-18 13:53:25,965 Answer received: !ym
2024-06-18 13:53:25,965 Command to send: c
z:org.apache.spark.sql.functions
count
ro674
e

2024-06-18 13:53:25,965 Answer received: !yro675
2024-06-18 13:53:25,965 Command to send: c
o675
as
sN_hotels
e

2024-06-18 13:53:25,966 Answer received: !yro676
2024-06-18 13:53:25,966 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,967 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,967 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,967 Answer received: !ym
2024-06-18 13:53:25,968 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lat
e

2024-06-18 13:53:25,968 Answer received: !yro677
2024-06-18 13:53:25,968 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,969 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,969 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:53:25,969 Answer received: !ym
2024-06-18 13:53:25,969 Command to send: c
z:org.apache.spark.sql.functions
avg
ro677
e

2024-06-18 13:53:25,970 Answer received: !yro678
2024-06-18 13:53:25,970 Command to send: c
o678
as
sAvg_lat
e

2024-06-18 13:53:25,970 Answer received: !yro679
2024-06-18 13:53:25,970 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,971 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,971 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,972 Answer received: !ym
2024-06-18 13:53:25,972 Command to send: c
z:org.apache.spark.sql.functions
col
sgeo_epgs_4326_lon
e

2024-06-18 13:53:25,972 Answer received: !yro680
2024-06-18 13:53:25,972 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,974 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,974 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:53:25,974 Answer received: !ym
2024-06-18 13:53:25,974 Command to send: c
z:org.apache.spark.sql.functions
avg
ro680
e

2024-06-18 13:53:25,975 Answer received: !yro681
2024-06-18 13:53:25,975 Command to send: c
o681
as
sAvg_long
e

2024-06-18 13:53:25,975 Answer received: !yro682
2024-06-18 13:53:25,975 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,976 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,976 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,976 Answer received: !ym
2024-06-18 13:53:25,976 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,977 Answer received: !ylo683
2024-06-18 13:53:25,977 Command to send: c
o683
add
ro676
e

2024-06-18 13:53:25,977 Answer received: !ybtrue
2024-06-18 13:53:25,977 Command to send: c
o683
add
ro679
e

2024-06-18 13:53:25,977 Answer received: !ybtrue
2024-06-18 13:53:25,978 Command to send: c
o683
add
ro682
e

2024-06-18 13:53:25,978 Answer received: !ybtrue
2024-06-18 13:53:25,978 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro683
e

2024-06-18 13:53:25,978 Answer received: !yro684
2024-06-18 13:53:25,978 Command to send: c
o670
agg
ro673
ro684
e

2024-06-18 13:53:25,984 Answer received: !yro685
2024-06-18 13:53:25,985 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,986 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,986 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,986 Answer received: !ym
2024-06-18 13:53:25,986 Command to send: c
z:org.apache.spark.sql.functions
col
sNom_Districte
e

2024-06-18 13:53:25,987 Answer received: !yro686
2024-06-18 13:53:25,987 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,988 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,988 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,988 Answer received: !ym
2024-06-18 13:53:25,989 Command to send: c
z:org.apache.spark.sql.functions
col
sCodi_Districte
e

2024-06-18 13:53:25,989 Answer received: !yro687
2024-06-18 13:53:25,989 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,990 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,990 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,990 Answer received: !ym
2024-06-18 13:53:25,990 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:25,991 Answer received: !ylo688
2024-06-18 13:53:25,991 Command to send: c
o688
add
ro686
e

2024-06-18 13:53:25,991 Answer received: !ybtrue
2024-06-18 13:53:25,991 Command to send: c
o688
add
ro687
e

2024-06-18 13:53:25,992 Answer received: !ybtrue
2024-06-18 13:53:25,992 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro688
e

2024-06-18 13:53:25,992 Answer received: !yro689
2024-06-18 13:53:25,992 Command to send: c
o666
groupBy
ro689
e

2024-06-18 13:53:25,993 Answer received: !yro690
2024-06-18 13:53:25,993 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,995 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,995 Command to send: r
m
org.apache.spark.sql.functions
col
e

2024-06-18 13:53:25,995 Answer received: !ym
2024-06-18 13:53:25,995 Command to send: c
z:org.apache.spark.sql.functions
col
sndex RFD Barcelona = 100
e

2024-06-18 13:53:25,995 Answer received: !yro691
2024-06-18 13:53:25,995 Command to send: r
u
functions
rj
e

2024-06-18 13:53:25,996 Answer received: !ycorg.apache.spark.sql.functions
2024-06-18 13:53:25,996 Command to send: r
m
org.apache.spark.sql.functions
avg
e

2024-06-18 13:53:25,997 Answer received: !ym
2024-06-18 13:53:25,997 Command to send: c
z:org.apache.spark.sql.functions
avg
ro691
e

2024-06-18 13:53:25,997 Answer received: !yro692
2024-06-18 13:53:25,997 Command to send: c
o692
as
sAvg_Index_RFD
e

2024-06-18 13:53:25,998 Answer received: !yro693
2024-06-18 13:53:25,998 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:25,999 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:25,999 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:25,999 Answer received: !ym
2024-06-18 13:53:25,999 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:26,000 Answer received: !ylo694
2024-06-18 13:53:26,000 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro694
e

2024-06-18 13:53:26,000 Answer received: !yro695
2024-06-18 13:53:26,000 Command to send: c
o690
agg
ro693
ro695
e

2024-06-18 13:53:26,005 Answer received: !yro696
2024-06-18 13:53:26,007 Command to send: c
o685
schema
e

2024-06-18 13:53:26,007 Answer received: !yro697
2024-06-18 13:53:26,007 Command to send: c
o697
json
e

2024-06-18 13:53:26,007 Answer received: !ys{"type":"struct","fields":[{"name":"addresses_district_id","type":"integer","nullable":true,"metadata":{}},{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:53:26,007 Command to send: c
o685
apply
saddresses_district_id
e

2024-06-18 13:53:26,008 Answer received: !yro698
2024-06-18 13:53:26,008 Command to send: c
o696
schema
e

2024-06-18 13:53:26,008 Answer received: !yro699
2024-06-18 13:53:26,008 Command to send: c
o699
json
e

2024-06-18 13:53:26,009 Answer received: !ys{"type":"struct","fields":[{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Codi_Districte","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2304-","Description":"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file","keywords":["renda_familiar"]}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:53:26,009 Command to send: c
o696
apply
sCodi_Districte
e

2024-06-18 13:53:26,009 Answer received: !yro700
2024-06-18 13:53:26,009 Command to send: c
o698
equalTo
ro700
e

2024-06-18 13:53:26,010 Answer received: !yro701
2024-06-18 13:53:26,010 Command to send: c
o685
join
ro696
ro701
sinner
e

2024-06-18 13:53:26,019 Answer received: !yro702
2024-06-18 13:53:26,019 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:26,020 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:26,020 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:26,021 Answer received: !ym
2024-06-18 13:53:26,021 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:26,021 Answer received: !ylo703
2024-06-18 13:53:26,022 Command to send: c
o703
add
sCodi_Districte
e

2024-06-18 13:53:26,022 Answer received: !ybtrue
2024-06-18 13:53:26,022 Command to send: c
o703
add
saddresses_district_id
e

2024-06-18 13:53:26,023 Answer received: !ybtrue
2024-06-18 13:53:26,023 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro703
e

2024-06-18 13:53:26,023 Answer received: !yro704
2024-06-18 13:53:26,023 Command to send: c
o702
drop
ro704
e

2024-06-18 13:53:26,027 Answer received: !yro705
2024-06-18 13:53:26,027 Command to send: c
o705
schema
e

2024-06-18 13:53:26,027 Answer received: !yro706
2024-06-18 13:53:26,028 Command to send: c
o706
json
e

2024-06-18 13:53:26,028 Answer received: !ys{"type":"struct","fields":[{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}},{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}}]}
2024-06-18 13:53:26,028 Command to send: c
o705
apply
sNom_Districte
e

2024-06-18 13:53:26,029 Answer received: !yro707
2024-06-18 13:53:26,029 Command to send: c
o594
schema
e

2024-06-18 13:53:26,030 Answer received: !yro708
2024-06-18 13:53:26,030 Command to send: c
o708
json
e

2024-06-18 13:53:26,030 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}},{"name":"district_id","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:53:26,031 Command to send: c
o594
apply
sdistrict
e

2024-06-18 13:53:26,031 Answer received: !yro709
2024-06-18 13:53:26,032 Command to send: c
o707
equalTo
ro709
e

2024-06-18 13:53:26,032 Answer received: !yro710
2024-06-18 13:53:26,032 Command to send: c
o705
join
ro594
ro710
sinner
e

2024-06-18 13:53:26,037 Answer received: !yro711
2024-06-18 13:53:26,037 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:26,038 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:26,039 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:26,039 Answer received: !ym
2024-06-18 13:53:26,039 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:26,039 Answer received: !ylo712
2024-06-18 13:53:26,040 Command to send: c
o712
add
sdistrict_id
e

2024-06-18 13:53:26,040 Answer received: !ybtrue
2024-06-18 13:53:26,040 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro712
e

2024-06-18 13:53:26,040 Answer received: !yro713
2024-06-18 13:53:26,040 Command to send: c
o711
drop
ro713
e

2024-06-18 13:53:26,044 Answer received: !yro714
2024-06-18 13:53:26,045 Command to send: c
o714
schema
e

2024-06-18 13:53:26,045 Answer received: !yro715
2024-06-18 13:53:26,045 Command to send: c
o715
json
e

2024-06-18 13:53:26,046 Answer received: !ys{"type":"struct","fields":[{"name":"Avg_stars","type":"double","nullable":true,"metadata":{}},{"name":"N_hotels","type":"long","nullable":false,"metadata":{}},{"name":"Avg_lat","type":"double","nullable":true,"metadata":{}},{"name":"Avg_long","type":"double","nullable":true,"metadata":{}},{"name":"Nom_Districte","type":"string","nullable":true,"metadata":{}},{"name":"Avg_Index_RFD","type":"double","nullable":true,"metadata":{}},{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2308-","Description":"Lookup datable to join 'Idealista' and 'renda familiar'","keywords":["extended"]}}]}
2024-06-18 13:53:26,047 Command to send: c
o714
apply
sdistrict
e

2024-06-18 13:53:26,047 Answer received: !yro716
2024-06-18 13:53:26,047 Command to send: c
o654
schema
e

2024-06-18 13:53:26,048 Answer received: !yro717
2024-06-18 13:53:26,048 Command to send: c
o717
json
e

2024-06-18 13:53:26,048 Answer received: !ys{"type":"struct","fields":[{"name":"district","type":"string","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"numPhotos","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"price","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"floor","type":"integer","nullable":true,"metadata":{}},{"name":"size","type":"double","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}},{"name":"bathrooms","type":"long","nullable":true,"metadata":{"origin_file_names":[],"mod_time":"20240614-2306-","Description":"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file","keywords":["idealista"]}}]}
2024-06-18 13:53:26,049 Command to send: c
o654
apply
sdistrict
e

2024-06-18 13:53:26,049 Answer received: !yro718
2024-06-18 13:53:26,049 Command to send: c
o716
equalTo
ro718
e

2024-06-18 13:53:26,050 Answer received: !yro719
2024-06-18 13:53:26,050 Command to send: c
o714
join
ro654
ro719
sinner
e

2024-06-18 13:53:26,055 Answer received: !yro720
2024-06-18 13:53:26,055 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:26,056 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:26,057 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:26,057 Answer received: !ym
2024-06-18 13:53:26,057 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:26,057 Answer received: !ylo721
2024-06-18 13:53:26,058 Command to send: c
o721
add
sdistrict
e

2024-06-18 13:53:26,058 Answer received: !ybtrue
2024-06-18 13:53:26,058 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro721
e

2024-06-18 13:53:26,058 Answer received: !yro722
2024-06-18 13:53:26,058 Command to send: c
o720
drop
ro722
e

2024-06-18 13:53:26,064 Answer received: !yro723
2024-06-18 13:53:26,064 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:53:26,065 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:53:26,066 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:53:26,066 Answer received: !ym
2024-06-18 13:53:26,066 Command to send: i
java.util.ArrayList
e

2024-06-18 13:53:26,066 Answer received: !ylo724
2024-06-18 13:53:26,066 Command to send: c
o724
add
sNom_districte
e

2024-06-18 13:53:26,067 Answer received: !ybtrue
2024-06-18 13:53:26,067 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro724
e

2024-06-18 13:53:26,067 Answer received: !yro725
2024-06-18 13:53:26,067 Command to send: c
o723
drop
ro725
e

2024-06-18 13:53:26,071 Answer received: !yro726
2024-06-18 13:53:26,801 Command to send: m
d
o616
e

2024-06-18 13:53:26,801 Answer received: !yv
2024-06-18 13:53:26,801 Command to send: m
d
o624
e

2024-06-18 13:53:26,801 Answer received: !yv
2024-06-18 13:53:26,801 Command to send: m
d
o633
e

2024-06-18 13:53:26,802 Answer received: !yv
2024-06-18 13:53:26,802 Command to send: m
d
o642
e

2024-06-18 13:53:26,802 Answer received: !yv
2024-06-18 13:53:26,802 Command to send: m
d
o651
e

2024-06-18 13:53:26,803 Answer received: !yv
2024-06-18 13:53:26,803 Command to send: m
d
o668
e

2024-06-18 13:53:26,803 Answer received: !yv
2024-06-18 13:53:26,803 Command to send: m
d
o557
e

2024-06-18 13:53:26,803 Answer received: !yv
2024-06-18 13:53:26,803 Command to send: m
d
o558
e

2024-06-18 13:53:26,804 Answer received: !yv
2024-06-18 13:53:26,804 Command to send: m
d
o559
e

2024-06-18 13:53:26,804 Answer received: !yv
2024-06-18 13:53:26,804 Command to send: m
d
o561
e

2024-06-18 13:53:26,804 Answer received: !yv
2024-06-18 13:53:26,805 Command to send: m
d
o562
e

2024-06-18 13:53:26,805 Answer received: !yv
2024-06-18 13:53:26,805 Command to send: m
d
o563
e

2024-06-18 13:53:26,805 Answer received: !yv
2024-06-18 13:53:26,805 Command to send: m
d
o564
e

2024-06-18 13:53:26,806 Answer received: !yv
2024-06-18 13:53:26,806 Command to send: m
d
o565
e

2024-06-18 13:53:26,806 Answer received: !yv
2024-06-18 13:53:26,806 Command to send: m
d
o567
e

2024-06-18 13:53:26,806 Answer received: !yv
2024-06-18 13:53:26,807 Command to send: m
d
o568
e

2024-06-18 13:53:26,807 Answer received: !yv
2024-06-18 13:53:26,807 Command to send: m
d
o569
e

2024-06-18 13:53:26,807 Answer received: !yv
2024-06-18 13:53:26,807 Command to send: m
d
o570
e

2024-06-18 13:53:26,808 Answer received: !yv
2024-06-18 13:53:26,808 Command to send: m
d
o571
e

2024-06-18 13:53:26,808 Answer received: !yv
2024-06-18 13:53:26,808 Command to send: m
d
o572
e

2024-06-18 13:53:26,808 Answer received: !yv
2024-06-18 13:53:26,809 Command to send: m
d
o573
e

2024-06-18 13:53:26,809 Answer received: !yv
2024-06-18 13:53:26,809 Command to send: m
d
o574
e

2024-06-18 13:53:26,809 Answer received: !yv
2024-06-18 13:53:26,809 Command to send: m
d
o576
e

2024-06-18 13:53:26,810 Answer received: !yv
2024-06-18 13:53:26,810 Command to send: m
d
o577
e

2024-06-18 13:53:26,810 Answer received: !yv
2024-06-18 13:53:26,810 Command to send: m
d
o578
e

2024-06-18 13:53:26,810 Answer received: !yv
2024-06-18 13:53:26,810 Command to send: m
d
o579
e

2024-06-18 13:53:26,811 Answer received: !yv
2024-06-18 13:53:26,811 Command to send: m
d
o580
e

2024-06-18 13:53:26,811 Answer received: !yv
2024-06-18 13:53:26,811 Command to send: m
d
o582
e

2024-06-18 13:53:26,812 Answer received: !yv
2024-06-18 13:53:26,812 Command to send: m
d
o583
e

2024-06-18 13:53:26,812 Answer received: !yv
2024-06-18 13:53:26,812 Command to send: m
d
o584
e

2024-06-18 13:53:26,812 Answer received: !yv
2024-06-18 13:53:26,812 Command to send: m
d
o585
e

2024-06-18 13:53:26,813 Answer received: !yv
2024-06-18 13:53:26,813 Command to send: m
d
o587
e

2024-06-18 13:53:26,813 Answer received: !yv
2024-06-18 13:53:26,813 Command to send: m
d
o588
e

2024-06-18 13:53:26,813 Answer received: !yv
2024-06-18 13:53:26,813 Command to send: m
d
o589
e

2024-06-18 13:53:26,814 Answer received: !yv
2024-06-18 13:53:26,814 Command to send: m
d
o590
e

2024-06-18 13:53:26,814 Answer received: !yv
2024-06-18 13:53:26,814 Command to send: m
d
o591
e

2024-06-18 13:53:26,814 Answer received: !yv
2024-06-18 13:53:26,814 Command to send: m
d
o593
e

2024-06-18 13:53:26,815 Answer received: !yv
2024-06-18 13:53:26,815 Command to send: m
d
o595
e

2024-06-18 13:53:26,815 Answer received: !yv
2024-06-18 13:53:26,815 Command to send: m
d
o596
e

2024-06-18 13:53:26,815 Answer received: !yv
2024-06-18 13:53:26,815 Command to send: m
d
o597
e

2024-06-18 13:53:26,816 Answer received: !yv
2024-06-18 13:53:26,816 Command to send: m
d
o598
e

2024-06-18 13:53:26,816 Answer received: !yv
2024-06-18 13:53:26,816 Command to send: m
d
o599
e

2024-06-18 13:53:26,817 Answer received: !yv
2024-06-18 13:53:26,817 Command to send: m
d
o601
e

2024-06-18 13:53:26,817 Answer received: !yv
2024-06-18 13:53:26,817 Command to send: m
d
o602
e

2024-06-18 13:53:26,817 Answer received: !yv
2024-06-18 13:53:26,818 Command to send: m
d
o603
e

2024-06-18 13:53:26,818 Answer received: !yv
2024-06-18 13:53:26,818 Command to send: m
d
o604
e

2024-06-18 13:53:26,818 Answer received: !yv
2024-06-18 13:53:26,818 Command to send: m
d
o605
e

2024-06-18 13:53:26,818 Answer received: !yv
2024-06-18 13:53:26,819 Command to send: m
d
o607
e

2024-06-18 13:53:26,819 Answer received: !yv
2024-06-18 13:53:26,819 Command to send: m
d
o609
e

2024-06-18 13:53:26,819 Answer received: !yv
2024-06-18 13:53:26,819 Command to send: m
d
o610
e

2024-06-18 13:53:26,820 Answer received: !yv
2024-06-18 13:53:26,820 Command to send: m
d
o611
e

2024-06-18 13:53:26,820 Answer received: !yv
2024-06-18 13:53:26,820 Command to send: m
d
o612
e

2024-06-18 13:53:26,820 Answer received: !yv
2024-06-18 13:53:26,820 Command to send: m
d
o613
e

2024-06-18 13:53:26,820 Answer received: !yv
2024-06-18 13:53:26,821 Command to send: m
d
o614
e

2024-06-18 13:53:26,821 Answer received: !yv
2024-06-18 13:53:26,821 Command to send: m
d
o615
e

2024-06-18 13:53:26,821 Answer received: !yv
2024-06-18 13:53:26,821 Command to send: m
d
o617
e

2024-06-18 13:53:26,821 Answer received: !yv
2024-06-18 13:53:26,822 Command to send: m
d
o618
e

2024-06-18 13:53:26,822 Answer received: !yv
2024-06-18 13:53:26,822 Command to send: m
d
o619
e

2024-06-18 13:53:26,822 Answer received: !yv
2024-06-18 13:53:26,822 Command to send: m
d
o620
e

2024-06-18 13:53:26,822 Answer received: !yv
2024-06-18 13:53:26,823 Command to send: m
d
o621
e

2024-06-18 13:53:26,823 Answer received: !yv
2024-06-18 13:53:26,823 Command to send: m
d
o622
e

2024-06-18 13:53:26,823 Answer received: !yv
2024-06-18 13:53:26,823 Command to send: m
d
o623
e

2024-06-18 13:53:26,823 Answer received: !yv
2024-06-18 13:53:26,824 Command to send: m
d
o625
e

2024-06-18 13:53:26,824 Answer received: !yv
2024-06-18 13:53:26,824 Command to send: m
d
o626
e

2024-06-18 13:53:26,824 Answer received: !yv
2024-06-18 13:53:26,824 Command to send: m
d
o627
e

2024-06-18 13:53:26,824 Answer received: !yv
2024-06-18 13:53:26,825 Command to send: m
d
o628
e

2024-06-18 13:53:26,825 Answer received: !yv
2024-06-18 13:53:26,825 Command to send: m
d
o629
e

2024-06-18 13:53:26,825 Answer received: !yv
2024-06-18 13:53:26,825 Command to send: m
d
o630
e

2024-06-18 13:53:26,825 Answer received: !yv
2024-06-18 13:53:26,825 Command to send: m
d
o631
e

2024-06-18 13:53:26,826 Answer received: !yv
2024-06-18 13:53:26,826 Command to send: m
d
o632
e

2024-06-18 13:53:26,826 Answer received: !yv
2024-06-18 13:53:26,826 Command to send: m
d
o634
e

2024-06-18 13:53:26,826 Answer received: !yv
2024-06-18 13:53:26,826 Command to send: m
d
o635
e

2024-06-18 13:53:26,826 Answer received: !yv
2024-06-18 13:53:26,826 Command to send: m
d
o636
e

2024-06-18 13:53:26,827 Answer received: !yv
2024-06-18 13:53:26,827 Command to send: m
d
o637
e

2024-06-18 13:53:26,827 Answer received: !yv
2024-06-18 13:53:26,827 Command to send: m
d
o638
e

2024-06-18 13:53:26,827 Answer received: !yv
2024-06-18 13:53:26,827 Command to send: m
d
o639
e

2024-06-18 13:53:26,828 Answer received: !yv
2024-06-18 13:53:26,828 Command to send: m
d
o640
e

2024-06-18 13:53:26,828 Answer received: !yv
2024-06-18 13:53:26,828 Command to send: m
d
o641
e

2024-06-18 13:53:26,828 Answer received: !yv
2024-06-18 13:53:26,828 Command to send: m
d
o643
e

2024-06-18 13:53:26,829 Answer received: !yv
2024-06-18 13:53:26,829 Command to send: m
d
o644
e

2024-06-18 13:53:26,829 Answer received: !yv
2024-06-18 13:53:26,829 Command to send: m
d
o646
e

2024-06-18 13:53:26,829 Answer received: !yv
2024-06-18 13:53:26,829 Command to send: m
d
o647
e

2024-06-18 13:53:26,829 Answer received: !yv
2024-06-18 13:53:26,830 Command to send: m
d
o648
e

2024-06-18 13:53:26,830 Answer received: !yv
2024-06-18 13:53:26,830 Command to send: m
d
o650
e

2024-06-18 13:53:26,830 Answer received: !yv
2024-06-18 13:53:26,830 Command to send: m
d
o652
e

2024-06-18 13:53:26,830 Answer received: !yv
2024-06-18 13:53:26,831 Command to send: m
d
o653
e

2024-06-18 13:53:26,831 Answer received: !yv
2024-06-18 13:53:26,831 Command to send: m
d
o655
e

2024-06-18 13:53:26,831 Answer received: !yv
2024-06-18 13:53:26,831 Command to send: m
d
o656
e

2024-06-18 13:53:26,831 Answer received: !yv
2024-06-18 13:53:26,831 Command to send: m
d
o657
e

2024-06-18 13:53:26,832 Answer received: !yv
2024-06-18 13:53:26,832 Command to send: m
d
o658
e

2024-06-18 13:53:26,832 Answer received: !yv
2024-06-18 13:53:26,832 Command to send: m
d
o659
e

2024-06-18 13:53:26,832 Answer received: !yv
2024-06-18 13:53:26,832 Command to send: m
d
o660
e

2024-06-18 13:53:26,833 Answer received: !yv
2024-06-18 13:53:26,833 Command to send: m
d
o661
e

2024-06-18 13:53:26,833 Answer received: !yv
2024-06-18 13:53:26,833 Command to send: m
d
o662
e

2024-06-18 13:53:26,833 Answer received: !yv
2024-06-18 13:53:26,833 Command to send: m
d
o663
e

2024-06-18 13:53:26,834 Answer received: !yv
2024-06-18 13:53:26,834 Command to send: m
d
o664
e

2024-06-18 13:53:26,834 Answer received: !yv
2024-06-18 13:53:26,834 Command to send: m
d
o665
e

2024-06-18 13:53:26,834 Answer received: !yv
2024-06-18 13:53:26,834 Command to send: m
d
o667
e

2024-06-18 13:53:26,834 Answer received: !yv
2024-06-18 13:53:26,835 Command to send: m
d
o669
e

2024-06-18 13:53:26,835 Answer received: !yv
2024-06-18 13:53:26,835 Command to send: m
d
o671
e

2024-06-18 13:53:26,835 Answer received: !yv
2024-06-18 13:53:26,835 Command to send: m
d
o683
e

2024-06-18 13:53:26,835 Answer received: !yv
2024-06-18 13:53:26,835 Command to send: m
d
o688
e

2024-06-18 13:53:26,836 Answer received: !yv
2024-06-18 13:53:26,836 Command to send: m
d
o694
e

2024-06-18 13:53:26,836 Answer received: !yv
2024-06-18 13:53:26,836 Command to send: m
d
o703
e

2024-06-18 13:53:26,836 Answer received: !yv
2024-06-18 13:53:26,836 Command to send: m
d
o712
e

2024-06-18 13:53:26,837 Answer received: !yv
2024-06-18 13:53:26,837 Command to send: m
d
o721
e

2024-06-18 13:53:26,837 Answer received: !yv
2024-06-18 13:53:26,837 Command to send: m
d
o724
e

2024-06-18 13:53:26,837 Answer received: !yv
2024-06-18 13:53:36,600 Command to send: c
o726
write
e

2024-06-18 13:53:36,602 Answer received: !yro727
2024-06-18 13:53:36,602 Command to send: c
o727
csv
stest.cvs
e

2024-06-18 13:53:37,820 Answer received: !yv
2024-06-18 13:54:11,194 Command to send: c
o726
write
e

2024-06-18 13:54:11,196 Answer received: !yro728
2024-06-18 13:54:11,197 Command to send: c
o728
parquet
smodel_name
e

2024-06-18 13:54:12,073 Answer received: !yv
2024-06-18 13:54:34,677 Command to send: c
o726
write
e

2024-06-18 13:54:34,680 Answer received: !yro729
2024-06-18 13:54:34,680 Command to send: c
o729
parquet
sGLM_V1.parquet
e

2024-06-18 13:54:35,380 Answer received: !yv
2024-06-18 13:55:22,569 Instantiated <InsecureClient(url='http://10.4.41.35:9870/')>.
2024-06-18 13:55:22,569 Command to send: c
o726
write
e

2024-06-18 13:55:22,571 Answer received: !yro730
2024-06-18 13:55:22,572 Command to send: c
o730
parquet
sGLM_V1.parquet
e

2024-06-18 13:55:23,237 Answer received: !yv
2024-06-18 13:55:23,238 Uploading 'GLM_V1.parquet' to 'user/bdm/Model_explotation_zone/GLM_V1.parquet'.
2024-06-18 13:55:23,238 Resolved path '/' to '/'.
2024-06-18 13:55:23,241 Starting new HTTP connection (1): 10.4.41.35:9870
2024-06-18 13:55:23,256 http://10.4.41.35:9870 "GET /webhdfs/v1/?user.name=bdm&op=GETHOMEDIRECTORY HTTP/1.1" 200 None
2024-06-18 13:55:23,257 Updated root to '/user/bdm'.
2024-06-18 13:55:23,257 Resolved path 'user/bdm/Model_explotation_zone/GLM_V1.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet'.
2024-06-18 13:55:23,257 Listing '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet'.
2024-06-18 13:55:23,257 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet'.
2024-06-18 13:55:23,257 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet'.
2024-06-18 13:55:23,268 http://10.4.41.35:9870 "GET /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet?user.name=bdm&op=LISTSTATUS HTTP/1.1" 404 None
2024-06-18 13:55:23,268 Uploading 4 files using 1 thread(s).
2024-06-18 13:55:23,268 Uploading 'GLM_V1.parquet\\.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc'.
2024-06-18 13:55:23,270 Writing to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc'.
2024-06-18 13:55:23,270 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc'.
2024-06-18 13:55:23,279 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:55:23,283 Starting new HTTP connection (1): charmander.fib.upc.es:9864
2024-06-18 13:55:23,309 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/.part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet.crc?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:55:23,309 Uploading 'GLM_V1.parquet\\._SUCCESS.crc' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc'.
2024-06-18 13:55:23,311 Writing to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc'.
2024-06-18 13:55:23,311 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc'.
2024-06-18 13:55:23,353 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:55:23,356 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:55:23,381 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/._SUCCESS.crc?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:55:23,382 Uploading 'GLM_V1.parquet\\part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet'.
2024-06-18 13:55:23,403 Writing to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet'.
2024-06-18 13:55:23,403 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet'.
2024-06-18 13:55:23,412 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:55:23,415 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:55:23,482 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/part-00000-8f5236a4-d0a6-43cd-9a00-10c3615bc8f5-c000.snappy.parquet?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:55:23,482 Uploading 'GLM_V1.parquet\\_SUCCESS' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS'.
2024-06-18 13:55:23,482 Writing to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS'.
2024-06-18 13:55:23,482 Resolved path '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS'.
2024-06-18 13:55:23,491 http://10.4.41.35:9870 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS?user.name=bdm&overwrite=True&op=CREATE HTTP/1.1" 307 0
2024-06-18 13:55:23,494 Resetting dropped connection: charmander.fib.upc.es
2024-06-18 13:55:23,514 http://charmander.fib.upc.es:9864 "PUT /webhdfs/v1/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet/_SUCCESS?op=CREATE&user.name=bdm&namenoderpcaddress=charmander.fib.upc.es:27000&createflag=&createparent=true&overwrite=true&user.name=bdm HTTP/1.1" 201 0
2024-06-18 13:55:23,514 Upload of 'GLM_V1.parquet' to '/user/bdm/user/bdm/Model_explotation_zone/GLM_V1.parquet' complete.
2024-06-18 13:55:23,515 Model 20240615-1328-hotels uploaded correctly at 'user/bdm/Model_explotation_zone/GLM_V1.parquet' path
2024-06-18 13:57:00,695 Command to send: r
u
org
rj
e

2024-06-18 13:57:00,697 Answer received: !yp
2024-06-18 13:57:00,698 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:00,698 Answer received: !yp
2024-06-18 13:57:00,698 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:00,699 Answer received: !yp
2024-06-18 13:57:00,699 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:57:00,700 Answer received: !yp
2024-06-18 13:57:00,700 Command to send: r
u
org.apache.spark.ml.feature
rj
e

2024-06-18 13:57:00,700 Answer received: !yp
2024-06-18 13:57:00,700 Command to send: r
u
org.apache.spark.ml.feature.VectorAssembler
rj
e

2024-06-18 13:57:00,797 Answer received: !ycorg.apache.spark.ml.feature.VectorAssembler
2024-06-18 13:57:00,797 Command to send: i
org.apache.spark.ml.feature.VectorAssembler
sVectorAssembler_292a59dfe59e
e

2024-06-18 13:57:00,810 Answer received: !yro731
2024-06-18 13:57:00,811 Command to send: c
o731
getParam
shandleInvalid
e

2024-06-18 13:57:00,849 Answer received: !yro732
2024-06-18 13:57:00,849 Command to send: c
o732
w
sskip
e

2024-06-18 13:57:00,850 Answer received: !yro733
2024-06-18 13:57:00,850 Command to send: c
o731
set
ro733
e

2024-06-18 13:57:00,852 Answer received: !yro734
2024-06-18 13:57:00,853 Command to send: c
o731
getParam
shandleInvalid
e

2024-06-18 13:57:00,853 Answer received: !yro735
2024-06-18 13:57:00,854 Command to send: c
o735
w
serror
e

2024-06-18 13:57:00,854 Answer received: !yro736
2024-06-18 13:57:00,854 Command to send: c
o731
getParam
sinputCols
e

2024-06-18 13:57:00,855 Answer received: !yro737
2024-06-18 13:57:00,855 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:00,856 Answer received: !ylo738
2024-06-18 13:57:00,857 Command to send: c
o738
add
sAvg_stars
e

2024-06-18 13:57:00,858 Answer received: !ybtrue
2024-06-18 13:57:00,858 Command to send: c
o738
add
sN_hotels
e

2024-06-18 13:57:00,858 Answer received: !ybtrue
2024-06-18 13:57:00,858 Command to send: c
o738
add
sAvg_lat
e

2024-06-18 13:57:00,859 Answer received: !ybtrue
2024-06-18 13:57:00,859 Command to send: c
o738
add
sAvg_lat
e

2024-06-18 13:57:00,859 Answer received: !ybtrue
2024-06-18 13:57:00,860 Command to send: c
o738
add
snumPhotos
e

2024-06-18 13:57:00,860 Answer received: !ybtrue
2024-06-18 13:57:00,860 Command to send: c
o738
add
sfloor
e

2024-06-18 13:57:00,861 Answer received: !ybtrue
2024-06-18 13:57:00,861 Command to send: c
o738
add
ssize
e

2024-06-18 13:57:00,861 Answer received: !ybtrue
2024-06-18 13:57:00,861 Command to send: c
o738
add
sAvg_Index_RFD
e

2024-06-18 13:57:00,861 Answer received: !ybtrue
2024-06-18 13:57:00,862 Command to send: c
o738
add
snumPhotos
e

2024-06-18 13:57:00,862 Answer received: !ybtrue
2024-06-18 13:57:00,862 Command to send: c
o738
add
sbathrooms
e

2024-06-18 13:57:00,863 Answer received: !ybtrue
2024-06-18 13:57:00,863 Command to send: c
o738
add
sAvg_Index_RFD
e

2024-06-18 13:57:00,863 Answer received: !ybtrue
2024-06-18 13:57:00,863 Command to send: c
o737
w
ro738
e

2024-06-18 13:57:00,864 Answer received: !yro739
2024-06-18 13:57:00,865 Command to send: c
o731
set
ro739
e

2024-06-18 13:57:00,866 Answer received: !yro740
2024-06-18 13:57:00,866 Command to send: c
o731
getParam
soutputCol
e

2024-06-18 13:57:00,867 Answer received: !yro741
2024-06-18 13:57:00,867 Command to send: c
o741
w
sfeatures
e

2024-06-18 13:57:00,868 Answer received: !yro742
2024-06-18 13:57:00,868 Command to send: c
o731
set
ro742
e

2024-06-18 13:57:00,868 Answer received: !yro743
2024-06-18 13:57:00,869 Command to send: c
o731
getParam
soutputCol
e

2024-06-18 13:57:00,869 Answer received: !yro744
2024-06-18 13:57:00,869 Command to send: c
o744
w
sVectorAssembler_292a59dfe59e__output
e

2024-06-18 13:57:00,869 Answer received: !yro745
2024-06-18 13:57:00,870 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:00,870 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:00,871 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:00,871 Answer received: !ym
2024-06-18 13:57:00,871 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:00,871 Answer received: !ylo746
2024-06-18 13:57:00,871 Command to send: c
o746
add
ro736
e

2024-06-18 13:57:00,874 Answer received: !ybtrue
2024-06-18 13:57:00,874 Command to send: c
o746
add
ro745
e

2024-06-18 13:57:00,874 Answer received: !ybtrue
2024-06-18 13:57:00,875 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro746
e

2024-06-18 13:57:00,876 Answer received: !yro747
2024-06-18 13:57:00,876 Command to send: c
o731
setDefault
ro747
e

2024-06-18 13:57:00,878 Answer received: !yro748
2024-06-18 13:57:00,878 Command to send: c
o731
transform
ro608
e

2024-06-18 13:57:00,882 Answer received: !xro749
2024-06-18 13:57:00,882 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,884 Answer received: !yp
2024-06-18 13:57:00,885 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,886 Answer received: !yp
2024-06-18 13:57:00,886 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,886 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,886 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,886 Answer received: !ym
2024-06-18 13:57:00,887 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.catalyst.parser.ParseException
ro749
e

2024-06-18 13:57:00,887 Answer received: !ybfalse
2024-06-18 13:57:00,887 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,889 Answer received: !yp
2024-06-18 13:57:00,889 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,890 Answer received: !yp
2024-06-18 13:57:00,890 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,890 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,890 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,891 Answer received: !ym
2024-06-18 13:57:00,891 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.AnalysisException
ro749
e

2024-06-18 13:57:00,891 Answer received: !ybfalse
2024-06-18 13:57:00,891 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,893 Answer received: !yp
2024-06-18 13:57:00,893 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,894 Answer received: !yp
2024-06-18 13:57:00,894 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,894 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,894 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,894 Answer received: !ym
2024-06-18 13:57:00,895 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.streaming.StreamingQueryException
ro749
e

2024-06-18 13:57:00,895 Answer received: !ybfalse
2024-06-18 13:57:00,895 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,897 Answer received: !yp
2024-06-18 13:57:00,897 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,897 Answer received: !yp
2024-06-18 13:57:00,898 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,898 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,898 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,898 Answer received: !ym
2024-06-18 13:57:00,898 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sorg.apache.spark.sql.execution.QueryExecutionException
ro749
e

2024-06-18 13:57:00,899 Answer received: !ybfalse
2024-06-18 13:57:00,899 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,900 Answer received: !yp
2024-06-18 13:57:00,901 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,901 Answer received: !yp
2024-06-18 13:57:00,901 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,901 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,902 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,902 Answer received: !ym
2024-06-18 13:57:00,902 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.NumberFormatException
ro749
e

2024-06-18 13:57:00,902 Answer received: !ybfalse
2024-06-18 13:57:00,902 Command to send: r
u
py4j
rj
e

2024-06-18 13:57:00,904 Answer received: !yp
2024-06-18 13:57:00,904 Command to send: r
u
py4j.reflection
rj
e

2024-06-18 13:57:00,905 Answer received: !yp
2024-06-18 13:57:00,905 Command to send: r
u
py4j.reflection.TypeUtil
rj
e

2024-06-18 13:57:00,905 Answer received: !ycpy4j.reflection.TypeUtil
2024-06-18 13:57:00,905 Command to send: r
m
py4j.reflection.TypeUtil
isInstanceOf
e

2024-06-18 13:57:00,906 Answer received: !ym
2024-06-18 13:57:00,906 Command to send: c
z:py4j.reflection.TypeUtil
isInstanceOf
sjava.lang.IllegalArgumentException
ro749
e

2024-06-18 13:57:00,906 Answer received: !ybtrue
2024-06-18 13:57:00,906 Command to send: c
o749
getMessage
e

2024-06-18 13:57:00,907 Answer received: !ysAvg_stars does not exist. Available: addresses_district_id, name, secondary_filters_name, geo_epgs_4326_lat, geo_epgs_4326_lon
2024-06-18 13:57:00,907 Command to send: r
u
org
rj
e

2024-06-18 13:57:00,909 Answer received: !yp
2024-06-18 13:57:00,909 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:00,909 Answer received: !yp
2024-06-18 13:57:00,909 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:00,910 Answer received: !yp
2024-06-18 13:57:00,910 Command to send: r
u
org.apache.spark.util
rj
e

2024-06-18 13:57:00,911 Answer received: !yp
2024-06-18 13:57:00,911 Command to send: r
u
org.apache.spark.util.Utils
rj
e

2024-06-18 13:57:00,911 Answer received: !ycorg.apache.spark.util.Utils
2024-06-18 13:57:00,911 Command to send: r
m
org.apache.spark.util.Utils
exceptionString
e

2024-06-18 13:57:00,911 Answer received: !ym
2024-06-18 13:57:00,912 Command to send: c
z:org.apache.spark.util.Utils
exceptionString
ro749
e

2024-06-18 13:57:00,912 Answer received: !ysjava.lang.IllegalArgumentException: Avg_stars does not exist. Available: addresses_district_id, name, secondary_filters_name, geo_epgs_4326_lat, geo_epgs_4326_lon\r\n	at org.apache.spark.sql.types.StructType.$anonfun$apply$1(StructType.scala:282)\r\n	at scala.collection.immutable.HashMap$HashTrieMap.getOrElse0(HashMap.scala:596)\r\n	at scala.collection.immutable.HashMap.getOrElse(HashMap.scala:73)\r\n	at org.apache.spark.sql.types.StructType.apply(StructType.scala:281)\r\n	at org.apache.spark.ml.feature.VectorAssembler.$anonfun$transformSchema$1(VectorAssembler.scala:161)\r\n	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)\r\n	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)\r\n	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)\r\n	at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\r\n	at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:160)\r\n	at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n	at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:85)\r\n	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\r\n	at java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n	at py4j.Gateway.invoke(Gateway.java:282)\r\n	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n	at py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n	at java.base/java.lang.Thread.run(Thread.java:1570)\r\n
2024-06-18 13:57:00,912 Command to send: c
o749
getCause
e

2024-06-18 13:57:00,913 Answer received: !yn
2024-06-18 13:57:00,947 Command to send: m
d
o734
e

2024-06-18 13:57:00,948 Answer received: !yv
2024-06-18 13:57:00,948 Command to send: m
d
o738
e

2024-06-18 13:57:00,958 Answer received: !yv
2024-06-18 13:57:00,958 Command to send: m
d
o740
e

2024-06-18 13:57:00,966 Answer received: !yv
2024-06-18 13:57:00,966 Command to send: m
d
o743
e

2024-06-18 13:57:00,996 Answer received: !yv
2024-06-18 13:57:01,010 Command to send: m
d
o746
e

2024-06-18 13:57:01,037 Command to send: r
u
org
rj
e

2024-06-18 13:57:01,037 Answer received: !yv
2024-06-18 13:57:01,037 Command to send: m
d
o748
e

2024-06-18 13:57:01,037 Answer received: !yv
2024-06-18 13:57:01,038 Command to send: m
d
o673
e

2024-06-18 13:57:01,038 Answer received: !yv
2024-06-18 13:57:01,038 Command to send: m
d
o674
e

2024-06-18 13:57:01,038 Answer received: !yv
2024-06-18 13:57:01,038 Command to send: m
d
o675
e

2024-06-18 13:57:01,039 Answer received: !yv
2024-06-18 13:57:01,039 Command to send: m
d
o676
e

2024-06-18 13:57:01,039 Answer received: !yp
2024-06-18 13:57:01,039 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:01,039 Answer received: !yv
2024-06-18 13:57:01,039 Command to send: m
d
o677
e

2024-06-18 13:57:01,039 Answer received: !yp
2024-06-18 13:57:01,040 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:01,040 Answer received: !yv
2024-06-18 13:57:01,040 Command to send: m
d
o678
e

2024-06-18 13:57:01,040 Answer received: !yp
2024-06-18 13:57:01,040 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:57:01,040 Answer received: !yv
2024-06-18 13:57:01,041 Command to send: m
d
o679
e

2024-06-18 13:57:01,041 Answer received: !yp
2024-06-18 13:57:01,041 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:57:01,041 Answer received: !yv
2024-06-18 13:57:01,041 Command to send: m
d
o680
e

2024-06-18 13:57:01,041 Answer received: !yp
2024-06-18 13:57:01,042 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:57:01,042 Answer received: !yv
2024-06-18 13:57:01,042 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:57:01,042 Command to send: m
d
o681
e

2024-06-18 13:57:01,042 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,042 Answer received: !yv
2024-06-18 13:57:01,043 Answer received: !ym
2024-06-18 13:57:01,043 Command to send: m
d
o682
e

2024-06-18 13:57:01,043 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,043 Answer received: !yv
2024-06-18 13:57:01,043 Command to send: m
d
o684
e

2024-06-18 13:57:01,043 Answer received: !yro750
2024-06-18 13:57:01,044 Command to send: c
o750
pysparkJVMStacktraceEnabled
e

2024-06-18 13:57:01,044 Answer received: !yv
2024-06-18 13:57:01,044 Answer received: !ybfalse
2024-06-18 13:57:01,044 Command to send: m
d
o686
e

2024-06-18 13:57:01,045 Answer received: !yv
2024-06-18 13:57:01,045 Command to send: r
u
org
rj
e

2024-06-18 13:57:01,045 Command to send: m
d
o687
e

2024-06-18 13:57:01,046 Answer received: !yv
2024-06-18 13:57:01,046 Command to send: m
d
o689
e

2024-06-18 13:57:01,046 Answer received: !yv
2024-06-18 13:57:01,046 Command to send: m
d
o690
e

2024-06-18 13:57:01,046 Answer received: !yv
2024-06-18 13:57:01,046 Command to send: m
d
o691
e

2024-06-18 13:57:01,046 Answer received: !yv
2024-06-18 13:57:01,046 Command to send: m
d
o692
e

2024-06-18 13:57:01,047 Answer received: !yv
2024-06-18 13:57:01,047 Command to send: m
d
o693
e

2024-06-18 13:57:01,047 Answer received: !yv
2024-06-18 13:57:01,047 Answer received: !yp
2024-06-18 13:57:01,047 Command to send: m
d
o695
e

2024-06-18 13:57:01,047 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:01,047 Answer received: !yv
2024-06-18 13:57:01,048 Command to send: m
d
o697
e

2024-06-18 13:57:01,048 Answer received: !yp
2024-06-18 13:57:01,048 Answer received: !yv
2024-06-18 13:57:01,048 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:01,048 Command to send: m
d
o698
e

2024-06-18 13:57:01,048 Answer received: !yv
2024-06-18 13:57:01,048 Command to send: m
d
o699
e

2024-06-18 13:57:01,049 Answer received: !yp
2024-06-18 13:57:01,049 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:57:01,049 Answer received: !yv
2024-06-18 13:57:01,049 Command to send: m
d
o700
e

2024-06-18 13:57:01,049 Answer received: !yv
2024-06-18 13:57:01,049 Answer received: !yp
2024-06-18 13:57:01,049 Command to send: m
d
o701
e

2024-06-18 13:57:01,050 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:57:01,050 Answer received: !yv
2024-06-18 13:57:01,050 Command to send: m
d
o702
e

2024-06-18 13:57:01,050 Answer received: !yv
2024-06-18 13:57:01,050 Answer received: !yp
2024-06-18 13:57:01,050 Command to send: m
d
o704
e

2024-06-18 13:57:01,051 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:57:01,051 Answer received: !yv
2024-06-18 13:57:01,051 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:57:01,051 Command to send: m
d
o705
e

2024-06-18 13:57:01,051 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,051 Answer received: !yv
2024-06-18 13:57:01,052 Command to send: m
d
o706
e

2024-06-18 13:57:01,052 Answer received: !ym
2024-06-18 13:57:01,052 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,052 Answer received: !yv
2024-06-18 13:57:01,052 Command to send: m
d
o707
e

2024-06-18 13:57:01,052 Answer received: !yro751
2024-06-18 13:57:01,053 Command to send: c
o751
pysparkJVMStacktraceEnabled
e

2024-06-18 13:57:01,053 Answer received: !yv
2024-06-18 13:57:01,053 Command to send: m
d
o708
e

2024-06-18 13:57:01,053 Answer received: !ybfalse
2024-06-18 13:57:01,053 Answer received: !yv
2024-06-18 13:57:01,053 Command to send: m
d
o709
e

2024-06-18 13:57:01,054 Command to send: r
u
org
rj
e

2024-06-18 13:57:01,054 Answer received: !yv
2024-06-18 13:57:01,054 Command to send: m
d
o710
e

2024-06-18 13:57:01,055 Answer received: !yv
2024-06-18 13:57:01,055 Command to send: m
d
o711
e

2024-06-18 13:57:01,055 Answer received: !yv
2024-06-18 13:57:01,055 Command to send: m
d
o713
e

2024-06-18 13:57:01,055 Answer received: !yv
2024-06-18 13:57:01,055 Command to send: m
d
o714
e

2024-06-18 13:57:01,056 Answer received: !yv
2024-06-18 13:57:01,056 Answer received: !yp
2024-06-18 13:57:01,056 Command to send: m
d
o715
e

2024-06-18 13:57:01,056 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:01,056 Answer received: !yv
2024-06-18 13:57:01,056 Command to send: m
d
o716
e

2024-06-18 13:57:01,056 Answer received: !yp
2024-06-18 13:57:01,057 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:01,057 Answer received: !yv
2024-06-18 13:57:01,057 Command to send: m
d
o717
e

2024-06-18 13:57:01,057 Answer received: !yv
2024-06-18 13:57:01,057 Answer received: !yp
2024-06-18 13:57:01,057 Command to send: m
d
o718
e

2024-06-18 13:57:01,058 Command to send: r
u
org.apache.spark.sql
rj
e

2024-06-18 13:57:01,058 Answer received: !yv
2024-06-18 13:57:01,058 Command to send: m
d
o719
e

2024-06-18 13:57:01,058 Answer received: !yp
2024-06-18 13:57:01,058 Command to send: r
u
org.apache.spark.sql.internal
rj
e

2024-06-18 13:57:01,058 Answer received: !yv
2024-06-18 13:57:01,058 Command to send: m
d
o720
e

2024-06-18 13:57:01,059 Answer received: !yv
2024-06-18 13:57:01,059 Answer received: !yp
2024-06-18 13:57:01,059 Command to send: m
d
o722
e

2024-06-18 13:57:01,059 Command to send: r
u
org.apache.spark.sql.internal.SQLConf
rj
e

2024-06-18 13:57:01,059 Answer received: !yv
2024-06-18 13:57:01,059 Answer received: !ycorg.apache.spark.sql.internal.SQLConf
2024-06-18 13:57:01,059 Command to send: m
d
o723
e

2024-06-18 13:57:01,060 Command to send: r
m
org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,060 Answer received: !yv
2024-06-18 13:57:01,060 Command to send: m
d
o725
e

2024-06-18 13:57:01,060 Answer received: !ym
2024-06-18 13:57:01,060 Answer received: !yv
2024-06-18 13:57:01,060 Command to send: c
z:org.apache.spark.sql.internal.SQLConf
get
e

2024-06-18 13:57:01,060 Command to send: m
d
o727
e

2024-06-18 13:57:01,061 Answer received: !yv
2024-06-18 13:57:01,061 Command to send: m
d
o728
e

2024-06-18 13:57:01,061 Answer received: !yro752
2024-06-18 13:57:01,061 Answer received: !yv
2024-06-18 13:57:01,061 Command to send: c
o752
pysparkJVMStacktraceEnabled
e

2024-06-18 13:57:01,062 Command to send: m
d
o729
e

2024-06-18 13:57:01,062 Answer received: !ybfalse
2024-06-18 13:57:01,062 Answer received: !yv
2024-06-18 13:57:01,062 Command to send: m
d
o730
e

2024-06-18 13:57:01,063 Answer received: !yv
2024-06-18 13:57:01,063 Command to send: m
d
o732
e

2024-06-18 13:57:01,064 Answer received: !yv
2024-06-18 13:57:01,065 Command to send: m
d
o733
e

2024-06-18 13:57:01,065 Answer received: !yv
2024-06-18 13:57:01,065 Command to send: m
d
o735
e

2024-06-18 13:57:01,065 Answer received: !yv
2024-06-18 13:57:01,065 Command to send: m
d
o736
e

2024-06-18 13:57:01,065 Answer received: !yv
2024-06-18 13:57:01,065 Command to send: m
d
o737
e

2024-06-18 13:57:01,066 Answer received: !yv
2024-06-18 13:57:01,066 Command to send: m
d
o739
e

2024-06-18 13:57:01,066 Answer received: !yv
2024-06-18 13:57:01,066 Command to send: m
d
o741
e

2024-06-18 13:57:01,067 Answer received: !yv
2024-06-18 13:57:01,067 Command to send: m
d
o742
e

2024-06-18 13:57:01,067 Answer received: !yv
2024-06-18 13:57:01,067 Command to send: m
d
o744
e

2024-06-18 13:57:01,068 Answer received: !yv
2024-06-18 13:57:01,068 Command to send: m
d
o745
e

2024-06-18 13:57:01,068 Answer received: !yv
2024-06-18 13:57:01,068 Command to send: m
d
o747
e

2024-06-18 13:57:01,069 Answer received: !yv
2024-06-18 13:57:01,069 Command to send: m
d
o645
e

2024-06-18 13:57:01,069 Answer received: !yv
2024-06-18 13:57:01,069 Command to send: m
d
o666
e

2024-06-18 13:57:01,069 Answer received: !yv
2024-06-18 13:57:01,070 Command to send: m
d
o670
e

2024-06-18 13:57:01,070 Answer received: !yv
2024-06-18 13:57:01,070 Command to send: m
d
o672
e

2024-06-18 13:57:01,070 Answer received: !yv
2024-06-18 13:57:32,153 Command to send: r
u
org
rj
e

2024-06-18 13:57:32,155 Answer received: !yp
2024-06-18 13:57:32,155 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:32,156 Answer received: !yp
2024-06-18 13:57:32,156 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:32,157 Answer received: !yp
2024-06-18 13:57:32,157 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:57:32,157 Answer received: !yp
2024-06-18 13:57:32,158 Command to send: r
u
org.apache.spark.ml.feature
rj
e

2024-06-18 13:57:32,158 Answer received: !yp
2024-06-18 13:57:32,158 Command to send: r
u
org.apache.spark.ml.feature.VectorAssembler
rj
e

2024-06-18 13:57:32,159 Answer received: !ycorg.apache.spark.ml.feature.VectorAssembler
2024-06-18 13:57:32,159 Command to send: i
org.apache.spark.ml.feature.VectorAssembler
sVectorAssembler_34812d0eded5
e

2024-06-18 13:57:32,160 Answer received: !yro753
2024-06-18 13:57:32,162 Command to send: c
o753
getParam
shandleInvalid
e

2024-06-18 13:57:32,162 Answer received: !yro754
2024-06-18 13:57:32,163 Command to send: c
o754
w
sskip
e

2024-06-18 13:57:32,163 Answer received: !yro755
2024-06-18 13:57:32,163 Command to send: c
o753
set
ro755
e

2024-06-18 13:57:32,163 Answer received: !yro756
2024-06-18 13:57:32,164 Command to send: c
o753
getParam
shandleInvalid
e

2024-06-18 13:57:32,164 Answer received: !yro757
2024-06-18 13:57:32,164 Command to send: c
o757
w
serror
e

2024-06-18 13:57:32,164 Answer received: !yro758
2024-06-18 13:57:32,165 Command to send: c
o753
getParam
sinputCols
e

2024-06-18 13:57:32,165 Answer received: !yro759
2024-06-18 13:57:32,165 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:32,166 Answer received: !ylo760
2024-06-18 13:57:32,166 Command to send: c
o760
add
sAvg_stars
e

2024-06-18 13:57:32,166 Answer received: !ybtrue
2024-06-18 13:57:32,166 Command to send: c
o760
add
sN_hotels
e

2024-06-18 13:57:32,167 Answer received: !ybtrue
2024-06-18 13:57:32,167 Command to send: c
o760
add
sAvg_lat
e

2024-06-18 13:57:32,167 Answer received: !ybtrue
2024-06-18 13:57:32,167 Command to send: c
o760
add
sAvg_lat
e

2024-06-18 13:57:32,167 Answer received: !ybtrue
2024-06-18 13:57:32,168 Command to send: c
o760
add
snumPhotos
e

2024-06-18 13:57:32,168 Answer received: !ybtrue
2024-06-18 13:57:32,168 Command to send: c
o760
add
sfloor
e

2024-06-18 13:57:32,169 Answer received: !ybtrue
2024-06-18 13:57:32,169 Command to send: c
o760
add
ssize
e

2024-06-18 13:57:32,169 Answer received: !ybtrue
2024-06-18 13:57:32,169 Command to send: c
o760
add
sAvg_Index_RFD
e

2024-06-18 13:57:32,170 Answer received: !ybtrue
2024-06-18 13:57:32,170 Command to send: c
o760
add
snumPhotos
e

2024-06-18 13:57:32,170 Answer received: !ybtrue
2024-06-18 13:57:32,170 Command to send: c
o760
add
sbathrooms
e

2024-06-18 13:57:32,170 Answer received: !ybtrue
2024-06-18 13:57:32,171 Command to send: c
o760
add
sAvg_Index_RFD
e

2024-06-18 13:57:32,171 Answer received: !ybtrue
2024-06-18 13:57:32,171 Command to send: c
o759
w
ro760
e

2024-06-18 13:57:32,171 Answer received: !yro761
2024-06-18 13:57:32,172 Command to send: c
o753
set
ro761
e

2024-06-18 13:57:32,172 Answer received: !yro762
2024-06-18 13:57:32,173 Command to send: c
o753
getParam
soutputCol
e

2024-06-18 13:57:32,173 Answer received: !yro763
2024-06-18 13:57:32,173 Command to send: c
o763
w
sfeatures
e

2024-06-18 13:57:32,174 Answer received: !yro764
2024-06-18 13:57:32,174 Command to send: c
o753
set
ro764
e

2024-06-18 13:57:32,174 Answer received: !yro765
2024-06-18 13:57:32,174 Command to send: c
o753
getParam
soutputCol
e

2024-06-18 13:57:32,175 Answer received: !yro766
2024-06-18 13:57:32,175 Command to send: c
o766
w
sVectorAssembler_34812d0eded5__output
e

2024-06-18 13:57:32,175 Answer received: !yro767
2024-06-18 13:57:32,175 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:32,176 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:32,176 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:32,177 Answer received: !ym
2024-06-18 13:57:32,177 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:32,177 Answer received: !ylo768
2024-06-18 13:57:32,178 Command to send: c
o768
add
ro758
e

2024-06-18 13:57:32,178 Answer received: !ybtrue
2024-06-18 13:57:32,178 Command to send: c
o768
add
ro767
e

2024-06-18 13:57:32,178 Answer received: !ybtrue
2024-06-18 13:57:32,179 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro768
e

2024-06-18 13:57:32,179 Answer received: !yro769
2024-06-18 13:57:32,179 Command to send: c
o753
setDefault
ro769
e

2024-06-18 13:57:32,180 Answer received: !yro770
2024-06-18 13:57:32,180 Command to send: c
o753
transform
ro726
e

2024-06-18 13:57:33,086 Command to send: m
d
o756
e

2024-06-18 13:57:33,086 Answer received: !yv
2024-06-18 13:57:33,086 Command to send: m
d
o760
e

2024-06-18 13:57:33,086 Answer received: !yv
2024-06-18 13:57:33,087 Command to send: m
d
o762
e

2024-06-18 13:57:33,087 Answer received: !yv
2024-06-18 13:57:33,087 Command to send: m
d
o765
e

2024-06-18 13:57:33,087 Answer received: !yv
2024-06-18 13:57:33,087 Command to send: m
d
o768
e

2024-06-18 13:57:33,088 Answer received: !yv
2024-06-18 13:57:33,088 Command to send: m
d
o770
e

2024-06-18 13:57:33,088 Answer received: !yv
2024-06-18 13:57:34,991 Answer received: !yro771
2024-06-18 13:57:34,991 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:34,992 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:34,993 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toList
e

2024-06-18 13:57:34,993 Answer received: !ym
2024-06-18 13:57:34,993 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:34,993 Answer received: !ylo772
2024-06-18 13:57:34,994 Command to send: c
o772
add
d0.7
e

2024-06-18 13:57:34,994 Answer received: !ybtrue
2024-06-18 13:57:34,994 Command to send: c
o772
add
d0.3
e

2024-06-18 13:57:34,995 Answer received: !ybtrue
2024-06-18 13:57:34,995 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toList
ro772
e

2024-06-18 13:57:34,996 Answer received: !yro773
2024-06-18 13:57:34,996 Command to send: c
o771
randomSplit
ro773
i123
e

2024-06-18 13:57:35,035 Answer received: !yto774
2024-06-18 13:57:35,035 Command to send: a
e
o774
e

2024-06-18 13:57:35,036 Answer received: !yi2
2024-06-18 13:57:35,036 Command to send: a
g
o774
i0
e

2024-06-18 13:57:35,036 Answer received: !yro775
2024-06-18 13:57:35,036 Command to send: a
e
o774
e

2024-06-18 13:57:35,036 Answer received: !yi2
2024-06-18 13:57:35,037 Command to send: a
g
o774
i1
e

2024-06-18 13:57:35,037 Answer received: !yro776
2024-06-18 13:57:35,037 Command to send: a
e
o774
e

2024-06-18 13:57:35,037 Answer received: !yi2
2024-06-18 13:57:35,038 Command to send: c
o775
count
e

2024-06-18 13:57:35,091 Command to send: m
d
o772
e

2024-06-18 13:57:35,091 Answer received: !yv
2024-06-18 13:57:35,091 Command to send: m
d
o774
e

2024-06-18 13:57:35,092 Answer received: !yv
2024-06-18 13:57:37,452 Answer received: !yL38520
2024-06-18 13:57:37,452 Command to send: c
o776
count
e

2024-06-18 13:57:38,617 Answer received: !yL16720
2024-06-18 13:57:38,619 Command to send: r
u
org
rj
e

2024-06-18 13:57:38,622 Answer received: !yp
2024-06-18 13:57:38,623 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:38,623 Answer received: !yp
2024-06-18 13:57:38,623 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:38,624 Answer received: !yp
2024-06-18 13:57:38,624 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:57:38,624 Answer received: !yp
2024-06-18 13:57:38,624 Command to send: r
u
org.apache.spark.ml.regression
rj
e

2024-06-18 13:57:38,625 Answer received: !yp
2024-06-18 13:57:38,625 Command to send: r
u
org.apache.spark.ml.regression.GeneralizedLinearRegression
rj
e

2024-06-18 13:57:38,694 Answer received: !ycorg.apache.spark.ml.regression.GeneralizedLinearRegression
2024-06-18 13:57:38,695 Command to send: i
org.apache.spark.ml.regression.GeneralizedLinearRegression
sGeneralizedLinearRegression_11f9ba4caa8a
e

2024-06-18 13:57:38,713 Answer received: !yro777
2024-06-18 13:57:38,714 Command to send: c
o777
getParam
saggregationDepth
e

2024-06-18 13:57:38,725 Answer received: !yro778
2024-06-18 13:57:38,725 Command to send: c
o778
w
i2
e

2024-06-18 13:57:38,726 Answer received: !yro779
2024-06-18 13:57:38,726 Command to send: c
o777
getParam
sfamily
e

2024-06-18 13:57:38,727 Answer received: !yro780
2024-06-18 13:57:38,727 Command to send: c
o780
w
sgaussian
e

2024-06-18 13:57:38,727 Answer received: !yro781
2024-06-18 13:57:38,727 Command to send: c
o777
set
ro781
e

2024-06-18 13:57:38,728 Answer received: !yro782
2024-06-18 13:57:38,728 Command to send: c
o777
getParam
sfamily
e

2024-06-18 13:57:38,729 Answer received: !yro783
2024-06-18 13:57:38,729 Command to send: c
o783
w
sgaussian
e

2024-06-18 13:57:38,730 Answer received: !yro784
2024-06-18 13:57:38,730 Command to send: c
o777
getParam
sfeaturesCol
e

2024-06-18 13:57:38,730 Answer received: !yro785
2024-06-18 13:57:38,730 Command to send: c
o785
w
sfeatures
e

2024-06-18 13:57:38,731 Answer received: !yro786
2024-06-18 13:57:38,731 Command to send: c
o777
getParam
sfitIntercept
e

2024-06-18 13:57:38,731 Answer received: !yro787
2024-06-18 13:57:38,731 Command to send: c
o787
w
bTrue
e

2024-06-18 13:57:38,732 Answer received: !yro788
2024-06-18 13:57:38,732 Command to send: c
o777
getParam
slabelCol
e

2024-06-18 13:57:38,733 Answer received: !yro789
2024-06-18 13:57:38,733 Command to send: c
o789
w
sprice
e

2024-06-18 13:57:38,733 Answer received: !yro790
2024-06-18 13:57:38,733 Command to send: c
o777
set
ro790
e

2024-06-18 13:57:38,734 Answer received: !yro791
2024-06-18 13:57:38,734 Command to send: c
o777
getParam
slabelCol
e

2024-06-18 13:57:38,734 Answer received: !yro792
2024-06-18 13:57:38,734 Command to send: c
o792
w
slabel
e

2024-06-18 13:57:38,734 Answer received: !yro793
2024-06-18 13:57:38,735 Command to send: c
o777
getParam
slink
e

2024-06-18 13:57:38,735 Answer received: !yro794
2024-06-18 13:57:38,735 Command to send: c
o794
w
sidentity
e

2024-06-18 13:57:38,735 Answer received: !yro795
2024-06-18 13:57:38,736 Command to send: c
o777
set
ro795
e

2024-06-18 13:57:38,736 Answer received: !yro796
2024-06-18 13:57:38,736 Command to send: c
o777
getParam
smaxIter
e

2024-06-18 13:57:38,737 Answer received: !yro797
2024-06-18 13:57:38,737 Command to send: c
o797
w
i10
e

2024-06-18 13:57:38,738 Answer received: !yro798
2024-06-18 13:57:38,738 Command to send: c
o777
set
ro798
e

2024-06-18 13:57:38,738 Answer received: !yro799
2024-06-18 13:57:38,739 Command to send: c
o777
getParam
smaxIter
e

2024-06-18 13:57:38,739 Answer received: !yro800
2024-06-18 13:57:38,739 Command to send: c
o800
w
i25
e

2024-06-18 13:57:38,739 Answer received: !yro801
2024-06-18 13:57:38,740 Command to send: c
o777
getParam
spredictionCol
e

2024-06-18 13:57:38,740 Answer received: !yro802
2024-06-18 13:57:38,740 Command to send: c
o802
w
sprediction
e

2024-06-18 13:57:38,740 Answer received: !yro803
2024-06-18 13:57:38,741 Command to send: c
o777
getParam
sregParam
e

2024-06-18 13:57:38,741 Answer received: !yro804
2024-06-18 13:57:38,741 Command to send: c
o804
w
d0.3
e

2024-06-18 13:57:38,743 Answer received: !yro805
2024-06-18 13:57:38,743 Command to send: c
o777
set
ro805
e

2024-06-18 13:57:38,743 Answer received: !yro806
2024-06-18 13:57:38,743 Command to send: c
o777
getParam
sregParam
e

2024-06-18 13:57:38,744 Answer received: !yro807
2024-06-18 13:57:38,744 Command to send: c
o807
w
d0.0
e

2024-06-18 13:57:38,744 Answer received: !yro808
2024-06-18 13:57:38,744 Command to send: c
o777
getParam
ssolver
e

2024-06-18 13:57:38,745 Answer received: !yro809
2024-06-18 13:57:38,745 Command to send: c
o809
w
sirls
e

2024-06-18 13:57:38,745 Answer received: !yro810
2024-06-18 13:57:38,746 Command to send: c
o777
getParam
stol
e

2024-06-18 13:57:38,746 Answer received: !yro811
2024-06-18 13:57:38,746 Command to send: c
o811
w
d1e-06
e

2024-06-18 13:57:38,746 Answer received: !yro812
2024-06-18 13:57:38,747 Command to send: c
o777
getParam
svariancePower
e

2024-06-18 13:57:38,747 Answer received: !yro813
2024-06-18 13:57:38,747 Command to send: c
o813
w
d0.0
e

2024-06-18 13:57:38,747 Answer received: !yro814
2024-06-18 13:57:38,748 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:38,748 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:38,748 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:38,749 Answer received: !ym
2024-06-18 13:57:38,749 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:38,749 Answer received: !ylo815
2024-06-18 13:57:38,749 Command to send: c
o815
add
ro779
e

2024-06-18 13:57:38,749 Answer received: !ybtrue
2024-06-18 13:57:38,750 Command to send: c
o815
add
ro784
e

2024-06-18 13:57:38,750 Answer received: !ybtrue
2024-06-18 13:57:38,750 Command to send: c
o815
add
ro786
e

2024-06-18 13:57:38,750 Answer received: !ybtrue
2024-06-18 13:57:38,750 Command to send: c
o815
add
ro788
e

2024-06-18 13:57:38,750 Answer received: !ybtrue
2024-06-18 13:57:38,751 Command to send: c
o815
add
ro793
e

2024-06-18 13:57:38,751 Answer received: !ybtrue
2024-06-18 13:57:38,751 Command to send: c
o815
add
ro801
e

2024-06-18 13:57:38,751 Answer received: !ybtrue
2024-06-18 13:57:38,751 Command to send: c
o815
add
ro803
e

2024-06-18 13:57:38,752 Answer received: !ybtrue
2024-06-18 13:57:38,752 Command to send: c
o815
add
ro808
e

2024-06-18 13:57:38,752 Answer received: !ybtrue
2024-06-18 13:57:38,752 Command to send: c
o815
add
ro810
e

2024-06-18 13:57:38,752 Answer received: !ybtrue
2024-06-18 13:57:38,752 Command to send: c
o815
add
ro812
e

2024-06-18 13:57:38,753 Answer received: !ybtrue
2024-06-18 13:57:38,753 Command to send: c
o815
add
ro814
e

2024-06-18 13:57:38,753 Answer received: !ybtrue
2024-06-18 13:57:38,753 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro815
e

2024-06-18 13:57:38,754 Answer received: !yro816
2024-06-18 13:57:38,754 Command to send: c
o777
setDefault
ro816
e

2024-06-18 13:57:38,754 Answer received: !yro817
2024-06-18 13:57:38,755 Command to send: c
o777
fit
ro775
e

2024-06-18 13:57:39,093 Command to send: m
d
o782
e

2024-06-18 13:57:39,093 Answer received: !yv
2024-06-18 13:57:39,093 Command to send: m
d
o791
e

2024-06-18 13:57:39,094 Answer received: !yv
2024-06-18 13:57:39,094 Command to send: m
d
o796
e

2024-06-18 13:57:39,094 Answer received: !yv
2024-06-18 13:57:39,094 Command to send: m
d
o799
e

2024-06-18 13:57:39,095 Answer received: !yv
2024-06-18 13:57:39,095 Command to send: m
d
o806
e

2024-06-18 13:57:39,095 Answer received: !yv
2024-06-18 13:57:39,095 Command to send: m
d
o815
e

2024-06-18 13:57:39,095 Answer received: !yv
2024-06-18 13:57:39,096 Command to send: m
d
o817
e

2024-06-18 13:57:39,096 Answer received: !yv
2024-06-18 13:57:41,454 Answer received: !yro818
2024-06-18 13:57:41,455 Command to send: c
o818
params
e

2024-06-18 13:57:41,456 Answer received: !yto819
2024-06-18 13:57:41,456 Command to send: a
e
o819
e

2024-06-18 13:57:41,456 Answer received: !yi16
2024-06-18 13:57:41,456 Command to send: a
e
o819
e

2024-06-18 13:57:41,457 Answer received: !yi16
2024-06-18 13:57:41,457 Command to send: a
g
o819
i0
e

2024-06-18 13:57:41,457 Answer received: !yro820
2024-06-18 13:57:41,457 Command to send: a
e
o819
e

2024-06-18 13:57:41,458 Answer received: !yi16
2024-06-18 13:57:41,458 Command to send: a
g
o819
i1
e

2024-06-18 13:57:41,458 Answer received: !yro821
2024-06-18 13:57:41,458 Command to send: a
e
o819
e

2024-06-18 13:57:41,459 Answer received: !yi16
2024-06-18 13:57:41,459 Command to send: a
g
o819
i2
e

2024-06-18 13:57:41,459 Answer received: !yro822
2024-06-18 13:57:41,459 Command to send: a
e
o819
e

2024-06-18 13:57:41,460 Answer received: !yi16
2024-06-18 13:57:41,460 Command to send: a
g
o819
i3
e

2024-06-18 13:57:41,460 Answer received: !yro823
2024-06-18 13:57:41,460 Command to send: a
e
o819
e

2024-06-18 13:57:41,460 Answer received: !yi16
2024-06-18 13:57:41,460 Command to send: a
g
o819
i4
e

2024-06-18 13:57:41,461 Answer received: !yro824
2024-06-18 13:57:41,461 Command to send: a
e
o819
e

2024-06-18 13:57:41,461 Answer received: !yi16
2024-06-18 13:57:41,461 Command to send: a
g
o819
i5
e

2024-06-18 13:57:41,462 Answer received: !yro825
2024-06-18 13:57:41,462 Command to send: a
e
o819
e

2024-06-18 13:57:41,462 Answer received: !yi16
2024-06-18 13:57:41,462 Command to send: a
g
o819
i6
e

2024-06-18 13:57:41,462 Answer received: !yro826
2024-06-18 13:57:41,463 Command to send: a
e
o819
e

2024-06-18 13:57:41,463 Answer received: !yi16
2024-06-18 13:57:41,463 Command to send: a
g
o819
i7
e

2024-06-18 13:57:41,463 Answer received: !yro827
2024-06-18 13:57:41,463 Command to send: a
e
o819
e

2024-06-18 13:57:41,464 Answer received: !yi16
2024-06-18 13:57:41,464 Command to send: a
g
o819
i8
e

2024-06-18 13:57:41,464 Answer received: !yro828
2024-06-18 13:57:41,464 Command to send: a
e
o819
e

2024-06-18 13:57:41,464 Answer received: !yi16
2024-06-18 13:57:41,465 Command to send: a
g
o819
i9
e

2024-06-18 13:57:41,465 Answer received: !yro829
2024-06-18 13:57:41,465 Command to send: a
e
o819
e

2024-06-18 13:57:41,465 Answer received: !yi16
2024-06-18 13:57:41,465 Command to send: a
g
o819
i10
e

2024-06-18 13:57:41,466 Answer received: !yro830
2024-06-18 13:57:41,466 Command to send: a
e
o819
e

2024-06-18 13:57:41,466 Answer received: !yi16
2024-06-18 13:57:41,466 Command to send: a
g
o819
i11
e

2024-06-18 13:57:41,466 Answer received: !yro831
2024-06-18 13:57:41,467 Command to send: a
e
o819
e

2024-06-18 13:57:41,467 Answer received: !yi16
2024-06-18 13:57:41,467 Command to send: a
g
o819
i12
e

2024-06-18 13:57:41,467 Answer received: !yro832
2024-06-18 13:57:41,467 Command to send: a
e
o819
e

2024-06-18 13:57:41,468 Answer received: !yi16
2024-06-18 13:57:41,468 Command to send: a
g
o819
i13
e

2024-06-18 13:57:41,468 Answer received: !yro833
2024-06-18 13:57:41,468 Command to send: a
e
o819
e

2024-06-18 13:57:41,468 Answer received: !yi16
2024-06-18 13:57:41,468 Command to send: a
g
o819
i14
e

2024-06-18 13:57:41,469 Answer received: !yro834
2024-06-18 13:57:41,469 Command to send: a
e
o819
e

2024-06-18 13:57:41,469 Answer received: !yi16
2024-06-18 13:57:41,469 Command to send: a
g
o819
i15
e

2024-06-18 13:57:41,470 Answer received: !yro835
2024-06-18 13:57:41,470 Command to send: a
e
o819
e

2024-06-18 13:57:41,470 Answer received: !yi16
2024-06-18 13:57:41,470 Command to send: c
o820
name
e

2024-06-18 13:57:41,471 Answer received: !ysaggregationDepth
2024-06-18 13:57:41,471 Command to send: c
o821
name
e

2024-06-18 13:57:41,471 Answer received: !ysfamily
2024-06-18 13:57:41,471 Command to send: c
o822
name
e

2024-06-18 13:57:41,472 Answer received: !ysfeaturesCol
2024-06-18 13:57:41,472 Command to send: c
o823
name
e

2024-06-18 13:57:41,472 Answer received: !ysfitIntercept
2024-06-18 13:57:41,472 Command to send: c
o824
name
e

2024-06-18 13:57:41,473 Answer received: !yslabelCol
2024-06-18 13:57:41,473 Command to send: c
o825
name
e

2024-06-18 13:57:41,473 Answer received: !yslink
2024-06-18 13:57:41,473 Command to send: c
o826
name
e

2024-06-18 13:57:41,473 Answer received: !yslinkPower
2024-06-18 13:57:41,473 Command to send: c
o827
name
e

2024-06-18 13:57:41,474 Answer received: !yslinkPredictionCol
2024-06-18 13:57:41,474 Command to send: c
o828
name
e

2024-06-18 13:57:41,474 Answer received: !ysmaxIter
2024-06-18 13:57:41,474 Command to send: c
o829
name
e

2024-06-18 13:57:41,475 Answer received: !ysoffsetCol
2024-06-18 13:57:41,475 Command to send: c
o830
name
e

2024-06-18 13:57:41,475 Answer received: !yspredictionCol
2024-06-18 13:57:41,475 Command to send: c
o831
name
e

2024-06-18 13:57:41,475 Answer received: !ysregParam
2024-06-18 13:57:41,475 Command to send: c
o832
name
e

2024-06-18 13:57:41,476 Answer received: !yssolver
2024-06-18 13:57:41,476 Command to send: c
o833
name
e

2024-06-18 13:57:41,476 Answer received: !ystol
2024-06-18 13:57:41,476 Command to send: c
o834
name
e

2024-06-18 13:57:41,477 Answer received: !ysvariancePower
2024-06-18 13:57:41,477 Command to send: c
o835
name
e

2024-06-18 13:57:41,477 Answer received: !ysweightCol
2024-06-18 13:57:41,477 Command to send: c
o818
uid
e

2024-06-18 13:57:41,478 Answer received: !ysGeneralizedLinearRegression_11f9ba4caa8a
2024-06-18 13:57:41,479 Command to send: c
o818
getParam
saggregationDepth
e

2024-06-18 13:57:41,480 Answer received: !yro836
2024-06-18 13:57:41,480 Command to send: c
o836
w
i2
e

2024-06-18 13:57:41,480 Answer received: !yro837
2024-06-18 13:57:41,480 Command to send: c
o818
getParam
sfamily
e

2024-06-18 13:57:41,481 Answer received: !yro838
2024-06-18 13:57:41,481 Command to send: c
o838
w
sgaussian
e

2024-06-18 13:57:41,481 Answer received: !yro839
2024-06-18 13:57:41,481 Command to send: c
o818
set
ro839
e

2024-06-18 13:57:41,482 Answer received: !yro840
2024-06-18 13:57:41,482 Command to send: c
o818
getParam
sfamily
e

2024-06-18 13:57:41,482 Answer received: !yro841
2024-06-18 13:57:41,483 Command to send: c
o841
w
sgaussian
e

2024-06-18 13:57:41,483 Answer received: !yro842
2024-06-18 13:57:41,483 Command to send: c
o818
getParam
sfeaturesCol
e

2024-06-18 13:57:41,483 Answer received: !yro843
2024-06-18 13:57:41,484 Command to send: c
o843
w
sfeatures
e

2024-06-18 13:57:41,484 Answer received: !yro844
2024-06-18 13:57:41,484 Command to send: c
o818
getParam
sfitIntercept
e

2024-06-18 13:57:41,484 Answer received: !yro845
2024-06-18 13:57:41,485 Command to send: c
o845
w
bTrue
e

2024-06-18 13:57:41,485 Answer received: !yro846
2024-06-18 13:57:41,485 Command to send: c
o818
getParam
slabelCol
e

2024-06-18 13:57:41,485 Answer received: !yro847
2024-06-18 13:57:41,486 Command to send: c
o847
w
sprice
e

2024-06-18 13:57:41,486 Answer received: !yro848
2024-06-18 13:57:41,486 Command to send: c
o818
set
ro848
e

2024-06-18 13:57:41,486 Answer received: !yro849
2024-06-18 13:57:41,487 Command to send: c
o818
getParam
slabelCol
e

2024-06-18 13:57:41,487 Answer received: !yro850
2024-06-18 13:57:41,488 Command to send: c
o850
w
slabel
e

2024-06-18 13:57:41,488 Answer received: !yro851
2024-06-18 13:57:41,488 Command to send: c
o818
getParam
slink
e

2024-06-18 13:57:41,488 Answer received: !yro852
2024-06-18 13:57:41,489 Command to send: c
o852
w
sidentity
e

2024-06-18 13:57:41,489 Answer received: !yro853
2024-06-18 13:57:41,489 Command to send: c
o818
set
ro853
e

2024-06-18 13:57:41,490 Answer received: !yro854
2024-06-18 13:57:41,490 Command to send: c
o818
getParam
smaxIter
e

2024-06-18 13:57:41,490 Answer received: !yro855
2024-06-18 13:57:41,490 Command to send: c
o855
w
i10
e

2024-06-18 13:57:41,491 Answer received: !yro856
2024-06-18 13:57:41,491 Command to send: c
o818
set
ro856
e

2024-06-18 13:57:41,491 Answer received: !yro857
2024-06-18 13:57:41,491 Command to send: c
o818
getParam
smaxIter
e

2024-06-18 13:57:41,492 Answer received: !yro858
2024-06-18 13:57:41,492 Command to send: c
o858
w
i25
e

2024-06-18 13:57:41,492 Answer received: !yro859
2024-06-18 13:57:41,492 Command to send: c
o818
getParam
spredictionCol
e

2024-06-18 13:57:41,493 Answer received: !yro860
2024-06-18 13:57:41,493 Command to send: c
o860
w
sprediction
e

2024-06-18 13:57:41,493 Answer received: !yro861
2024-06-18 13:57:41,493 Command to send: c
o818
getParam
sregParam
e

2024-06-18 13:57:41,494 Answer received: !yro862
2024-06-18 13:57:41,494 Command to send: c
o862
w
d0.3
e

2024-06-18 13:57:41,494 Answer received: !yro863
2024-06-18 13:57:41,494 Command to send: c
o818
set
ro863
e

2024-06-18 13:57:41,495 Answer received: !yro864
2024-06-18 13:57:41,495 Command to send: c
o818
getParam
sregParam
e

2024-06-18 13:57:41,495 Answer received: !yro865
2024-06-18 13:57:41,496 Command to send: c
o865
w
d0.0
e

2024-06-18 13:57:41,496 Answer received: !yro866
2024-06-18 13:57:41,496 Command to send: c
o818
getParam
ssolver
e

2024-06-18 13:57:41,497 Answer received: !yro867
2024-06-18 13:57:41,497 Command to send: c
o867
w
sirls
e

2024-06-18 13:57:41,497 Answer received: !yro868
2024-06-18 13:57:41,497 Command to send: c
o818
getParam
stol
e

2024-06-18 13:57:41,498 Answer received: !yro869
2024-06-18 13:57:41,498 Command to send: c
o869
w
d1e-06
e

2024-06-18 13:57:41,498 Answer received: !yro870
2024-06-18 13:57:41,498 Command to send: c
o818
getParam
svariancePower
e

2024-06-18 13:57:41,499 Answer received: !yro871
2024-06-18 13:57:41,499 Command to send: c
o871
w
d0.0
e

2024-06-18 13:57:41,499 Answer received: !yro872
2024-06-18 13:57:41,499 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:41,500 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:41,500 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:41,501 Answer received: !ym
2024-06-18 13:57:41,501 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:41,501 Answer received: !ylo873
2024-06-18 13:57:41,501 Command to send: c
o873
add
ro837
e

2024-06-18 13:57:41,502 Answer received: !ybtrue
2024-06-18 13:57:41,502 Command to send: c
o873
add
ro842
e

2024-06-18 13:57:41,502 Answer received: !ybtrue
2024-06-18 13:57:41,502 Command to send: c
o873
add
ro844
e

2024-06-18 13:57:41,503 Answer received: !ybtrue
2024-06-18 13:57:41,503 Command to send: c
o873
add
ro846
e

2024-06-18 13:57:41,503 Answer received: !ybtrue
2024-06-18 13:57:41,503 Command to send: c
o873
add
ro851
e

2024-06-18 13:57:41,504 Answer received: !ybtrue
2024-06-18 13:57:41,504 Command to send: c
o873
add
ro859
e

2024-06-18 13:57:41,504 Answer received: !ybtrue
2024-06-18 13:57:41,504 Command to send: c
o873
add
ro861
e

2024-06-18 13:57:41,505 Answer received: !ybtrue
2024-06-18 13:57:41,505 Command to send: c
o873
add
ro866
e

2024-06-18 13:57:41,506 Answer received: !ybtrue
2024-06-18 13:57:41,506 Command to send: c
o873
add
ro868
e

2024-06-18 13:57:41,506 Answer received: !ybtrue
2024-06-18 13:57:41,506 Command to send: c
o873
add
ro870
e

2024-06-18 13:57:41,507 Answer received: !ybtrue
2024-06-18 13:57:41,507 Command to send: c
o873
add
ro872
e

2024-06-18 13:57:41,507 Answer received: !ybtrue
2024-06-18 13:57:41,507 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro873
e

2024-06-18 13:57:41,507 Answer received: !yro874
2024-06-18 13:57:41,508 Command to send: c
o818
setDefault
ro874
e

2024-06-18 13:57:41,508 Answer received: !yro875
2024-06-18 13:57:41,508 Command to send: c
o818
transform
ro775
e

2024-06-18 13:57:41,533 Answer received: !yro876
2024-06-18 13:57:42,098 Command to send: m
d
o750
e

2024-06-18 13:57:42,098 Answer received: !yv
2024-06-18 13:57:42,098 Command to send: m
d
o751
e

2024-06-18 13:57:42,098 Answer received: !yv
2024-06-18 13:57:42,099 Command to send: m
d
o752
e

2024-06-18 13:57:42,099 Answer received: !yv
2024-06-18 13:57:42,099 Command to send: m
d
o754
e

2024-06-18 13:57:42,099 Answer received: !yv
2024-06-18 13:57:42,099 Command to send: m
d
o755
e

2024-06-18 13:57:42,101 Answer received: !yv
2024-06-18 13:57:42,101 Command to send: m
d
o757
e

2024-06-18 13:57:42,101 Answer received: !yv
2024-06-18 13:57:42,101 Command to send: m
d
o758
e

2024-06-18 13:57:42,101 Answer received: !yv
2024-06-18 13:57:42,101 Command to send: m
d
o759
e

2024-06-18 13:57:42,102 Answer received: !yv
2024-06-18 13:57:42,102 Command to send: m
d
o761
e

2024-06-18 13:57:42,102 Answer received: !yv
2024-06-18 13:57:42,102 Command to send: m
d
o763
e

2024-06-18 13:57:42,102 Answer received: !yv
2024-06-18 13:57:42,102 Command to send: m
d
o764
e

2024-06-18 13:57:42,102 Answer received: !yv
2024-06-18 13:57:42,102 Command to send: m
d
o766
e

2024-06-18 13:57:42,102 Answer received: !yv
2024-06-18 13:57:42,102 Command to send: m
d
o767
e

2024-06-18 13:57:42,103 Answer received: !yv
2024-06-18 13:57:42,103 Command to send: m
d
o769
e

2024-06-18 13:57:42,103 Answer received: !yv
2024-06-18 13:57:42,103 Command to send: m
d
o773
e

2024-06-18 13:57:42,103 Answer received: !yv
2024-06-18 13:57:42,105 Command to send: m
d
o778
e

2024-06-18 13:57:42,105 Answer received: !yv
2024-06-18 13:57:42,105 Command to send: m
d
o779
e

2024-06-18 13:57:42,105 Answer received: !yv
2024-06-18 13:57:42,105 Command to send: m
d
o780
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,106 Command to send: m
d
o781
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,106 Command to send: m
d
o783
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,106 Command to send: m
d
o784
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,106 Command to send: m
d
o785
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,106 Command to send: m
d
o786
e

2024-06-18 13:57:42,106 Answer received: !yv
2024-06-18 13:57:42,107 Command to send: m
d
o787
e

2024-06-18 13:57:42,107 Answer received: !yv
2024-06-18 13:57:42,107 Command to send: m
d
o788
e

2024-06-18 13:57:42,107 Answer received: !yv
2024-06-18 13:57:42,107 Command to send: m
d
o789
e

2024-06-18 13:57:42,108 Answer received: !yv
2024-06-18 13:57:42,108 Command to send: m
d
o790
e

2024-06-18 13:57:42,108 Answer received: !yv
2024-06-18 13:57:42,108 Command to send: m
d
o792
e

2024-06-18 13:57:42,108 Answer received: !yv
2024-06-18 13:57:42,109 Command to send: m
d
o793
e

2024-06-18 13:57:42,109 Answer received: !yv
2024-06-18 13:57:42,109 Command to send: m
d
o794
e

2024-06-18 13:57:42,109 Answer received: !yv
2024-06-18 13:57:42,109 Command to send: m
d
o795
e

2024-06-18 13:57:42,110 Answer received: !yv
2024-06-18 13:57:42,110 Command to send: m
d
o797
e

2024-06-18 13:57:42,110 Answer received: !yv
2024-06-18 13:57:42,110 Command to send: m
d
o798
e

2024-06-18 13:57:42,110 Answer received: !yv
2024-06-18 13:57:42,110 Command to send: m
d
o800
e

2024-06-18 13:57:42,111 Answer received: !yv
2024-06-18 13:57:42,111 Command to send: m
d
o801
e

2024-06-18 13:57:42,111 Answer received: !yv
2024-06-18 13:57:42,111 Command to send: m
d
o802
e

2024-06-18 13:57:42,111 Answer received: !yv
2024-06-18 13:57:42,112 Command to send: m
d
o803
e

2024-06-18 13:57:42,112 Answer received: !yv
2024-06-18 13:57:42,112 Command to send: m
d
o804
e

2024-06-18 13:57:42,112 Answer received: !yv
2024-06-18 13:57:42,112 Command to send: m
d
o805
e

2024-06-18 13:57:42,113 Answer received: !yv
2024-06-18 13:57:42,113 Command to send: m
d
o807
e

2024-06-18 13:57:42,113 Answer received: !yv
2024-06-18 13:57:42,113 Command to send: m
d
o808
e

2024-06-18 13:57:42,113 Answer received: !yv
2024-06-18 13:57:42,113 Command to send: m
d
o809
e

2024-06-18 13:57:42,114 Answer received: !yv
2024-06-18 13:57:42,114 Command to send: m
d
o810
e

2024-06-18 13:57:42,114 Answer received: !yv
2024-06-18 13:57:42,114 Command to send: m
d
o811
e

2024-06-18 13:57:42,114 Answer received: !yv
2024-06-18 13:57:42,115 Command to send: m
d
o812
e

2024-06-18 13:57:42,115 Answer received: !yv
2024-06-18 13:57:42,115 Command to send: m
d
o813
e

2024-06-18 13:57:42,115 Answer received: !yv
2024-06-18 13:57:42,115 Command to send: m
d
o814
e

2024-06-18 13:57:42,115 Answer received: !yv
2024-06-18 13:57:42,116 Command to send: m
d
o816
e

2024-06-18 13:57:42,116 Answer received: !yv
2024-06-18 13:57:42,116 Command to send: m
d
o819
e

2024-06-18 13:57:42,116 Answer received: !yv
2024-06-18 13:57:42,116 Command to send: m
d
o840
e

2024-06-18 13:57:42,117 Answer received: !yv
2024-06-18 13:57:42,117 Command to send: m
d
o849
e

2024-06-18 13:57:42,117 Answer received: !yv
2024-06-18 13:57:42,117 Command to send: m
d
o854
e

2024-06-18 13:57:42,117 Answer received: !yv
2024-06-18 13:57:42,117 Command to send: m
d
o857
e

2024-06-18 13:57:42,118 Answer received: !yv
2024-06-18 13:57:42,118 Command to send: m
d
o864
e

2024-06-18 13:57:42,118 Answer received: !yv
2024-06-18 13:57:42,118 Command to send: m
d
o873
e

2024-06-18 13:57:42,118 Answer received: !yv
2024-06-18 13:57:42,118 Command to send: m
d
o875
e

2024-06-18 13:57:42,119 Answer received: !yv
2024-06-18 13:57:43,540 Command to send: r
u
org
rj
e

2024-06-18 13:57:43,542 Answer received: !yp
2024-06-18 13:57:43,542 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:43,543 Answer received: !yp
2024-06-18 13:57:43,543 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:43,544 Answer received: !yp
2024-06-18 13:57:43,544 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:57:43,545 Answer received: !yp
2024-06-18 13:57:43,545 Command to send: r
u
org.apache.spark.ml.evaluation
rj
e

2024-06-18 13:57:43,545 Answer received: !yp
2024-06-18 13:57:43,546 Command to send: r
u
org.apache.spark.ml.evaluation.RegressionEvaluator
rj
e

2024-06-18 13:57:43,557 Answer received: !ycorg.apache.spark.ml.evaluation.RegressionEvaluator
2024-06-18 13:57:43,557 Command to send: i
org.apache.spark.ml.evaluation.RegressionEvaluator
sRegressionEvaluator_0acb236e1775
e

2024-06-18 13:57:43,557 Answer received: !yro877
2024-06-18 13:57:43,558 Command to send: c
o877
getParam
slabelCol
e

2024-06-18 13:57:43,592 Answer received: !yro878
2024-06-18 13:57:43,592 Command to send: c
o878
w
sprice
e

2024-06-18 13:57:43,593 Answer received: !yro879
2024-06-18 13:57:43,593 Command to send: c
o877
set
ro879
e

2024-06-18 13:57:43,593 Answer received: !yro880
2024-06-18 13:57:43,594 Command to send: c
o877
getParam
slabelCol
e

2024-06-18 13:57:43,594 Answer received: !yro881
2024-06-18 13:57:43,594 Command to send: c
o881
w
slabel
e

2024-06-18 13:57:43,594 Answer received: !yro882
2024-06-18 13:57:43,595 Command to send: c
o877
getParam
smetricName
e

2024-06-18 13:57:43,595 Answer received: !yro883
2024-06-18 13:57:43,595 Command to send: c
o883
w
sr2
e

2024-06-18 13:57:43,595 Answer received: !yro884
2024-06-18 13:57:43,596 Command to send: c
o877
set
ro884
e

2024-06-18 13:57:43,596 Answer received: !yro885
2024-06-18 13:57:43,596 Command to send: c
o877
getParam
smetricName
e

2024-06-18 13:57:43,596 Answer received: !yro886
2024-06-18 13:57:43,597 Command to send: c
o886
w
srmse
e

2024-06-18 13:57:43,597 Answer received: !yro887
2024-06-18 13:57:43,597 Command to send: c
o877
getParam
spredictionCol
e

2024-06-18 13:57:43,597 Answer received: !yro888
2024-06-18 13:57:43,598 Command to send: c
o888
w
sprediction
e

2024-06-18 13:57:43,598 Answer received: !yro889
2024-06-18 13:57:43,598 Command to send: c
o877
getParam
sthroughOrigin
e

2024-06-18 13:57:43,598 Answer received: !yro890
2024-06-18 13:57:43,599 Command to send: c
o890
w
bFalse
e

2024-06-18 13:57:43,599 Answer received: !yro891
2024-06-18 13:57:43,599 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:43,600 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:43,600 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:43,600 Answer received: !ym
2024-06-18 13:57:43,601 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:43,601 Answer received: !ylo892
2024-06-18 13:57:43,601 Command to send: c
o892
add
ro882
e

2024-06-18 13:57:43,601 Answer received: !ybtrue
2024-06-18 13:57:43,602 Command to send: c
o892
add
ro887
e

2024-06-18 13:57:43,602 Answer received: !ybtrue
2024-06-18 13:57:43,602 Command to send: c
o892
add
ro889
e

2024-06-18 13:57:43,602 Answer received: !ybtrue
2024-06-18 13:57:43,602 Command to send: c
o892
add
ro891
e

2024-06-18 13:57:43,603 Answer received: !ybtrue
2024-06-18 13:57:43,603 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro892
e

2024-06-18 13:57:43,603 Answer received: !yro893
2024-06-18 13:57:43,603 Command to send: c
o877
setDefault
ro893
e

2024-06-18 13:57:43,604 Answer received: !yro894
2024-06-18 13:57:43,604 Command to send: c
o877
evaluate
ro876
e

2024-06-18 13:57:44,120 Command to send: m
d
o880
e

2024-06-18 13:57:44,121 Answer received: !yv
2024-06-18 13:57:44,121 Command to send: m
d
o885
e

2024-06-18 13:57:44,122 Answer received: !yv
2024-06-18 13:57:44,122 Command to send: m
d
o892
e

2024-06-18 13:57:44,122 Answer received: !yv
2024-06-18 13:57:44,122 Command to send: m
d
o894
e

2024-06-18 13:57:44,122 Answer received: !yv
2024-06-18 13:57:45,113 Answer received: !yd0.7444120529989751
2024-06-18 13:57:45,114 Command to send: r
u
org
rj
e

2024-06-18 13:57:45,116 Answer received: !yp
2024-06-18 13:57:45,116 Command to send: r
u
org.apache
rj
e

2024-06-18 13:57:45,116 Answer received: !yp
2024-06-18 13:57:45,116 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:57:45,117 Answer received: !yp
2024-06-18 13:57:45,117 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:57:45,118 Answer received: !yp
2024-06-18 13:57:45,118 Command to send: r
u
org.apache.spark.ml.evaluation
rj
e

2024-06-18 13:57:45,118 Answer received: !yp
2024-06-18 13:57:45,118 Command to send: r
u
org.apache.spark.ml.evaluation.RegressionEvaluator
rj
e

2024-06-18 13:57:45,119 Answer received: !ycorg.apache.spark.ml.evaluation.RegressionEvaluator
2024-06-18 13:57:45,119 Command to send: i
org.apache.spark.ml.evaluation.RegressionEvaluator
sRegressionEvaluator_77bdc4b71a44
e

2024-06-18 13:57:45,119 Answer received: !yro895
2024-06-18 13:57:45,120 Command to send: c
o895
getParam
slabelCol
e

2024-06-18 13:57:45,121 Answer received: !yro896
2024-06-18 13:57:45,121 Command to send: c
o896
w
sprice
e

2024-06-18 13:57:45,121 Answer received: !yro897
2024-06-18 13:57:45,121 Command to send: c
o895
set
ro897
e

2024-06-18 13:57:45,122 Answer received: !yro898
2024-06-18 13:57:45,122 Command to send: c
o895
getParam
slabelCol
e

2024-06-18 13:57:45,122 Answer received: !yro899
2024-06-18 13:57:45,122 Command to send: c
o899
w
slabel
e

2024-06-18 13:57:45,123 Answer received: !yro900
2024-06-18 13:57:45,123 Command to send: c
o895
getParam
smetricName
e

2024-06-18 13:57:45,123 Answer received: !yro901
2024-06-18 13:57:45,123 Command to send: c
o901
w
srmse
e

2024-06-18 13:57:45,123 Answer received: !yro902
2024-06-18 13:57:45,124 Command to send: c
o895
set
ro902
e

2024-06-18 13:57:45,124 Command to send: m
d
o877
e

2024-06-18 13:57:45,124 Answer received: !yro903
2024-06-18 13:57:45,124 Command to send: c
o895
getParam
smetricName
e

2024-06-18 13:57:45,124 Answer received: !yv
2024-06-18 13:57:45,124 Command to send: m
d
o898
e

2024-06-18 13:57:45,125 Answer received: !yro904
2024-06-18 13:57:45,125 Command to send: c
o904
w
srmse
e

2024-06-18 13:57:45,125 Answer received: !yv
2024-06-18 13:57:45,125 Command to send: m
d
o903
e

2024-06-18 13:57:45,125 Answer received: !yro905
2024-06-18 13:57:45,125 Command to send: c
o895
getParam
spredictionCol
e

2024-06-18 13:57:45,126 Answer received: !yv
2024-06-18 13:57:45,126 Answer received: !yro906
2024-06-18 13:57:45,126 Command to send: c
o906
w
sprediction
e

2024-06-18 13:57:45,126 Answer received: !yro907
2024-06-18 13:57:45,126 Command to send: c
o895
getParam
sthroughOrigin
e

2024-06-18 13:57:45,127 Answer received: !yro908
2024-06-18 13:57:45,127 Command to send: c
o908
w
bFalse
e

2024-06-18 13:57:45,127 Answer received: !yro909
2024-06-18 13:57:45,128 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:57:45,129 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:57:45,129 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:57:45,129 Answer received: !ym
2024-06-18 13:57:45,129 Command to send: i
java.util.ArrayList
e

2024-06-18 13:57:45,130 Answer received: !ylo910
2024-06-18 13:57:45,130 Command to send: c
o910
add
ro900
e

2024-06-18 13:57:45,130 Answer received: !ybtrue
2024-06-18 13:57:45,130 Command to send: c
o910
add
ro905
e

2024-06-18 13:57:45,131 Answer received: !ybtrue
2024-06-18 13:57:45,131 Command to send: c
o910
add
ro907
e

2024-06-18 13:57:45,131 Answer received: !ybtrue
2024-06-18 13:57:45,131 Command to send: c
o910
add
ro909
e

2024-06-18 13:57:45,131 Answer received: !ybtrue
2024-06-18 13:57:45,132 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro910
e

2024-06-18 13:57:45,132 Answer received: !yro911
2024-06-18 13:57:45,132 Command to send: c
o895
setDefault
ro911
e

2024-06-18 13:57:45,132 Answer received: !yro912
2024-06-18 13:57:45,133 Command to send: c
o895
evaluate
ro876
e

2024-06-18 13:57:45,938 Answer received: !yd211984.0575014574
2024-06-18 13:57:46,126 Command to send: m
d
o910
e

2024-06-18 13:57:46,126 Answer received: !yv
2024-06-18 13:57:46,126 Command to send: m
d
o912
e

2024-06-18 13:57:46,126 Answer received: !yv
2024-06-18 13:58:53,421 Command to send: r
u
org
rj
e

2024-06-18 13:58:55,065 Answer received: !yp
2024-06-18 13:58:55,065 Command to send: r
u
org.apache
rj
e

2024-06-18 13:58:55,066 Answer received: !yp
2024-06-18 13:58:55,066 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:58:55,066 Answer received: !yp
2024-06-18 13:58:55,066 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:58:55,067 Answer received: !yp
2024-06-18 13:58:55,067 Command to send: r
u
org.apache.spark.ml.feature
rj
e

2024-06-18 13:58:55,067 Answer received: !yp
2024-06-18 13:58:55,067 Command to send: r
u
org.apache.spark.ml.feature.VectorAssembler
rj
e

2024-06-18 13:58:55,067 Answer received: !ycorg.apache.spark.ml.feature.VectorAssembler
2024-06-18 13:58:55,068 Command to send: i
org.apache.spark.ml.feature.VectorAssembler
sVectorAssembler_a528efc65366
e

2024-06-18 13:58:55,069 Answer received: !yro913
2024-06-18 13:58:55,070 Command to send: c
o913
getParam
shandleInvalid
e

2024-06-18 13:58:55,071 Answer received: !yro914
2024-06-18 13:58:55,071 Command to send: c
o914
w
sskip
e

2024-06-18 13:58:55,072 Answer received: !yro915
2024-06-18 13:58:55,072 Command to send: c
o913
set
ro915
e

2024-06-18 13:58:55,072 Answer received: !yro916
2024-06-18 13:58:55,072 Command to send: c
o913
getParam
shandleInvalid
e

2024-06-18 13:58:55,073 Answer received: !yro917
2024-06-18 13:58:55,073 Command to send: c
o917
w
serror
e

2024-06-18 13:58:55,073 Answer received: !yro918
2024-06-18 13:58:55,073 Command to send: c
o913
getParam
sinputCols
e

2024-06-18 13:58:55,074 Answer received: !yro919
2024-06-18 13:58:55,074 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:55,075 Answer received: !ylo920
2024-06-18 13:58:55,075 Command to send: c
o920
add
sAvg_stars
e

2024-06-18 13:58:55,075 Answer received: !ybtrue
2024-06-18 13:58:55,075 Command to send: c
o920
add
sN_hotels
e

2024-06-18 13:58:55,076 Answer received: !ybtrue
2024-06-18 13:58:55,076 Command to send: c
o920
add
sAvg_lat
e

2024-06-18 13:58:55,076 Answer received: !ybtrue
2024-06-18 13:58:55,076 Command to send: c
o920
add
sAvg_lat
e

2024-06-18 13:58:55,077 Answer received: !ybtrue
2024-06-18 13:58:55,077 Command to send: c
o920
add
snumPhotos
e

2024-06-18 13:58:55,077 Answer received: !ybtrue
2024-06-18 13:58:55,078 Command to send: c
o920
add
sfloor
e

2024-06-18 13:58:55,078 Answer received: !ybtrue
2024-06-18 13:58:55,078 Command to send: c
o920
add
ssize
e

2024-06-18 13:58:55,079 Answer received: !ybtrue
2024-06-18 13:58:55,079 Command to send: c
o920
add
sAvg_Index_RFD
e

2024-06-18 13:58:55,080 Answer received: !ybtrue
2024-06-18 13:58:55,080 Command to send: c
o920
add
snumPhotos
e

2024-06-18 13:58:55,080 Answer received: !ybtrue
2024-06-18 13:58:55,080 Command to send: c
o920
add
sbathrooms
e

2024-06-18 13:58:55,080 Answer received: !ybtrue
2024-06-18 13:58:55,081 Command to send: c
o920
add
sAvg_Index_RFD
e

2024-06-18 13:58:55,081 Answer received: !ybtrue
2024-06-18 13:58:55,081 Command to send: c
o919
w
ro920
e

2024-06-18 13:58:55,081 Answer received: !yro921
2024-06-18 13:58:55,082 Command to send: c
o913
set
ro921
e

2024-06-18 13:58:55,082 Answer received: !yro922
2024-06-18 13:58:55,082 Command to send: c
o913
getParam
soutputCol
e

2024-06-18 13:58:55,082 Answer received: !yro923
2024-06-18 13:58:55,083 Command to send: c
o923
w
sfeatures
e

2024-06-18 13:58:55,083 Answer received: !yro924
2024-06-18 13:58:55,083 Command to send: c
o913
set
ro924
e

2024-06-18 13:58:55,083 Answer received: !yro925
2024-06-18 13:58:55,084 Command to send: c
o913
getParam
soutputCol
e

2024-06-18 13:58:55,084 Answer received: !yro926
2024-06-18 13:58:55,084 Command to send: c
o926
w
sVectorAssembler_a528efc65366__output
e

2024-06-18 13:58:55,085 Answer received: !yro927
2024-06-18 13:58:55,085 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:58:55,086 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:58:55,087 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:58:55,087 Answer received: !ym
2024-06-18 13:58:55,088 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:55,088 Answer received: !ylo928
2024-06-18 13:58:55,088 Command to send: c
o928
add
ro918
e

2024-06-18 13:58:55,089 Answer received: !ybtrue
2024-06-18 13:58:55,089 Command to send: c
o928
add
ro927
e

2024-06-18 13:58:55,089 Answer received: !ybtrue
2024-06-18 13:58:55,089 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro928
e

2024-06-18 13:58:55,089 Answer received: !yro929
2024-06-18 13:58:55,090 Command to send: c
o913
setDefault
ro929
e

2024-06-18 13:58:55,090 Answer received: !yro930
2024-06-18 13:58:55,090 Command to send: c
o913
transform
ro726
e

2024-06-18 13:58:55,158 Answer received: !yro931
2024-06-18 13:58:55,159 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:58:55,160 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:58:55,160 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toList
e

2024-06-18 13:58:55,161 Answer received: !ym
2024-06-18 13:58:55,161 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:55,161 Answer received: !ylo932
2024-06-18 13:58:55,161 Command to send: c
o932
add
d0.7
e

2024-06-18 13:58:55,162 Answer received: !ybtrue
2024-06-18 13:58:55,162 Command to send: c
o932
add
d0.3
e

2024-06-18 13:58:55,162 Answer received: !ybtrue
2024-06-18 13:58:55,162 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toList
ro932
e

2024-06-18 13:58:55,163 Answer received: !yro933
2024-06-18 13:58:55,163 Command to send: c
o931
randomSplit
ro933
i123
e

2024-06-18 13:58:55,164 Command to send: m
d
o753
e

2024-06-18 13:58:55,164 Answer received: !yv
2024-06-18 13:58:55,164 Command to send: m
d
o916
e

2024-06-18 13:58:55,164 Answer received: !yv
2024-06-18 13:58:55,164 Command to send: m
d
o920
e

2024-06-18 13:58:55,165 Answer received: !yv
2024-06-18 13:58:55,165 Command to send: m
d
o922
e

2024-06-18 13:58:55,165 Answer received: !yv
2024-06-18 13:58:55,165 Command to send: m
d
o925
e

2024-06-18 13:58:55,165 Answer received: !yv
2024-06-18 13:58:55,165 Command to send: m
d
o928
e

2024-06-18 13:58:55,166 Answer received: !yv
2024-06-18 13:58:55,166 Command to send: m
d
o930
e

2024-06-18 13:58:55,166 Answer received: !yv
2024-06-18 13:58:55,166 Command to send: m
d
o932
e

2024-06-18 13:58:55,166 Answer received: !yv
2024-06-18 13:58:55,170 Answer received: !yto934
2024-06-18 13:58:55,170 Command to send: a
e
o934
e

2024-06-18 13:58:55,171 Answer received: !yi2
2024-06-18 13:58:55,171 Command to send: a
g
o934
i0
e

2024-06-18 13:58:55,171 Answer received: !yro935
2024-06-18 13:58:55,171 Command to send: a
e
o934
e

2024-06-18 13:58:55,171 Answer received: !yi2
2024-06-18 13:58:55,171 Command to send: a
g
o934
i1
e

2024-06-18 13:58:55,172 Answer received: !yro936
2024-06-18 13:58:55,172 Command to send: a
e
o934
e

2024-06-18 13:58:55,172 Answer received: !yi2
2024-06-18 13:58:55,172 Command to send: c
o935
count
e

2024-06-18 13:58:56,167 Command to send: m
d
o934
e

2024-06-18 13:58:56,167 Answer received: !yv
2024-06-18 13:58:56,326 Answer received: !yL38520
2024-06-18 13:58:56,327 Command to send: c
o936
count
e

2024-06-18 13:58:57,456 Answer received: !yL16720
2024-06-18 13:58:57,457 Command to send: r
u
org
rj
e

2024-06-18 13:58:57,460 Answer received: !yp
2024-06-18 13:58:57,460 Command to send: r
u
org.apache
rj
e

2024-06-18 13:58:57,461 Answer received: !yp
2024-06-18 13:58:57,461 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:58:57,461 Answer received: !yp
2024-06-18 13:58:57,461 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:58:57,462 Answer received: !yp
2024-06-18 13:58:57,462 Command to send: r
u
org.apache.spark.ml.regression
rj
e

2024-06-18 13:58:57,462 Answer received: !yp
2024-06-18 13:58:57,463 Command to send: r
u
org.apache.spark.ml.regression.GeneralizedLinearRegression
rj
e

2024-06-18 13:58:57,463 Answer received: !ycorg.apache.spark.ml.regression.GeneralizedLinearRegression
2024-06-18 13:58:57,463 Command to send: i
org.apache.spark.ml.regression.GeneralizedLinearRegression
sGeneralizedLinearRegression_63879d505356
e

2024-06-18 13:58:57,464 Answer received: !yro937
2024-06-18 13:58:57,466 Command to send: c
o937
getParam
saggregationDepth
e

2024-06-18 13:58:57,466 Answer received: !yro938
2024-06-18 13:58:57,467 Command to send: c
o938
w
i2
e

2024-06-18 13:58:57,467 Answer received: !yro939
2024-06-18 13:58:57,468 Command to send: c
o937
getParam
sfamily
e

2024-06-18 13:58:57,468 Answer received: !yro940
2024-06-18 13:58:57,469 Command to send: c
o940
w
sgaussian
e

2024-06-18 13:58:57,469 Answer received: !yro941
2024-06-18 13:58:57,470 Command to send: c
o937
set
ro941
e

2024-06-18 13:58:57,470 Answer received: !yro942
2024-06-18 13:58:57,470 Command to send: c
o937
getParam
sfamily
e

2024-06-18 13:58:57,470 Answer received: !yro943
2024-06-18 13:58:57,471 Command to send: c
o943
w
sgaussian
e

2024-06-18 13:58:57,471 Answer received: !yro944
2024-06-18 13:58:57,471 Command to send: c
o937
getParam
sfeaturesCol
e

2024-06-18 13:58:57,471 Answer received: !yro945
2024-06-18 13:58:57,472 Command to send: c
o945
w
sfeatures
e

2024-06-18 13:58:57,472 Answer received: !yro946
2024-06-18 13:58:57,472 Command to send: c
o937
getParam
sfitIntercept
e

2024-06-18 13:58:57,472 Answer received: !yro947
2024-06-18 13:58:57,472 Command to send: c
o947
w
bTrue
e

2024-06-18 13:58:57,473 Answer received: !yro948
2024-06-18 13:58:57,473 Command to send: c
o937
getParam
slabelCol
e

2024-06-18 13:58:57,473 Answer received: !yro949
2024-06-18 13:58:57,473 Command to send: c
o949
w
sprice
e

2024-06-18 13:58:57,474 Answer received: !yro950
2024-06-18 13:58:57,474 Command to send: c
o937
set
ro950
e

2024-06-18 13:58:57,474 Answer received: !yro951
2024-06-18 13:58:57,474 Command to send: c
o937
getParam
slabelCol
e

2024-06-18 13:58:57,475 Answer received: !yro952
2024-06-18 13:58:57,475 Command to send: c
o952
w
slabel
e

2024-06-18 13:58:57,475 Answer received: !yro953
2024-06-18 13:58:57,476 Command to send: c
o937
getParam
slink
e

2024-06-18 13:58:57,476 Answer received: !yro954
2024-06-18 13:58:57,477 Command to send: c
o954
w
sidentity
e

2024-06-18 13:58:57,477 Answer received: !yro955
2024-06-18 13:58:57,477 Command to send: c
o937
set
ro955
e

2024-06-18 13:58:57,478 Answer received: !yro956
2024-06-18 13:58:57,478 Command to send: c
o937
getParam
smaxIter
e

2024-06-18 13:58:57,479 Answer received: !yro957
2024-06-18 13:58:57,479 Command to send: c
o957
w
i10
e

2024-06-18 13:58:57,479 Answer received: !yro958
2024-06-18 13:58:57,479 Command to send: c
o937
set
ro958
e

2024-06-18 13:58:57,480 Answer received: !yro959
2024-06-18 13:58:57,480 Command to send: c
o937
getParam
smaxIter
e

2024-06-18 13:58:57,481 Answer received: !yro960
2024-06-18 13:58:57,481 Command to send: c
o960
w
i25
e

2024-06-18 13:58:57,482 Answer received: !yro961
2024-06-18 13:58:57,482 Command to send: c
o937
getParam
spredictionCol
e

2024-06-18 13:58:57,482 Answer received: !yro962
2024-06-18 13:58:57,482 Command to send: c
o962
w
sprediction
e

2024-06-18 13:58:57,483 Answer received: !yro963
2024-06-18 13:58:57,483 Command to send: c
o937
getParam
sregParam
e

2024-06-18 13:58:57,484 Answer received: !yro964
2024-06-18 13:58:57,484 Command to send: c
o964
w
d0.3
e

2024-06-18 13:58:57,484 Answer received: !yro965
2024-06-18 13:58:57,485 Command to send: c
o937
set
ro965
e

2024-06-18 13:58:57,486 Answer received: !yro966
2024-06-18 13:58:57,486 Command to send: c
o937
getParam
sregParam
e

2024-06-18 13:58:57,486 Answer received: !yro967
2024-06-18 13:58:57,487 Command to send: c
o967
w
d0.0
e

2024-06-18 13:58:57,487 Answer received: !yro968
2024-06-18 13:58:57,487 Command to send: c
o937
getParam
ssolver
e

2024-06-18 13:58:57,488 Answer received: !yro969
2024-06-18 13:58:57,489 Command to send: c
o969
w
sirls
e

2024-06-18 13:58:57,489 Answer received: !yro970
2024-06-18 13:58:57,489 Command to send: c
o937
getParam
stol
e

2024-06-18 13:58:57,490 Answer received: !yro971
2024-06-18 13:58:57,490 Command to send: c
o971
w
d1e-06
e

2024-06-18 13:58:57,490 Answer received: !yro972
2024-06-18 13:58:57,490 Command to send: c
o937
getParam
svariancePower
e

2024-06-18 13:58:57,491 Answer received: !yro973
2024-06-18 13:58:57,491 Command to send: c
o973
w
d0.0
e

2024-06-18 13:58:57,491 Answer received: !yro974
2024-06-18 13:58:57,491 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:58:57,493 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:58:57,493 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:58:57,494 Answer received: !ym
2024-06-18 13:58:57,494 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:57,494 Answer received: !ylo975
2024-06-18 13:58:57,495 Command to send: c
o975
add
ro939
e

2024-06-18 13:58:57,495 Answer received: !ybtrue
2024-06-18 13:58:57,495 Command to send: c
o975
add
ro944
e

2024-06-18 13:58:57,496 Answer received: !ybtrue
2024-06-18 13:58:57,496 Command to send: c
o975
add
ro946
e

2024-06-18 13:58:57,496 Answer received: !ybtrue
2024-06-18 13:58:57,496 Command to send: c
o975
add
ro948
e

2024-06-18 13:58:57,497 Answer received: !ybtrue
2024-06-18 13:58:57,497 Command to send: c
o975
add
ro953
e

2024-06-18 13:58:57,497 Answer received: !ybtrue
2024-06-18 13:58:57,497 Command to send: c
o975
add
ro961
e

2024-06-18 13:58:57,498 Answer received: !ybtrue
2024-06-18 13:58:57,498 Command to send: c
o975
add
ro963
e

2024-06-18 13:58:57,498 Answer received: !ybtrue
2024-06-18 13:58:57,498 Command to send: c
o975
add
ro968
e

2024-06-18 13:58:57,498 Answer received: !ybtrue
2024-06-18 13:58:57,499 Command to send: c
o975
add
ro970
e

2024-06-18 13:58:57,499 Answer received: !ybtrue
2024-06-18 13:58:57,499 Command to send: c
o975
add
ro972
e

2024-06-18 13:58:57,499 Answer received: !ybtrue
2024-06-18 13:58:57,499 Command to send: c
o975
add
ro974
e

2024-06-18 13:58:57,500 Answer received: !ybtrue
2024-06-18 13:58:57,500 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro975
e

2024-06-18 13:58:57,500 Answer received: !yro976
2024-06-18 13:58:57,500 Command to send: c
o937
setDefault
ro976
e

2024-06-18 13:58:57,501 Answer received: !yro977
2024-06-18 13:58:57,501 Command to send: c
o937
fit
ro935
e

2024-06-18 13:58:58,168 Command to send: m
d
o777
e

2024-06-18 13:58:58,169 Answer received: !yv
2024-06-18 13:58:58,169 Command to send: m
d
o822
e

2024-06-18 13:58:58,170 Answer received: !yv
2024-06-18 13:58:58,170 Command to send: m
d
o823
e

2024-06-18 13:58:58,183 Answer received: !yv
2024-06-18 13:58:58,184 Command to send: m
d
o824
e

2024-06-18 13:58:58,184 Answer received: !yv
2024-06-18 13:58:58,184 Command to send: m
d
o825
e

2024-06-18 13:58:58,185 Answer received: !yv
2024-06-18 13:58:58,185 Command to send: m
d
o826
e

2024-06-18 13:58:58,186 Answer received: !yv
2024-06-18 13:58:58,186 Command to send: m
d
o827
e

2024-06-18 13:58:58,186 Answer received: !yv
2024-06-18 13:58:58,187 Command to send: m
d
o828
e

2024-06-18 13:58:58,187 Answer received: !yv
2024-06-18 13:58:58,187 Command to send: m
d
o829
e

2024-06-18 13:58:58,187 Answer received: !yv
2024-06-18 13:58:58,188 Command to send: m
d
o830
e

2024-06-18 13:58:58,188 Answer received: !yv
2024-06-18 13:58:58,188 Command to send: m
d
o831
e

2024-06-18 13:58:58,188 Answer received: !yv
2024-06-18 13:58:58,188 Command to send: m
d
o832
e

2024-06-18 13:58:58,189 Answer received: !yv
2024-06-18 13:58:58,189 Command to send: m
d
o833
e

2024-06-18 13:58:58,189 Answer received: !yv
2024-06-18 13:58:58,189 Command to send: m
d
o834
e

2024-06-18 13:58:58,190 Answer received: !yv
2024-06-18 13:58:58,190 Command to send: m
d
o835
e

2024-06-18 13:58:58,190 Answer received: !yv
2024-06-18 13:58:58,190 Command to send: m
d
o836
e

2024-06-18 13:58:58,190 Answer received: !yv
2024-06-18 13:58:58,190 Command to send: m
d
o837
e

2024-06-18 13:58:58,191 Answer received: !yv
2024-06-18 13:58:58,191 Command to send: m
d
o838
e

2024-06-18 13:58:58,191 Answer received: !yv
2024-06-18 13:58:58,191 Command to send: m
d
o839
e

2024-06-18 13:58:58,191 Answer received: !yv
2024-06-18 13:58:58,192 Command to send: m
d
o841
e

2024-06-18 13:58:58,192 Answer received: !yv
2024-06-18 13:58:58,192 Command to send: m
d
o842
e

2024-06-18 13:58:58,192 Answer received: !yv
2024-06-18 13:58:58,193 Command to send: m
d
o843
e

2024-06-18 13:58:58,193 Answer received: !yv
2024-06-18 13:58:58,193 Command to send: m
d
o844
e

2024-06-18 13:58:58,194 Answer received: !yv
2024-06-18 13:58:58,194 Command to send: m
d
o845
e

2024-06-18 13:58:58,194 Answer received: !yv
2024-06-18 13:58:58,194 Command to send: m
d
o846
e

2024-06-18 13:58:58,196 Answer received: !yv
2024-06-18 13:58:58,196 Command to send: m
d
o847
e

2024-06-18 13:58:58,196 Answer received: !yv
2024-06-18 13:58:58,196 Command to send: m
d
o848
e

2024-06-18 13:58:58,197 Answer received: !yv
2024-06-18 13:58:58,197 Command to send: m
d
o850
e

2024-06-18 13:58:58,197 Answer received: !yv
2024-06-18 13:58:58,197 Command to send: m
d
o851
e

2024-06-18 13:58:58,198 Answer received: !yv
2024-06-18 13:58:58,198 Command to send: m
d
o852
e

2024-06-18 13:58:58,198 Answer received: !yv
2024-06-18 13:58:58,198 Command to send: m
d
o853
e

2024-06-18 13:58:58,198 Answer received: !yv
2024-06-18 13:58:58,199 Command to send: m
d
o855
e

2024-06-18 13:58:58,199 Answer received: !yv
2024-06-18 13:58:58,199 Command to send: m
d
o856
e

2024-06-18 13:58:58,199 Answer received: !yv
2024-06-18 13:58:58,199 Command to send: m
d
o858
e

2024-06-18 13:58:58,200 Answer received: !yv
2024-06-18 13:58:58,200 Command to send: m
d
o859
e

2024-06-18 13:58:58,200 Answer received: !yv
2024-06-18 13:58:58,200 Command to send: m
d
o860
e

2024-06-18 13:58:58,200 Answer received: !yv
2024-06-18 13:58:58,201 Command to send: m
d
o861
e

2024-06-18 13:58:58,201 Answer received: !yv
2024-06-18 13:58:58,201 Command to send: m
d
o862
e

2024-06-18 13:58:58,201 Answer received: !yv
2024-06-18 13:58:58,202 Command to send: m
d
o863
e

2024-06-18 13:58:58,202 Answer received: !yv
2024-06-18 13:58:58,202 Command to send: m
d
o865
e

2024-06-18 13:58:58,202 Answer received: !yv
2024-06-18 13:58:58,202 Command to send: m
d
o866
e

2024-06-18 13:58:58,203 Answer received: !yv
2024-06-18 13:58:58,203 Command to send: m
d
o867
e

2024-06-18 13:58:58,203 Answer received: !yv
2024-06-18 13:58:58,203 Command to send: m
d
o868
e

2024-06-18 13:58:58,203 Answer received: !yv
2024-06-18 13:58:58,203 Command to send: m
d
o869
e

2024-06-18 13:58:58,204 Answer received: !yv
2024-06-18 13:58:58,204 Command to send: m
d
o870
e

2024-06-18 13:58:58,204 Answer received: !yv
2024-06-18 13:58:58,204 Command to send: m
d
o871
e

2024-06-18 13:58:58,205 Answer received: !yv
2024-06-18 13:58:58,205 Command to send: m
d
o872
e

2024-06-18 13:58:58,205 Answer received: !yv
2024-06-18 13:58:58,205 Command to send: m
d
o874
e

2024-06-18 13:58:58,205 Answer received: !yv
2024-06-18 13:58:58,205 Command to send: m
d
o878
e

2024-06-18 13:58:58,206 Answer received: !yv
2024-06-18 13:58:58,206 Command to send: m
d
o879
e

2024-06-18 13:58:58,206 Answer received: !yv
2024-06-18 13:58:58,206 Command to send: m
d
o881
e

2024-06-18 13:58:58,206 Answer received: !yv
2024-06-18 13:58:58,207 Command to send: m
d
o882
e

2024-06-18 13:58:58,207 Answer received: !yv
2024-06-18 13:58:58,207 Command to send: m
d
o883
e

2024-06-18 13:58:58,207 Answer received: !yv
2024-06-18 13:58:58,208 Command to send: m
d
o884
e

2024-06-18 13:58:58,208 Answer received: !yv
2024-06-18 13:58:58,208 Command to send: m
d
o886
e

2024-06-18 13:58:58,209 Answer received: !yv
2024-06-18 13:58:58,209 Command to send: m
d
o887
e

2024-06-18 13:58:58,209 Answer received: !yv
2024-06-18 13:58:58,210 Command to send: m
d
o888
e

2024-06-18 13:58:58,210 Answer received: !yv
2024-06-18 13:58:58,210 Command to send: m
d
o889
e

2024-06-18 13:58:58,210 Answer received: !yv
2024-06-18 13:58:58,210 Command to send: m
d
o890
e

2024-06-18 13:58:58,211 Answer received: !yv
2024-06-18 13:58:58,211 Command to send: m
d
o891
e

2024-06-18 13:58:58,211 Answer received: !yv
2024-06-18 13:58:58,211 Command to send: m
d
o893
e

2024-06-18 13:58:58,211 Answer received: !yv
2024-06-18 13:58:58,211 Command to send: m
d
o896
e

2024-06-18 13:58:58,212 Answer received: !yv
2024-06-18 13:58:58,212 Command to send: m
d
o897
e

2024-06-18 13:58:58,212 Answer received: !yv
2024-06-18 13:58:58,212 Command to send: m
d
o899
e

2024-06-18 13:58:58,212 Answer received: !yv
2024-06-18 13:58:58,212 Command to send: m
d
o900
e

2024-06-18 13:58:58,213 Answer received: !yv
2024-06-18 13:58:58,213 Command to send: m
d
o901
e

2024-06-18 13:58:58,213 Answer received: !yv
2024-06-18 13:58:58,213 Command to send: m
d
o902
e

2024-06-18 13:58:58,213 Answer received: !yv
2024-06-18 13:58:58,213 Command to send: m
d
o904
e

2024-06-18 13:58:58,214 Answer received: !yv
2024-06-18 13:58:58,214 Command to send: m
d
o905
e

2024-06-18 13:58:58,214 Answer received: !yv
2024-06-18 13:58:58,214 Command to send: m
d
o906
e

2024-06-18 13:58:58,214 Answer received: !yv
2024-06-18 13:58:58,214 Command to send: m
d
o907
e

2024-06-18 13:58:58,215 Answer received: !yv
2024-06-18 13:58:58,215 Command to send: m
d
o908
e

2024-06-18 13:58:58,215 Answer received: !yv
2024-06-18 13:58:58,215 Command to send: m
d
o909
e

2024-06-18 13:58:58,215 Answer received: !yv
2024-06-18 13:58:58,215 Command to send: m
d
o911
e

2024-06-18 13:58:58,216 Answer received: !yv
2024-06-18 13:58:58,216 Command to send: m
d
o914
e

2024-06-18 13:58:58,216 Answer received: !yv
2024-06-18 13:58:58,216 Command to send: m
d
o915
e

2024-06-18 13:58:58,216 Answer received: !yv
2024-06-18 13:58:58,216 Command to send: m
d
o917
e

2024-06-18 13:58:58,217 Answer received: !yv
2024-06-18 13:58:58,217 Command to send: m
d
o918
e

2024-06-18 13:58:58,217 Answer received: !yv
2024-06-18 13:58:58,217 Command to send: m
d
o919
e

2024-06-18 13:58:58,217 Answer received: !yv
2024-06-18 13:58:58,218 Command to send: m
d
o921
e

2024-06-18 13:58:58,218 Answer received: !yv
2024-06-18 13:58:58,218 Command to send: m
d
o923
e

2024-06-18 13:58:58,218 Answer received: !yv
2024-06-18 13:58:58,218 Command to send: m
d
o924
e

2024-06-18 13:58:58,218 Answer received: !yv
2024-06-18 13:58:58,219 Command to send: m
d
o926
e

2024-06-18 13:58:58,219 Answer received: !yv
2024-06-18 13:58:58,219 Command to send: m
d
o927
e

2024-06-18 13:58:58,219 Answer received: !yv
2024-06-18 13:58:58,219 Command to send: m
d
o929
e

2024-06-18 13:58:58,219 Answer received: !yv
2024-06-18 13:58:58,219 Command to send: m
d
o933
e

2024-06-18 13:58:58,220 Answer received: !yv
2024-06-18 13:58:58,220 Command to send: m
d
o942
e

2024-06-18 13:58:58,220 Answer received: !yv
2024-06-18 13:58:58,220 Command to send: m
d
o951
e

2024-06-18 13:58:58,220 Answer received: !yv
2024-06-18 13:58:58,220 Command to send: m
d
o956
e

2024-06-18 13:58:58,220 Answer received: !yv
2024-06-18 13:58:58,220 Command to send: m
d
o959
e

2024-06-18 13:58:58,221 Answer received: !yv
2024-06-18 13:58:58,221 Command to send: m
d
o966
e

2024-06-18 13:58:58,221 Answer received: !yv
2024-06-18 13:58:58,221 Command to send: m
d
o975
e

2024-06-18 13:58:58,221 Answer received: !yv
2024-06-18 13:58:58,221 Command to send: m
d
o977
e

2024-06-18 13:58:58,222 Answer received: !yv
2024-06-18 13:58:58,979 Answer received: !yro978
2024-06-18 13:58:58,980 Command to send: c
o978
params
e

2024-06-18 13:58:58,980 Answer received: !yto979
2024-06-18 13:58:58,980 Command to send: a
e
o979
e

2024-06-18 13:58:58,981 Answer received: !yi16
2024-06-18 13:58:58,981 Command to send: a
e
o979
e

2024-06-18 13:58:58,981 Answer received: !yi16
2024-06-18 13:58:58,981 Command to send: a
g
o979
i0
e

2024-06-18 13:58:58,982 Answer received: !yro980
2024-06-18 13:58:58,982 Command to send: a
e
o979
e

2024-06-18 13:58:58,982 Answer received: !yi16
2024-06-18 13:58:58,982 Command to send: a
g
o979
i1
e

2024-06-18 13:58:58,982 Answer received: !yro981
2024-06-18 13:58:58,982 Command to send: a
e
o979
e

2024-06-18 13:58:58,983 Answer received: !yi16
2024-06-18 13:58:58,983 Command to send: a
g
o979
i2
e

2024-06-18 13:58:58,983 Answer received: !yro982
2024-06-18 13:58:58,983 Command to send: a
e
o979
e

2024-06-18 13:58:58,983 Answer received: !yi16
2024-06-18 13:58:58,984 Command to send: a
g
o979
i3
e

2024-06-18 13:58:58,984 Answer received: !yro983
2024-06-18 13:58:58,984 Command to send: a
e
o979
e

2024-06-18 13:58:58,984 Answer received: !yi16
2024-06-18 13:58:58,984 Command to send: a
g
o979
i4
e

2024-06-18 13:58:58,985 Answer received: !yro984
2024-06-18 13:58:58,985 Command to send: a
e
o979
e

2024-06-18 13:58:58,985 Answer received: !yi16
2024-06-18 13:58:58,985 Command to send: a
g
o979
i5
e

2024-06-18 13:58:58,985 Answer received: !yro985
2024-06-18 13:58:58,985 Command to send: a
e
o979
e

2024-06-18 13:58:58,986 Answer received: !yi16
2024-06-18 13:58:58,986 Command to send: a
g
o979
i6
e

2024-06-18 13:58:58,986 Answer received: !yro986
2024-06-18 13:58:58,986 Command to send: a
e
o979
e

2024-06-18 13:58:58,986 Answer received: !yi16
2024-06-18 13:58:58,986 Command to send: a
g
o979
i7
e

2024-06-18 13:58:58,987 Answer received: !yro987
2024-06-18 13:58:58,987 Command to send: a
e
o979
e

2024-06-18 13:58:58,987 Answer received: !yi16
2024-06-18 13:58:58,987 Command to send: a
g
o979
i8
e

2024-06-18 13:58:58,987 Answer received: !yro988
2024-06-18 13:58:58,987 Command to send: a
e
o979
e

2024-06-18 13:58:58,988 Answer received: !yi16
2024-06-18 13:58:58,988 Command to send: a
g
o979
i9
e

2024-06-18 13:58:58,988 Answer received: !yro989
2024-06-18 13:58:58,988 Command to send: a
e
o979
e

2024-06-18 13:58:58,988 Answer received: !yi16
2024-06-18 13:58:58,988 Command to send: a
g
o979
i10
e

2024-06-18 13:58:58,989 Answer received: !yro990
2024-06-18 13:58:58,989 Command to send: a
e
o979
e

2024-06-18 13:58:58,989 Answer received: !yi16
2024-06-18 13:58:58,989 Command to send: a
g
o979
i11
e

2024-06-18 13:58:58,989 Answer received: !yro991
2024-06-18 13:58:58,989 Command to send: a
e
o979
e

2024-06-18 13:58:58,989 Answer received: !yi16
2024-06-18 13:58:58,990 Command to send: a
g
o979
i12
e

2024-06-18 13:58:58,990 Answer received: !yro992
2024-06-18 13:58:58,990 Command to send: a
e
o979
e

2024-06-18 13:58:58,990 Answer received: !yi16
2024-06-18 13:58:58,990 Command to send: a
g
o979
i13
e

2024-06-18 13:58:58,990 Answer received: !yro993
2024-06-18 13:58:58,990 Command to send: a
e
o979
e

2024-06-18 13:58:58,991 Answer received: !yi16
2024-06-18 13:58:58,991 Command to send: a
g
o979
i14
e

2024-06-18 13:58:58,991 Answer received: !yro994
2024-06-18 13:58:58,991 Command to send: a
e
o979
e

2024-06-18 13:58:58,991 Answer received: !yi16
2024-06-18 13:58:58,991 Command to send: a
g
o979
i15
e

2024-06-18 13:58:58,992 Answer received: !yro995
2024-06-18 13:58:58,992 Command to send: a
e
o979
e

2024-06-18 13:58:58,992 Answer received: !yi16
2024-06-18 13:58:58,992 Command to send: c
o980
name
e

2024-06-18 13:58:58,993 Answer received: !ysaggregationDepth
2024-06-18 13:58:58,993 Command to send: c
o981
name
e

2024-06-18 13:58:58,993 Answer received: !ysfamily
2024-06-18 13:58:58,993 Command to send: c
o982
name
e

2024-06-18 13:58:58,993 Answer received: !ysfeaturesCol
2024-06-18 13:58:58,994 Command to send: c
o983
name
e

2024-06-18 13:58:58,994 Answer received: !ysfitIntercept
2024-06-18 13:58:58,994 Command to send: c
o984
name
e

2024-06-18 13:58:58,994 Answer received: !yslabelCol
2024-06-18 13:58:58,994 Command to send: c
o985
name
e

2024-06-18 13:58:58,994 Answer received: !yslink
2024-06-18 13:58:58,995 Command to send: c
o986
name
e

2024-06-18 13:58:58,995 Answer received: !yslinkPower
2024-06-18 13:58:58,995 Command to send: c
o987
name
e

2024-06-18 13:58:58,995 Answer received: !yslinkPredictionCol
2024-06-18 13:58:58,995 Command to send: c
o988
name
e

2024-06-18 13:58:58,996 Answer received: !ysmaxIter
2024-06-18 13:58:58,996 Command to send: c
o989
name
e

2024-06-18 13:58:58,996 Answer received: !ysoffsetCol
2024-06-18 13:58:58,996 Command to send: c
o990
name
e

2024-06-18 13:58:58,996 Answer received: !yspredictionCol
2024-06-18 13:58:58,996 Command to send: c
o991
name
e

2024-06-18 13:58:58,997 Answer received: !ysregParam
2024-06-18 13:58:58,997 Command to send: c
o992
name
e

2024-06-18 13:58:58,997 Answer received: !yssolver
2024-06-18 13:58:58,997 Command to send: c
o993
name
e

2024-06-18 13:58:58,998 Answer received: !ystol
2024-06-18 13:58:58,998 Command to send: c
o994
name
e

2024-06-18 13:58:58,998 Answer received: !ysvariancePower
2024-06-18 13:58:58,998 Command to send: c
o995
name
e

2024-06-18 13:58:58,998 Answer received: !ysweightCol
2024-06-18 13:58:58,998 Command to send: c
o978
uid
e

2024-06-18 13:58:58,999 Answer received: !ysGeneralizedLinearRegression_63879d505356
2024-06-18 13:58:59,011 Command to send: c
o978
getParam
saggregationDepth
e

2024-06-18 13:58:59,011 Answer received: !yro996
2024-06-18 13:58:59,012 Command to send: c
o996
w
i2
e

2024-06-18 13:58:59,012 Answer received: !yro997
2024-06-18 13:58:59,012 Command to send: c
o978
getParam
sfamily
e

2024-06-18 13:58:59,013 Answer received: !yro998
2024-06-18 13:58:59,013 Command to send: c
o998
w
sgaussian
e

2024-06-18 13:58:59,013 Answer received: !yro999
2024-06-18 13:58:59,013 Command to send: c
o978
set
ro999
e

2024-06-18 13:58:59,014 Answer received: !yro1000
2024-06-18 13:58:59,014 Command to send: c
o978
getParam
sfamily
e

2024-06-18 13:58:59,014 Answer received: !yro1001
2024-06-18 13:58:59,014 Command to send: c
o1001
w
sgaussian
e

2024-06-18 13:58:59,015 Answer received: !yro1002
2024-06-18 13:58:59,015 Command to send: c
o978
getParam
sfeaturesCol
e

2024-06-18 13:58:59,015 Answer received: !yro1003
2024-06-18 13:58:59,015 Command to send: c
o1003
w
sfeatures
e

2024-06-18 13:58:59,016 Answer received: !yro1004
2024-06-18 13:58:59,016 Command to send: c
o978
getParam
sfitIntercept
e

2024-06-18 13:58:59,016 Answer received: !yro1005
2024-06-18 13:58:59,016 Command to send: c
o1005
w
bTrue
e

2024-06-18 13:58:59,017 Answer received: !yro1006
2024-06-18 13:58:59,017 Command to send: c
o978
getParam
slabelCol
e

2024-06-18 13:58:59,017 Answer received: !yro1007
2024-06-18 13:58:59,018 Command to send: c
o1007
w
sprice
e

2024-06-18 13:58:59,018 Answer received: !yro1008
2024-06-18 13:58:59,019 Command to send: c
o978
set
ro1008
e

2024-06-18 13:58:59,019 Answer received: !yro1009
2024-06-18 13:58:59,019 Command to send: c
o978
getParam
slabelCol
e

2024-06-18 13:58:59,019 Answer received: !yro1010
2024-06-18 13:58:59,020 Command to send: c
o1010
w
slabel
e

2024-06-18 13:58:59,020 Answer received: !yro1011
2024-06-18 13:58:59,020 Command to send: c
o978
getParam
slink
e

2024-06-18 13:58:59,020 Answer received: !yro1012
2024-06-18 13:58:59,021 Command to send: c
o1012
w
sidentity
e

2024-06-18 13:58:59,021 Answer received: !yro1013
2024-06-18 13:58:59,021 Command to send: c
o978
set
ro1013
e

2024-06-18 13:58:59,021 Answer received: !yro1014
2024-06-18 13:58:59,022 Command to send: c
o978
getParam
smaxIter
e

2024-06-18 13:58:59,022 Answer received: !yro1015
2024-06-18 13:58:59,022 Command to send: c
o1015
w
i10
e

2024-06-18 13:58:59,022 Answer received: !yro1016
2024-06-18 13:58:59,022 Command to send: c
o978
set
ro1016
e

2024-06-18 13:58:59,023 Answer received: !yro1017
2024-06-18 13:58:59,023 Command to send: c
o978
getParam
smaxIter
e

2024-06-18 13:58:59,023 Answer received: !yro1018
2024-06-18 13:58:59,023 Command to send: c
o1018
w
i25
e

2024-06-18 13:58:59,024 Answer received: !yro1019
2024-06-18 13:58:59,024 Command to send: c
o978
getParam
spredictionCol
e

2024-06-18 13:58:59,024 Answer received: !yro1020
2024-06-18 13:58:59,024 Command to send: c
o1020
w
sprediction
e

2024-06-18 13:58:59,025 Answer received: !yro1021
2024-06-18 13:58:59,025 Command to send: c
o978
getParam
sregParam
e

2024-06-18 13:58:59,025 Answer received: !yro1022
2024-06-18 13:58:59,026 Command to send: c
o1022
w
d0.3
e

2024-06-18 13:58:59,026 Answer received: !yro1023
2024-06-18 13:58:59,026 Command to send: c
o978
set
ro1023
e

2024-06-18 13:58:59,027 Answer received: !yro1024
2024-06-18 13:58:59,027 Command to send: c
o978
getParam
sregParam
e

2024-06-18 13:58:59,028 Answer received: !yro1025
2024-06-18 13:58:59,028 Command to send: c
o1025
w
d0.0
e

2024-06-18 13:58:59,028 Answer received: !yro1026
2024-06-18 13:58:59,029 Command to send: c
o978
getParam
ssolver
e

2024-06-18 13:58:59,029 Answer received: !yro1027
2024-06-18 13:58:59,029 Command to send: c
o1027
w
sirls
e

2024-06-18 13:58:59,029 Answer received: !yro1028
2024-06-18 13:58:59,030 Command to send: c
o978
getParam
stol
e

2024-06-18 13:58:59,030 Answer received: !yro1029
2024-06-18 13:58:59,030 Command to send: c
o1029
w
d1e-06
e

2024-06-18 13:58:59,030 Answer received: !yro1030
2024-06-18 13:58:59,030 Command to send: c
o978
getParam
svariancePower
e

2024-06-18 13:58:59,031 Answer received: !yro1031
2024-06-18 13:58:59,031 Command to send: c
o1031
w
d0.0
e

2024-06-18 13:58:59,031 Answer received: !yro1032
2024-06-18 13:58:59,031 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:58:59,032 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:58:59,032 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:58:59,032 Answer received: !ym
2024-06-18 13:58:59,033 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:59,033 Answer received: !ylo1033
2024-06-18 13:58:59,033 Command to send: c
o1033
add
ro997
e

2024-06-18 13:58:59,033 Answer received: !ybtrue
2024-06-18 13:58:59,034 Command to send: c
o1033
add
ro1002
e

2024-06-18 13:58:59,034 Answer received: !ybtrue
2024-06-18 13:58:59,034 Command to send: c
o1033
add
ro1004
e

2024-06-18 13:58:59,034 Answer received: !ybtrue
2024-06-18 13:58:59,035 Command to send: c
o1033
add
ro1006
e

2024-06-18 13:58:59,035 Answer received: !ybtrue
2024-06-18 13:58:59,035 Command to send: c
o1033
add
ro1011
e

2024-06-18 13:58:59,036 Answer received: !ybtrue
2024-06-18 13:58:59,036 Command to send: c
o1033
add
ro1019
e

2024-06-18 13:58:59,036 Answer received: !ybtrue
2024-06-18 13:58:59,036 Command to send: c
o1033
add
ro1021
e

2024-06-18 13:58:59,036 Answer received: !ybtrue
2024-06-18 13:58:59,036 Command to send: c
o1033
add
ro1026
e

2024-06-18 13:58:59,037 Answer received: !ybtrue
2024-06-18 13:58:59,037 Command to send: c
o1033
add
ro1028
e

2024-06-18 13:58:59,037 Answer received: !ybtrue
2024-06-18 13:58:59,037 Command to send: c
o1033
add
ro1030
e

2024-06-18 13:58:59,038 Answer received: !ybtrue
2024-06-18 13:58:59,038 Command to send: c
o1033
add
ro1032
e

2024-06-18 13:58:59,038 Answer received: !ybtrue
2024-06-18 13:58:59,038 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro1033
e

2024-06-18 13:58:59,038 Answer received: !yro1034
2024-06-18 13:58:59,039 Command to send: c
o978
setDefault
ro1034
e

2024-06-18 13:58:59,039 Answer received: !yro1035
2024-06-18 13:58:59,039 Command to send: c
o978
transform
ro935
e

2024-06-18 13:58:59,069 Answer received: !yro1036
2024-06-18 13:58:59,071 Command to send: r
u
org
rj
e

2024-06-18 13:58:59,077 Answer received: !yp
2024-06-18 13:58:59,078 Command to send: r
u
org.apache
rj
e

2024-06-18 13:58:59,079 Answer received: !yp
2024-06-18 13:58:59,079 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:58:59,080 Answer received: !yp
2024-06-18 13:58:59,080 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:58:59,081 Answer received: !yp
2024-06-18 13:58:59,081 Command to send: r
u
org.apache.spark.ml.evaluation
rj
e

2024-06-18 13:58:59,081 Answer received: !yp
2024-06-18 13:58:59,081 Command to send: r
u
org.apache.spark.ml.evaluation.RegressionEvaluator
rj
e

2024-06-18 13:58:59,082 Answer received: !ycorg.apache.spark.ml.evaluation.RegressionEvaluator
2024-06-18 13:58:59,082 Command to send: i
org.apache.spark.ml.evaluation.RegressionEvaluator
sRegressionEvaluator_13b2c2c85947
e

2024-06-18 13:58:59,082 Answer received: !yro1037
2024-06-18 13:58:59,083 Command to send: c
o1037
getParam
slabelCol
e

2024-06-18 13:58:59,084 Answer received: !yro1038
2024-06-18 13:58:59,084 Command to send: c
o1038
w
sprice
e

2024-06-18 13:58:59,085 Answer received: !yro1039
2024-06-18 13:58:59,085 Command to send: c
o1037
set
ro1039
e

2024-06-18 13:58:59,085 Answer received: !yro1040
2024-06-18 13:58:59,086 Command to send: c
o1037
getParam
slabelCol
e

2024-06-18 13:58:59,086 Answer received: !yro1041
2024-06-18 13:58:59,086 Command to send: c
o1041
w
slabel
e

2024-06-18 13:58:59,087 Answer received: !yro1042
2024-06-18 13:58:59,087 Command to send: c
o1037
getParam
smetricName
e

2024-06-18 13:58:59,087 Answer received: !yro1043
2024-06-18 13:58:59,088 Command to send: c
o1043
w
sr2
e

2024-06-18 13:58:59,088 Answer received: !yro1044
2024-06-18 13:58:59,088 Command to send: c
o1037
set
ro1044
e

2024-06-18 13:58:59,089 Answer received: !yro1045
2024-06-18 13:58:59,089 Command to send: c
o1037
getParam
smetricName
e

2024-06-18 13:58:59,089 Answer received: !yro1046
2024-06-18 13:58:59,090 Command to send: c
o1046
w
srmse
e

2024-06-18 13:58:59,090 Answer received: !yro1047
2024-06-18 13:58:59,090 Command to send: c
o1037
getParam
spredictionCol
e

2024-06-18 13:58:59,091 Answer received: !yro1048
2024-06-18 13:58:59,091 Command to send: c
o1048
w
sprediction
e

2024-06-18 13:58:59,091 Answer received: !yro1049
2024-06-18 13:58:59,091 Command to send: c
o1037
getParam
sthroughOrigin
e

2024-06-18 13:58:59,092 Answer received: !yro1050
2024-06-18 13:58:59,092 Command to send: c
o1050
w
bFalse
e

2024-06-18 13:58:59,093 Answer received: !yro1051
2024-06-18 13:58:59,093 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:58:59,094 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:58:59,094 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:58:59,095 Answer received: !ym
2024-06-18 13:58:59,095 Command to send: i
java.util.ArrayList
e

2024-06-18 13:58:59,095 Answer received: !ylo1052
2024-06-18 13:58:59,096 Command to send: c
o1052
add
ro1042
e

2024-06-18 13:58:59,096 Answer received: !ybtrue
2024-06-18 13:58:59,096 Command to send: c
o1052
add
ro1047
e

2024-06-18 13:58:59,096 Answer received: !ybtrue
2024-06-18 13:58:59,097 Command to send: c
o1052
add
ro1049
e

2024-06-18 13:58:59,097 Answer received: !ybtrue
2024-06-18 13:58:59,097 Command to send: c
o1052
add
ro1051
e

2024-06-18 13:58:59,097 Answer received: !ybtrue
2024-06-18 13:58:59,097 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro1052
e

2024-06-18 13:58:59,098 Answer received: !yro1053
2024-06-18 13:58:59,098 Command to send: c
o1037
setDefault
ro1053
e

2024-06-18 13:58:59,098 Answer received: !yro1054
2024-06-18 13:58:59,098 Command to send: c
o1037
evaluate
ro1036
e

2024-06-18 13:58:59,223 Command to send: m
d
o979
e

2024-06-18 13:58:59,223 Answer received: !yv
2024-06-18 13:58:59,224 Command to send: m
d
o818
e

2024-06-18 13:58:59,224 Answer received: !yv
2024-06-18 13:58:59,224 Command to send: m
d
o1000
e

2024-06-18 13:58:59,224 Answer received: !yv
2024-06-18 13:58:59,224 Command to send: m
d
o1009
e

2024-06-18 13:58:59,225 Answer received: !yv
2024-06-18 13:58:59,225 Command to send: m
d
o1014
e

2024-06-18 13:58:59,225 Answer received: !yv
2024-06-18 13:58:59,226 Command to send: m
d
o1017
e

2024-06-18 13:58:59,226 Answer received: !yv
2024-06-18 13:58:59,226 Command to send: m
d
o1024
e

2024-06-18 13:58:59,227 Answer received: !yv
2024-06-18 13:58:59,227 Command to send: m
d
o1033
e

2024-06-18 13:58:59,227 Answer received: !yv
2024-06-18 13:58:59,227 Command to send: m
d
o1035
e

2024-06-18 13:58:59,227 Answer received: !yv
2024-06-18 13:58:59,227 Command to send: m
d
o895
e

2024-06-18 13:58:59,228 Answer received: !yv
2024-06-18 13:58:59,228 Command to send: m
d
o1040
e

2024-06-18 13:58:59,228 Answer received: !yv
2024-06-18 13:58:59,228 Command to send: m
d
o1045
e

2024-06-18 13:58:59,228 Answer received: !yv
2024-06-18 13:58:59,228 Command to send: m
d
o1052
e

2024-06-18 13:58:59,229 Answer received: !yv
2024-06-18 13:58:59,229 Command to send: m
d
o1054
e

2024-06-18 13:58:59,229 Answer received: !yv
2024-06-18 13:59:00,065 Answer received: !yd0.7444120529989751
2024-06-18 13:59:00,065 Command to send: r
u
org
rj
e

2024-06-18 13:59:00,068 Answer received: !yp
2024-06-18 13:59:00,068 Command to send: r
u
org.apache
rj
e

2024-06-18 13:59:00,069 Answer received: !yp
2024-06-18 13:59:00,069 Command to send: r
u
org.apache.spark
rj
e

2024-06-18 13:59:00,069 Answer received: !yp
2024-06-18 13:59:00,069 Command to send: r
u
org.apache.spark.ml
rj
e

2024-06-18 13:59:00,070 Answer received: !yp
2024-06-18 13:59:00,070 Command to send: r
u
org.apache.spark.ml.evaluation
rj
e

2024-06-18 13:59:00,070 Answer received: !yp
2024-06-18 13:59:00,071 Command to send: r
u
org.apache.spark.ml.evaluation.RegressionEvaluator
rj
e

2024-06-18 13:59:00,071 Answer received: !ycorg.apache.spark.ml.evaluation.RegressionEvaluator
2024-06-18 13:59:00,071 Command to send: i
org.apache.spark.ml.evaluation.RegressionEvaluator
sRegressionEvaluator_c159ba451716
e

2024-06-18 13:59:00,072 Answer received: !yro1055
2024-06-18 13:59:00,072 Command to send: c
o1055
getParam
slabelCol
e

2024-06-18 13:59:00,072 Answer received: !yro1056
2024-06-18 13:59:00,073 Command to send: c
o1056
w
sprice
e

2024-06-18 13:59:00,073 Answer received: !yro1057
2024-06-18 13:59:00,073 Command to send: c
o1055
set
ro1057
e

2024-06-18 13:59:00,073 Answer received: !yro1058
2024-06-18 13:59:00,074 Command to send: c
o1055
getParam
slabelCol
e

2024-06-18 13:59:00,074 Answer received: !yro1059
2024-06-18 13:59:00,074 Command to send: c
o1059
w
slabel
e

2024-06-18 13:59:00,074 Answer received: !yro1060
2024-06-18 13:59:00,075 Command to send: c
o1055
getParam
smetricName
e

2024-06-18 13:59:00,075 Answer received: !yro1061
2024-06-18 13:59:00,075 Command to send: c
o1061
w
srmse
e

2024-06-18 13:59:00,076 Answer received: !yro1062
2024-06-18 13:59:00,076 Command to send: c
o1055
set
ro1062
e

2024-06-18 13:59:00,076 Answer received: !yro1063
2024-06-18 13:59:00,076 Command to send: c
o1055
getParam
smetricName
e

2024-06-18 13:59:00,077 Answer received: !yro1064
2024-06-18 13:59:00,077 Command to send: c
o1064
w
srmse
e

2024-06-18 13:59:00,077 Answer received: !yro1065
2024-06-18 13:59:00,077 Command to send: c
o1055
getParam
spredictionCol
e

2024-06-18 13:59:00,078 Answer received: !yro1066
2024-06-18 13:59:00,078 Command to send: c
o1066
w
sprediction
e

2024-06-18 13:59:00,078 Answer received: !yro1067
2024-06-18 13:59:00,078 Command to send: c
o1055
getParam
sthroughOrigin
e

2024-06-18 13:59:00,079 Answer received: !yro1068
2024-06-18 13:59:00,079 Command to send: c
o1068
w
bFalse
e

2024-06-18 13:59:00,079 Answer received: !yro1069
2024-06-18 13:59:00,079 Command to send: r
u
PythonUtils
rj
e

2024-06-18 13:59:00,080 Answer received: !ycorg.apache.spark.api.python.PythonUtils
2024-06-18 13:59:00,080 Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2024-06-18 13:59:00,080 Answer received: !ym
2024-06-18 13:59:00,081 Command to send: i
java.util.ArrayList
e

2024-06-18 13:59:00,081 Answer received: !ylo1070
2024-06-18 13:59:00,081 Command to send: c
o1070
add
ro1060
e

2024-06-18 13:59:00,081 Answer received: !ybtrue
2024-06-18 13:59:00,082 Command to send: c
o1070
add
ro1065
e

2024-06-18 13:59:00,082 Answer received: !ybtrue
2024-06-18 13:59:00,082 Command to send: c
o1070
add
ro1067
e

2024-06-18 13:59:00,082 Answer received: !ybtrue
2024-06-18 13:59:00,083 Command to send: c
o1070
add
ro1069
e

2024-06-18 13:59:00,083 Answer received: !ybtrue
2024-06-18 13:59:00,083 Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro1070
e

2024-06-18 13:59:00,083 Answer received: !yro1071
2024-06-18 13:59:00,083 Command to send: c
o1055
setDefault
ro1071
e

2024-06-18 13:59:00,084 Answer received: !yro1072
2024-06-18 13:59:00,084 Command to send: c
o1055
evaluate
ro1036
e

2024-06-18 13:59:00,230 Command to send: m
d
o1037
e

2024-06-18 13:59:00,230 Answer received: !yv
2024-06-18 13:59:00,230 Command to send: m
d
o1058
e

2024-06-18 13:59:00,230 Answer received: !yv
2024-06-18 13:59:00,231 Command to send: m
d
o1063
e

2024-06-18 13:59:00,231 Answer received: !yv
2024-06-18 13:59:00,231 Command to send: m
d
o1070
e

2024-06-18 13:59:00,231 Answer received: !yv
2024-06-18 13:59:00,231 Command to send: m
d
o1072
e

2024-06-18 13:59:00,232 Answer received: !yv
2024-06-18 13:59:00,753 Answer received: !yd211984.0575014574
