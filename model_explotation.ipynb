{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "from time import strftime, gmtime\n",
    "from datetime import datetime\n",
    "import os\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "import re\n",
    "import os\n",
    "from time import strftime, gmtime\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "\n",
    "import findspark\n",
    "import warnings\n",
    "from pyspark.sql.functions import split, col, avg, count\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "\n",
    "import pyspark as py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read login information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.py\n",
    "\n",
    "log = log_config(\"My.log\")\n",
    "logging_info = logging_creation()\n",
    "\n",
    "user_name = logging_info[\"user_name\"]\n",
    "host = logging_info[\"host\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from formatted zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_SelectorsAndFormatters.py\n",
    "\n",
    "inp_path=\"model_temp\"\n",
    "master_files = {\n",
    "    \"hotels\": {\"Description\": \"Information about hotels in neighbourhoods\"},\n",
    "    \"renda_familiar\": {\n",
    "        \"keywords\": [\"renda_familiar\"],\n",
    "        \"Description\": \"Data from Open Barcelona with incomes of families. Can be joined with 'idealista' file using a lookup file\",\n",
    "    },\n",
    "    \"idealista\": {\n",
    "        \"keywords\": [\"idealista\"],\n",
    "        \"Description\": \"Appartments from idealista. Can be joined with 'renta familiar' file using a lookup file\",\n",
    "    },\n",
    "    \"lookup_renta_idealista\": {\n",
    "        \"keywords\": [\"extended\"],\n",
    "        \"Description\": \"Lookup datable to join 'Idealista' and 'renda familiar'\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load last versions of all dataframes from explotation zone\n",
    "for key_file in master_files.keys():\n",
    "    formatted_data_selector(log, user_name, host, key_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the spark session is correctly loaded,\n",
    "# then create a sesion\n",
    "findspark.init(\"D:\\spark\\spark-3.5.1-bin-hadoop3\")\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set(\"spark.master\", \"local\")\n",
    "        .set(\"spark.app.name\", \"Formatted zone loader\")\n",
    "    )\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "log.info(\"Spark sesion correctly initialized\")\n",
    "\n",
    "# Load all dataframes in a dictionary\n",
    "dfs = {}\n",
    "file_names = os.listdir(inp_path)\n",
    "\n",
    "for file in file_names:\n",
    "    full_in_path = inp_path + \"/\" + file\n",
    "\n",
    "    # Load dataframes\n",
    "    short_name = file.split(\"-\", maxsplit=2)[2]\n",
    "    short_name = \"\".join(short_name)\n",
    "\n",
    "    dfs[short_name] = (\n",
    "            spark.read.option(\"multiline\", \"true\").format(\"parquet\").load(full_in_path)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define from all files usefull columns for the analysis\n",
    "# {file1: [columns], file2: [columns]}\n",
    "selected_fields = {\n",
    "    \"renda_familiar\": [\n",
    "            \"Nom_Districte\",\n",
    "            \"Codi_Districte\",\n",
    "            \"Índex RFD Barcelona = 100\",\n",
    "        ],\n",
    "        \"idealista\": [\"district\", \"numPhotos\", \"price\", \"floor\", \"size\", \"bathrooms\"],\n",
    "        \"lookup_renta_idealista\": [\"district\", \"district_id\"],\n",
    "        \"hotels\": [\n",
    "            \"addresses_district_id\",\n",
    "            \"name\",\n",
    "            \"secondary_filters_name\",\n",
    "            \"geo_epgs_4326_lat\",\n",
    "            \"geo_epgs_4326_lon\",\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 'renda_familiar' dataframe, selected the following schema\n",
      "root\n",
      " |-- Nom_Districte: string (nullable = true)\n",
      " |-- Codi_Districte: string (nullable = true)\n",
      " |-- Índex RFD Barcelona = 100: string (nullable = true)\n",
      "\n",
      "Removed 0.0 % rows that contained NA's\n",
      "From 'idealista' dataframe, selected the following schema\n",
      "root\n",
      " |-- district: string (nullable = true)\n",
      " |-- numPhotos: long (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- floor: string (nullable = true)\n",
      " |-- size: double (nullable = true)\n",
      " |-- bathrooms: long (nullable = true)\n",
      "\n",
      "Removed 23.0026251919362 % rows that contained NA's\n",
      "From 'lookup_renta_idealista' dataframe, selected the following schema\n",
      "root\n",
      " |-- district: string (nullable = true)\n",
      " |-- district_id: string (nullable = true)\n",
      "\n",
      "Removed 0.0 % rows that contained NA's\n",
      "From 'hotels' dataframe, selected the following schema\n",
      "root\n",
      " |-- addresses_district_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- secondary_filters_name: string (nullable = true)\n",
      " |-- geo_epgs_4326_lat: string (nullable = true)\n",
      " |-- geo_epgs_4326_lon: string (nullable = true)\n",
      "\n",
      "Removed 0.22522522522522523 % rows that contained NA's\n"
     ]
    }
   ],
   "source": [
    "for key, df in dfs.items():\n",
    "\n",
    "    # Select columns and remove Nan\n",
    "    df = df.select(selected_fields[key])\n",
    "    print(f\"From '{key}' dataframe, selected the following schema\")\n",
    "    df.printSchema()\n",
    "    n_col_raw = df.count()\n",
    "    df = df.na.drop()\n",
    "\n",
    "    print(\n",
    "        f\"Removed {((n_col_raw - df.count())/n_col_raw) * 100} % rows that contained NA's\"\n",
    "    )\n",
    "\n",
    "    dfs[key] = df\n",
    "\n",
    "# Calculate KPI's\n",
    "dfs[\"hotels\"] = (\n",
    "    dfs[\"hotels\"]\n",
    "    .withColumn(\n",
    "        \"stars\",\n",
    "        split(col(\"secondary_filters_name\"), \" \").cast(\"array<int>\"),\n",
    "    )\n",
    "    .withColumn(\"stars\", col(\"stars\")[1])\n",
    ")\n",
    "dfs[\"hotels\"] = dfs[\"hotels\"].drop(\"secondary_filters_name\")\n",
    "\n",
    "# Declare numeric variables\n",
    "dfs[\"hotels\"] = (\n",
    "    dfs[\"hotels\"]\n",
    "    .withColumn(\n",
    "        \"addresses_district_id\",\n",
    "        dfs[\"hotels\"][\"addresses_district_id\"].cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"geo_epgs_4326_lat\",\n",
    "        dfs[\"hotels\"][\"geo_epgs_4326_lat\"].cast(IntegerType()),\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"geo_epgs_4326_lon\",\n",
    "        dfs[\"hotels\"][\"geo_epgs_4326_lon\"].cast(IntegerType()),\n",
    "    )\n",
    ")\n",
    "\n",
    "dfs[\"idealista\"] = dfs[\"idealista\"].withColumn(\n",
    "    \"floor\",\n",
    "    dfs[\"idealista\"][\"floor\"].cast(IntegerType()),\n",
    ")\n",
    "\n",
    "# Transform \"districts names\" from \"renda familiar\" \n",
    "# so that they match the lookup table\n",
    "dfs[\"renda_familiar\"] = (\n",
    "    dfs[\"renda_familiar\"]\n",
    "    .withColumn(\n",
    "        \"Nom_Districte\",\n",
    "        split(col(\"Nom_Districte\"), '\"'),\n",
    "    )\n",
    "    .withColumn(\"Nom_Districte\", col(\"Nom_Districte\")[1])\n",
    "    .withColumn(\n",
    "        \"Índex RFD Barcelona = 100\",\n",
    "        split(col(\"Índex RFD Barcelona = 100\"), '\"'),\n",
    "    )\n",
    "    .withColumn(\"Índex RFD Barcelona = 100\", col(\"Índex RFD Barcelona = 100\")[1])\n",
    ")\n",
    "\n",
    "# Group dataframes by district to reduce cardinality of joins\n",
    "dfs[\"hotels\"] = (\n",
    "    dfs[\"hotels\"]\n",
    "    .groupBy(\"addresses_district_id\")\n",
    "    .agg(\n",
    "        avg(\"stars\").alias(\"Avg_stars\"),\n",
    "        count(\"name\").alias(\"N_hotels\"),\n",
    "        avg(\"geo_epgs_4326_lat\").alias(\"Avg_lat\"),\n",
    "        avg(\"geo_epgs_4326_lon\").alias(\"Avg_long\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "dfs[\"renda_familiar\"] = (\n",
    "    dfs[\"renda_familiar\"]\n",
    "    .groupBy([\"Nom_Districte\", \"Codi_Districte\"])\n",
    "    .agg(avg(\"Índex RFD Barcelona = 100\").alias(\"Avg_Index_RFD\"))\n",
    ")\n",
    "\n",
    "## Join files (remove useless attributes after each join)\n",
    "# 'renta familiar' and hotels\n",
    "dfs[\"model\"] = dfs[\"hotels\"].join(\n",
    "    dfs[\"renda_familiar\"],\n",
    "    dfs[\"hotels\"].addresses_district_id == dfs[\"renda_familiar\"].Codi_Districte,\n",
    "    \"inner\",\n",
    ")\n",
    "\n",
    "dfs[\"model\"] = dfs[\"model\"].drop(*[\"Codi_Districte\", \"addresses_district_id\"])\n",
    "\n",
    "# join lookup tables\n",
    "dfs[\"model\"] = dfs[\"model\"].join(\n",
    "    dfs[\"lookup_renta_idealista\"],\n",
    "    dfs[\"model\"].Nom_Districte == dfs[\"lookup_renta_idealista\"].district,\n",
    "    \"inner\",\n",
    ")\n",
    "\n",
    "dfs[\"model\"] = dfs[\"model\"].drop(\"district_id\")\n",
    "\n",
    "# join idealista\n",
    "dfs[\"model\"] = dfs[\"model\"].join(\n",
    "    dfs[\"idealista\"],\n",
    "    dfs[\"model\"].district == dfs[\"idealista\"].district,\n",
    "    \"inner\",\n",
    ")\n",
    "dfs[\"model\"] = dfs[\"model\"].drop(\"district\")\n",
    "dfs[\"model\"] = dfs[\"model\"].drop(\"Nom_districte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload preprocessed dataframe to the explotation zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"GLM_V1.parquet\"\n",
    "explotation_zone_path = \"user/bdm/Model_explotation_zone\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client = InsecureClient(host, user=user_name)\n",
    "out_full_path = explotation_zone_path + \"/\" + model_name \n",
    "\n",
    "dfs[\"model\"].write.parquet(model_name)\n",
    "hdfs_client.upload(out_full_path, model_name, overwrite=True)\n",
    "\n",
    "log.info(f\"Model {file} uploaded correctly at '{out_full_path}' path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Avg_stars\",\n",
    "    \"N_hotels\",\n",
    "    \"Avg_lat\",\n",
    "    \"Avg_lat\",\n",
    "    \"numPhotos\",\n",
    "    \"floor\",\n",
    "    \"size\",\n",
    "    \"Avg_Index_RFD\",\n",
    "    \"numPhotos\",\n",
    "    \"bathrooms\",\n",
    "    \"Avg_Index_RFD\",\n",
    "]\n",
    "\n",
    "df = dfs[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data splitted in Test 38520 rows and Train 16720 rows\n"
     ]
    }
   ],
   "source": [
    "vectorAssembler = VectorAssembler(\n",
    "    inputCols=features,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\",\n",
    ")\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Divide dataset\n",
    "train, test = df.randomSplit([0.7, 0.3], seed=123)\n",
    "print(f\"Data splitted in Test {train.count()} rows and Train {test.count()} rows\")\n",
    "\n",
    "# Define the model and train it\n",
    "glr = GeneralizedLinearRegression(\n",
    "    family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3, labelCol=\"price\"\n",
    ")\n",
    "\n",
    "model1 = glr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating results of the model...\n",
      "r2 = 0.7444120529989751\n",
      "rmse = 211984.0575014574\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.transform(train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating results of the model...\")\n",
    "\n",
    "metrics = [\"r2\", \"rmse\"]\n",
    "\n",
    "for metric in metrics:\n",
    "    evaluator = RegressionEvaluator(metricName=metric, labelCol=\"price\")\n",
    "    results = evaluator.evaluate(predictions)\n",
    "\n",
    "    print(f\"{metric} = {results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
